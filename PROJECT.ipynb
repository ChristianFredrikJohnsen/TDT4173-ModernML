{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de319cba",
   "metadata": {},
   "source": [
    "# Overview of project\n",
    "We are provided historic data of raw material deliveries and orders through the end of 2024. GOAL: Develop a model that forecasts the cumulative weight of incoming deliveries of each raw material from Jan 1, 2025, up to any specified end date between Jan 1 and May 31, 2025.\n",
    "\n",
    "- recievals = historical records of material recievals\n",
    "- purchase_orders = ordered quantities and expected deliv\n",
    "- materials(opt) = metadata on various raw materials\n",
    "- transportation(opt) = transport-related data\n",
    "\n",
    "QuantileLoss0.2(Fi,Ai) = max(0.2*(Ai − Fi), 0.8*(Fi − Ai)).\n",
    "\n",
    "rm_id = unique identifer for raw material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6d52f776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>purchase_order_id</th>\n",
       "      <th>purchase_order_item_no_order</th>\n",
       "      <th>quantity</th>\n",
       "      <th>delivery_date</th>\n",
       "      <th>product_id_order</th>\n",
       "      <th>product_version</th>\n",
       "      <th>created_date_time</th>\n",
       "      <th>modified_date_time</th>\n",
       "      <th>unit_id</th>\n",
       "      <th>unit</th>\n",
       "      <th>...</th>\n",
       "      <th>status</th>\n",
       "      <th>rm_id</th>\n",
       "      <th>product_id_receival</th>\n",
       "      <th>purchase_order_item_no_receival</th>\n",
       "      <th>receival_item_no</th>\n",
       "      <th>batch_id</th>\n",
       "      <th>date_arrival</th>\n",
       "      <th>receival_status</th>\n",
       "      <th>net_weight</th>\n",
       "      <th>supplier_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>2003-05-12 00:00:00.0000000 +02:00</td>\n",
       "      <td>91900143</td>\n",
       "      <td>1</td>\n",
       "      <td>2003-05-12 10:00:48.0000000 +00:00</td>\n",
       "      <td>2004-06-15 06:16:18.0000000 +00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Closed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>23880.0</td>\n",
       "      <td>2003-05-27 00:00:00.0000000 +02:00</td>\n",
       "      <td>91900160</td>\n",
       "      <td>1</td>\n",
       "      <td>2003-05-27 12:42:07.0000000 +00:00</td>\n",
       "      <td>2012-06-29 09:41:13.0000000 +00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Closed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   purchase_order_id  purchase_order_item_no_order  quantity  \\\n",
       "0                  1                             1     -14.0   \n",
       "1                 22                             1   23880.0   \n",
       "\n",
       "                        delivery_date  product_id_order  product_version  \\\n",
       "0  2003-05-12 00:00:00.0000000 +02:00          91900143                1   \n",
       "1  2003-05-27 00:00:00.0000000 +02:00          91900160                1   \n",
       "\n",
       "                    created_date_time                  modified_date_time  \\\n",
       "0  2003-05-12 10:00:48.0000000 +00:00  2004-06-15 06:16:18.0000000 +00:00   \n",
       "1  2003-05-27 12:42:07.0000000 +00:00  2012-06-29 09:41:13.0000000 +00:00   \n",
       "\n",
       "   unit_id unit  ...  status rm_id  product_id_receival  \\\n",
       "0      NaN  NaN  ...  Closed   NaN                  NaN   \n",
       "1      NaN  NaN  ...  Closed   NaN                  NaN   \n",
       "\n",
       "   purchase_order_item_no_receival  receival_item_no  batch_id  date_arrival  \\\n",
       "0                              NaN               NaN       NaN           NaN   \n",
       "1                              NaN               NaN       NaN           NaN   \n",
       "\n",
       "  receival_status net_weight  supplier_id  \n",
       "0             NaN        NaN          NaN  \n",
       "1             NaN        NaN          NaN  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to explore the data\n",
    "\n",
    "# First I want to check the difference between purchase orders\n",
    "# and recievals. How much was the difference between the two?\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the data\n",
    "data_orders = pd.read_csv('data/kernel/purchase_orders.csv')\n",
    "data_receivals = pd.read_csv('data/kernel/receivals.csv')\n",
    "\n",
    "# Link the 5 first data orders to the recievals\n",
    "data = pd.merge(data_orders.head(5), data_receivals, on='purchase_order_id', suffixes=('_order', '_receival'), how='left')\n",
    "\n",
    "data.head(2)\n",
    "\n",
    "# NOT EVERY ORDER HAS A RECEIVAL? Oh... it makes sense cause some orders are never received? But I put the 5 in the head.... 5....\n",
    "# 5 whole orders are not received? That seems like a lot.... Nah maybe it's just purchase_order_id is a bad key to merge on.\n",
    "# Let's try purchase_order_item_no... # Absolutely not. I forgot it was like simple 1 etc...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8f9a8ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "563\n"
     ]
    }
   ],
   "source": [
    "# Count all rows with quantity ordered negative\n",
    "print(data_orders['quantity'].lt(0).sum())\n",
    "\n",
    "print(data_orders['quantity'].eq(150000).sum())\n",
    "# 6 rows with negative quantity... prob wrong.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9676afc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [purchase_order_id, purchase_order_item_no_order, quantity, delivery_date, product_id_order, product_version, created_date_time, modified_date_time, unit_id, unit, status_id, status, rm_id, product_id_transport, purchase_order_item_no_transport, receival_item_no, batch_id, transporter_name, vehicle_no, unit_status, vehicle_start_weight, vehicle_end_weight, gross_weight, tare_weight, net_weight, wood, ironbands, plastic, water, ice, other, chips, packaging, cardboard]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "# I want to check the transportation of the 5 orders in the head\n",
    "data_transport = pd.read_csv('data/extended/transportation.csv')\n",
    "\n",
    "data = pd.merge(data_orders.head(5), data_transport, on='purchase_order_id', suffixes=('_order', '_transport'))\n",
    "\n",
    "print(data)\n",
    "\n",
    "# Observation: Not all orders are transported either...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4c110812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to check the material details of the 5 orders in the head\n",
    "# NVM... cooked... orders have nothing to directly link to materials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ff303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OKAY! Let's try to drop all the orders with no recievals maybe? And try to predict? But in a real scenario I probably shouldn't\n",
    "# Cause maybe the orders with no recievals are equal to 0 recieved? But I don't know if that's true. Gotta test\n",
    "# So try 2 stuff: 1. drop the orders with no recievals, 2. set the recievals to 0 if no recievals\n",
    "\n",
    "# But first I neeed to know what my model will predict? Like will I get orders and recievals? Or just predict by the order prev?\n",
    "# Okay I don't think I'll get more orders in the future, so I guess I just have to predict based on previous orders\n",
    "\n",
    "# Purchase orders have an expected delivery_date though.\n",
    "# They are using YYYY-MM-DD format I guess\n",
    "# They want from 2025-01-01 to 2025-05-31\n",
    "# We got some deliveries expected in 2025-03-XX, but none after, so we prob need to predict that there will be more orders.\n",
    "\n",
    "# By making a model that predicts the quantity ordered based on previous orders, I can then use that to predict future orders\n",
    "# I should prob make a model for each of the materials and then sum them up for each order date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b23cab61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(122590, 20)\n",
      "(122537, 20)\n",
      "       purchase_order_id  purchase_order_item_no  quantity delivery_date  \\\n",
      "61798                NaN                     NaN       NaN           NaN   \n",
      "63356                NaN                     NaN       NaN           NaN   \n",
      "64105                NaN                     NaN       NaN           NaN   \n",
      "65448                NaN                     NaN       NaN           NaN   \n",
      "71981                NaN                     NaN       NaN           NaN   \n",
      "\n",
      "       product_id_order  product_version created_date_time modified_date_time  \\\n",
      "61798               NaN              NaN               NaN                NaN   \n",
      "63356               NaN              NaN               NaN                NaN   \n",
      "64105               NaN              NaN               NaN                NaN   \n",
      "65448               NaN              NaN               NaN                NaN   \n",
      "71981               NaN              NaN               NaN                NaN   \n",
      "\n",
      "       unit_id unit  status_id status  rm_id  product_id_receival  \\\n",
      "61798      NaN  NaN        NaN    NaN    NaN                  NaN   \n",
      "63356      NaN  NaN        NaN    NaN    NaN                  NaN   \n",
      "64105      NaN  NaN        NaN    NaN    NaN                  NaN   \n",
      "65448      NaN  NaN        NaN    NaN    NaN                  NaN   \n",
      "71981      NaN  NaN        NaN    NaN    NaN                  NaN   \n",
      "\n",
      "       receival_item_no  batch_id                date_arrival receival_status  \\\n",
      "61798                 0       NaN  2015-08-04 12:44:00 +02:00       Completed   \n",
      "63356                 0       NaN  2015-11-19 11:32:00 +01:00       Completed   \n",
      "64105                 0       NaN  2016-01-08 14:48:00 +01:00       Completed   \n",
      "65448                 0       NaN  2016-03-16 09:31:00 +01:00       Completed   \n",
      "71981                 0       NaN  2017-03-29 11:42:00 +02:00       Completed   \n",
      "\n",
      "       net_weight  supplier_id  \n",
      "61798         NaN        64997  \n",
      "63356         NaN        53377  \n",
      "64105         NaN        64997  \n",
      "65448         NaN        10001  \n",
      "71981         NaN        54008  \n"
     ]
    }
   ],
   "source": [
    "# Starting by dropping the orders with no recievals\n",
    "data = pd.merge(\n",
    "    data_orders,\n",
    "    data_receivals,\n",
    "    on=['purchase_order_id', 'purchase_order_item_no'],\n",
    "    suffixes=('_order', '_receival')\n",
    ")\n",
    "\n",
    "# 122537 rows, but recievals has 122590 rows. So some recievals are from orders not in the orders dataset?\n",
    "data_extra_receivals = pd.merge(\n",
    "    data_orders,\n",
    "    data_receivals,\n",
    "    on=['purchase_order_id', 'purchase_order_item_no'],\n",
    "    suffixes=('_order', '_receival'),\n",
    "    how='right'\n",
    ")\n",
    "\n",
    "print(data_extra_receivals.shape)\n",
    "print(data.shape)\n",
    "# 122591 rows, so 54 extra recievals that are not in the orders dataset\n",
    "# Let's check if they are all from the same purchase_order_id\n",
    "\n",
    "# I want the data_extra_receivals rows that are not in data\n",
    "data_diff = pd.concat([data_extra_receivals, data]).drop_duplicates(keep=False)\n",
    "print(data_diff.head(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "25f5057e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "# Check recievals with no purchase order id or purchase order item no\n",
    "print(data_receivals['purchase_order_id'].isna().sum())\n",
    "print(data_receivals['purchase_order_item_no'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b53ccbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay let me first try using the data with recievals and where the recievals can be linked to purchase\n",
    "data = pd.merge(\n",
    "    data_orders,\n",
    "    data_receivals,\n",
    "    on=['purchase_order_id', 'purchase_order_item_no'],\n",
    "    suffixes=('_order', '_receival'),\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b699d0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Completed' 'Finished unloading' 'Planned' 'Start unloading']\n",
      "receival_status\n",
      "Completed             122448\n",
      "Finished unloading       106\n",
      "Start unloading           32\n",
      "Planned                    4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data/kernel/receivals.csv')\n",
    "print(data['receival_status'].unique())\n",
    "print(data['receival_status'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea1ef6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Found out that some materials cease to be ordered after some time. Maybe they are obsolete?\n",
    "# Some dates use different time zones than others\n",
    "# Some units are in KG, LBs and pounds --> Need to make them to KG and drop that column\n",
    "# Some materials stock are deleted in the materials file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03cee73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   purchase_order_id  purchase_order_item_no  quantity  \\\n",
      "0                  1                       1     -14.0   \n",
      "1                 22                       1   23880.0   \n",
      "\n",
      "              delivery_date  product_id  product_version  \\\n",
      "0 2003-05-12 00:00:00+02:00    91900143                1   \n",
      "1 2003-05-27 00:00:00+02:00    91900160                1   \n",
      "\n",
      "          created_date_time        modified_date_time  status_id  status  \n",
      "0 2003-05-12 12:00:48+02:00 2004-06-15 08:16:18+02:00          2  Closed  \n",
      "1 2003-05-27 14:42:07+02:00 2012-06-29 11:41:13+02:00          2  Closed  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "############## CLEANING THE PURCHASE ORDERS DATA ##############\n",
    "\n",
    "orders = pd.read_csv(\"./data/kernel/purchase_orders.csv\")\n",
    "\n",
    "# Make the orders with PUND in KGs, and change quantity accordingly\n",
    "# 1 PUND = 0,45359237 kilogram\n",
    "orders.loc[orders['unit'] == 'PUND', 'quantity'] = orders.loc[orders['unit'] == 'PUND', 'quantity'] * 0.45359237\n",
    "# Change the unit to KG too: orders.loc[orders['unit'] == 'PUND', 'unit'] = 'KG'\n",
    "# Drop unit_id and unit columns\n",
    "orders = orders.drop(columns=['unit_id', 'unit'])\n",
    "\n",
    "# Time is in GMT+2 which is Norway time\n",
    "# Make delivery_date, created_date_time and modified_date_time to GMT +2\n",
    "orders['delivery_date'] = pd.to_datetime(orders['delivery_date'], utc=True).dt.tz_convert('Etc/GMT-2')\n",
    "orders['created_date_time'] = pd.to_datetime(orders['created_date_time'], utc=True).dt.tz_convert('Etc/GMT-2')\n",
    "orders['modified_date_time'] = pd.to_datetime(orders['modified_date_time'], utc=True).dt.tz_convert('Etc/GMT-2')\n",
    "\n",
    "# Save the cleaned data to a new CSV file in data_cleaned folder\n",
    "#orders.to_csv('./data_cleaned/purchase_orders_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd05ba47",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CLEANING THE RECEIVALS DATA ###\n",
    "receivals = pd.read_csv(\"./data/kernel/receivals.csv\")\n",
    "\n",
    "# Make the date_arrival to GMT +2\n",
    "receivals['date_arrival'] = pd.to_datetime(receivals['date_arrival'], utc=True).dt.tz_convert('Etc/GMT-2')\n",
    "# Save the cleaned data to a new CSV file in data_cleaned folder\n",
    "receivals.to_csv('./data_cleaned/receivals_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96051806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38297, 13)\n"
     ]
    }
   ],
   "source": [
    "# METHOD 1: Merge orders and receivals, group them by orders and aggregate the recievals\n",
    "# Then merge the aggregated recievals into orders. THIS IS NOT IN USE RN\n",
    "import pandas as pd\n",
    "\n",
    "orders = pd.read_csv(\"./data_cleaned/purchase_orders_cleaned.csv\", parse_dates=[\"delivery_date\", \"created_date_time\", \"modified_date_time\"])\n",
    "receivals = pd.read_csv(\"./data_cleaned/receivals_cleaned.csv\", parse_dates=[\"date_arrival\"])\n",
    "\n",
    "# --- Aggregate receivals per order line ---\n",
    "order_receivals = receivals.groupby(\n",
    "    [\"purchase_order_id\", \"purchase_order_item_no\", \"rm_id\"]\n",
    ").agg(\n",
    "    total_received_qty=(\"net_weight\", \"sum\"),\n",
    "    first_receival_date=(\"date_arrival\", \"min\")\n",
    ").reset_index()\n",
    "\n",
    "# --- Merge into orders ---\n",
    "orders = orders.merge(\n",
    "    order_receivals,\n",
    "    on=[\"purchase_order_id\", \"purchase_order_item_no\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "print(orders.shape)\n",
    "\n",
    "# --- Fill missing values for undelivered orders ---\n",
    "orders[\"total_received_qty\"] = orders[\"total_received_qty\"].fillna(0)\n",
    "orders[\"first_receival_date\"] = pd.to_datetime(orders[\"first_receival_date\"])\n",
    "\n",
    "# --- Derived features ---\n",
    "orders[\"fill_fraction\"] = orders[\"total_received_qty\"] / orders[\"quantity\"]\n",
    "orders[\"lead_time\"] = (orders[\"first_receival_date\"] - orders[\"delivery_date\"]).dt.days\n",
    "orders[\"lead_time\"] = orders[\"lead_time\"].fillna(0)\n",
    "\n",
    "# --- Save the final cleaned and merged dataset ---\n",
    "#orders.to_csv('./data_cleaned/orders_with_receivals.csv', index=False)\n",
    "orders.to_csv('./data_cleaned/rm_idGROUPED.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8136f941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(133409, 20)\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Merge orders and receivals directly, then aggregate the recievals per order line\n",
    "# This will create duplicate rows for orders with multiple recievals, but we can aggregate them\n",
    "\n",
    "# --- Load data ---\n",
    "orders = pd.read_csv(\n",
    "    \"./data_cleaned/purchase_orders_cleaned.csv\",\n",
    "    parse_dates=[\"delivery_date\", \"created_date_time\", \"modified_date_time\"]\n",
    ")\n",
    "receivals = pd.read_csv(\n",
    "    \"./data_cleaned/receivals_cleaned.csv\",\n",
    "    parse_dates=[\"date_arrival\"]\n",
    ")\n",
    "\n",
    "# --- Merge orders and receivals WITHOUT aggregation ---\n",
    "orders_with_receivals = orders.merge(\n",
    "    receivals,\n",
    "    on=[\"purchase_order_id\", \"purchase_order_item_no\"],\n",
    "    how=\"left\",\n",
    "    suffixes=('_order', '_receival')\n",
    ")\n",
    "\n",
    "# --- Fill missing values for orders with no receivals ---\n",
    "orders_with_receivals[\"net_weight\"] = orders_with_receivals[\"net_weight\"].fillna(0)\n",
    "orders_with_receivals[\"date_arrival\"] = pd.to_datetime(orders_with_receivals[\"date_arrival\"])\n",
    "\n",
    "# --- Derived features ---\n",
    "orders_with_receivals[\"fill_fraction\"] = orders_with_receivals[\"net_weight\"] / orders_with_receivals[\"quantity\"]\n",
    "orders_with_receivals[\"lead_time\"] = (\n",
    "    orders_with_receivals[\"date_arrival\"] - orders_with_receivals[\"delivery_date\"]\n",
    ").dt.days\n",
    "orders_with_receivals[\"lead_time\"] = orders_with_receivals[\"lead_time\"].fillna(0)\n",
    "\n",
    "# --- Save result ---\n",
    "#orders_with_receivals.to_csv(\"./data_cleaned/orders_with_receivals_detailed.csv\", index=False)\n",
    "\n",
    "print(orders_with_receivals.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bfb3ceae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27410\n",
      "10887\n",
      "(38297, 15)\n"
     ]
    }
   ],
   "source": [
    "orders_merged = pd.read_csv(\"./data_cleaned/orders_with_receivals.csv\")\n",
    "\n",
    "# print how many orders have value for total_received_qty that is different from 0\n",
    "print((orders_merged['total_received_qty'] != 0).sum())\n",
    "print((orders_merged['total_received_qty'] == 0).sum())\n",
    "print(orders_merged.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0b5f0e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11026\n",
      "(133409, 20)\n"
     ]
    }
   ],
   "source": [
    "# Check how many orders with no receivals\n",
    "print((orders_with_receivals['net_weight'] == 0).sum())\n",
    "print(orders_with_receivals.shape)\n",
    "# 122 591 rows, so 54 extra recievals that are not in the orders dataset\n",
    "# 11 026 orders with no receivals\n",
    "# 122 537 + 11 026 = 133 563\n",
    "# So it doesn't make sense that I get 133 409 rows when merging\n",
    "# Errr... whatever for now I guess\n",
    "# Ehhrm why do I get 10 887 above and 11 026 here? errrm...\n",
    "\n",
    "# TODO: FIND OUT WHY THE DIFFERENCE IN ROWS WHEN MERGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "595a376f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rm_id  avg_fill_fraction  avg_lead_time  avg_weekly_order_qty\n",
      "0  342.0           0.479615           23.0             42.573099\n",
      "1  343.0           0.004804         -642.0           3708.771930\n"
     ]
    }
   ],
   "source": [
    "# Okay now use orders_with_receivals_detailed.csv to make a model that predicts net_weight based on previous orders\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "orders = pd.read_csv(\"./data_cleaned/purchase_orders_cleaned.csv\", parse_dates=[\"delivery_date\", \"created_date_time\", \"modified_date_time\"])\n",
    "receivals = pd.read_csv(\"./data_cleaned/receivals_cleaned.csv\", parse_dates=[\"date_arrival\"])\n",
    "orders_with_receivals = pd.read_csv(\"./data_cleaned/orders_with_receivals_detailed.csv\", parse_dates=[\"delivery_date\", \"created_date_time\", \"modified_date_time\", \"date_arrival\"])\n",
    "\n",
    "# Make sure dates are tz-naive for calculations\n",
    "orders_with_receivals['date_arrival'] = orders_with_receivals['date_arrival'].dt.tz_convert(None)\n",
    "orders_with_receivals['delivery_date'] = orders_with_receivals['delivery_date'].dt.tz_convert(None)\n",
    "orders_with_receivals['created_date_time'] = orders_with_receivals['created_date_time'].dt.tz_convert(None)\n",
    "orders_with_receivals['modified_date_time'] = orders_with_receivals['modified_date_time'].dt.tz_convert(None)\n",
    "\n",
    "# Mean fill_fraction, lead_time and weekly order quantity per material\n",
    "\n",
    "material_stats = orders_with_receivals.groupby('rm_id').agg(\n",
    "    avg_fill_fraction=('fill_fraction', 'mean'),\n",
    "    avg_lead_time=('lead_time', 'mean'),\n",
    "    avg_weekly_order_qty=('quantity', lambda x: x.sum() / ((orders_with_receivals['delivery_date'].max() - orders_with_receivals['delivery_date'].min()).days / 7))\n",
    ").reset_index()\n",
    "\n",
    "print(material_stats.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6415fa7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19430, 20)\n",
      "(6340, 20)\n"
     ]
    }
   ],
   "source": [
    "# But when I think closely. Why don't I make a model that trains on\n",
    "# data up to 2013 and predicts 2014? Cause then I can actually see if it works\n",
    "# So I need to split the data into train and test based on date\n",
    "\n",
    "# Sidenote: I just thought about something for transportation: Mby some transporter names are more reliable than others?\n",
    "# Like some transporters always deliver on time, while others are late. They prob get better as time goes on too?\n",
    "\n",
    "# Okay let's start with splitting the data into train and test based on date\n",
    "train_data = orders_with_receivals[orders_with_receivals['delivery_date'] < '2024-01-01']\n",
    "test_data = orders_with_receivals[orders_with_receivals['delivery_date'] >= '2024-01-01']\n",
    "\n",
    "# The thing is I have so much data from previous years so I feel like I will overfit if I use all of it\n",
    "# So I will use only the most recent 3 years of data for training? Hmm some years we got events like\n",
    "# COVID or NM in skiing that might make some years different too\n",
    "\n",
    "train_data = train_data[train_data['delivery_date'] >= '2021-01-01']\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3f2bea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_11480\\1262774136.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['rm_id'] = X_train['rm_id'].astype('category')\n",
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_11480\\1262774136.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test['rm_id'] = X_test['rm_id'].astype('category')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000807 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 971\n",
      "[LightGBM] [Info] Number of data points in the train set: 6203, number of used features: 7\n",
      "[LightGBM] [Info] Start training from score 10093.000000\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's quantile: 32609.4\tvalid_1's quantile: 32665.8\n",
      "[100]\ttraining's quantile: 27325.5\tvalid_1's quantile: 25683.3\n",
      "[150]\ttraining's quantile: 20760.4\tvalid_1's quantile: 19496.3\n",
      "[200]\ttraining's quantile: 13914\tvalid_1's quantile: 17042.8\n",
      "[250]\ttraining's quantile: 12347.8\tvalid_1's quantile: 16252.5\n",
      "[300]\ttraining's quantile: 11880.1\tvalid_1's quantile: 16019.3\n",
      "[350]\ttraining's quantile: 11567.5\tvalid_1's quantile: 15620.5\n",
      "[400]\ttraining's quantile: 11341.8\tvalid_1's quantile: 15580.6\n",
      "[450]\ttraining's quantile: 11127.7\tvalid_1's quantile: 15337.9\n",
      "[500]\ttraining's quantile: 10823.5\tvalid_1's quantile: 15082.8\n",
      "[550]\ttraining's quantile: 10548.8\tvalid_1's quantile: 14952.3\n",
      "[600]\ttraining's quantile: 10414.9\tvalid_1's quantile: 14808.8\n",
      "[650]\ttraining's quantile: 10322.3\tvalid_1's quantile: 14732.7\n",
      "[700]\ttraining's quantile: 10235.4\tvalid_1's quantile: 14685.8\n",
      "[750]\ttraining's quantile: 10154.8\tvalid_1's quantile: 14718.8\n",
      "Early stopping, best iteration is:\n",
      "[727]\ttraining's quantile: 10195.3\tvalid_1's quantile: 14675.9\n",
      "Quantile Loss 0.2 on 2024 data (model): 14675.912117498572\n",
      "Quantile Loss 0.2 on 2024 data (all zeros): 46992.55287671233\n",
      "       rm_id       delivery_date  quantity  avg_lead_time  avg_fill_fraction  \\\n",
      "1483  2123.0 2024-10-08 22:00:00   12500.0            2.0                1.0   \n",
      "1544  2124.0 2024-01-03 23:00:00    3240.0            4.0                1.0   \n",
      "\n",
      "      received_qty  day_of_week  week_of_year  month  \\\n",
      "1483       12500.0            1            41     10   \n",
      "1544        3240.0            2             1      1   \n",
      "\n",
      "      forecast_received_qty_0_2  forecast_cum_qty_0_2  \n",
      "1483               11308.634400          11308.634400  \n",
      "1544                3020.013467           3020.013467  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_11480\\1262774136.py:86: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['forecast_received_qty_0_2'] = model_q.predict(X_test)\n",
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_11480\\1262774136.py:89: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['forecast_cum_qty_0_2'] = test_df.groupby('rm_id')['forecast_received_qty_0_2'].cumsum()\n"
     ]
    }
   ],
   "source": [
    "### TRYING TO PREDICT 2024 BY USING EARLIER YEARS\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Prepare data\n",
    "# -----------------------------\n",
    "# Make sure datetime columns are correct\n",
    "orders_with_receivals['delivery_date'] = pd.to_datetime(orders_with_receivals['delivery_date'])\n",
    "orders_with_receivals['date_arrival'] = pd.to_datetime(orders_with_receivals['date_arrival'])\n",
    "\n",
    "# Fill missing values for lead_time and fill_fraction\n",
    "orders_with_receivals['lead_time'] = orders_with_receivals['lead_time'].fillna(orders_with_receivals['lead_time'].mean())\n",
    "orders_with_receivals['fill_fraction'] = orders_with_receivals['fill_fraction'].fillna(orders_with_receivals['fill_fraction'].mean())\n",
    "\n",
    "# Compute received_qty (target)\n",
    "orders_with_receivals['received_qty'] = orders_with_receivals['net_weight']\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Aggregate daily per rm_id\n",
    "# -----------------------------\n",
    "daily_data = orders_with_receivals.groupby(\n",
    "    ['rm_id', 'delivery_date']\n",
    ").agg(\n",
    "    quantity=('quantity', 'sum'),\n",
    "    avg_lead_time=('lead_time', 'mean'),\n",
    "    avg_fill_fraction=('fill_fraction', 'mean'),\n",
    "    received_qty=('received_qty', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Extract temporal features\n",
    "daily_data['day_of_week'] = daily_data['delivery_date'].dt.dayofweek\n",
    "daily_data['week_of_year'] = daily_data['delivery_date'].dt.isocalendar().week\n",
    "daily_data['month'] = daily_data['delivery_date'].dt.month\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Train/test split\n",
    "# -----------------------------\n",
    "train_df = daily_data[daily_data['delivery_date'] < '2024-01-01']\n",
    "test_df  = daily_data[(daily_data['delivery_date'] >= '2024-01-01') & (daily_data['delivery_date'] < '2025-01-01')]\n",
    "\n",
    "features = ['rm_id', 'quantity', 'avg_lead_time', 'avg_fill_fraction', 'day_of_week', 'week_of_year', 'month']\n",
    "target = 'received_qty'\n",
    "\n",
    "X_train = train_df[features]\n",
    "y_train = train_df[target]\n",
    "X_test = test_df[features]\n",
    "y_test = test_df[target]\n",
    "\n",
    "# Convert rm_id to categorical\n",
    "X_train['rm_id'] = X_train['rm_id'].astype('category')\n",
    "X_test['rm_id'] = X_test['rm_id'].astype('category')\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Train LightGBM for 0.2 Quantile Regression\n",
    "# -----------------------------\n",
    "train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=['rm_id'])\n",
    "val_data = lgb.Dataset(X_test, label=y_test, reference=train_data, categorical_feature=['rm_id'])\n",
    "\n",
    "params_quantile = {\n",
    "    'objective': 'quantile',\n",
    "    'alpha': 0.2,  # 0.2 quantile\n",
    "    'metric': 'quantile',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 31,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "model_q = lgb.train(\n",
    "    params_quantile,\n",
    "    train_data,\n",
    "    valid_sets=[train_data, val_data],\n",
    "    num_boost_round=10000,\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=50),\n",
    "        lgb.log_evaluation(period=50)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Predict 2024 daily received quantities\n",
    "# -----------------------------\n",
    "test_df['forecast_received_qty_0_2'] = model_q.predict(X_test)\n",
    "\n",
    "# Compute cumulative forecast per rm_id\n",
    "test_df['forecast_cum_qty_0_2'] = test_df.groupby('rm_id')['forecast_received_qty_0_2'].cumsum()\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Compute 0.2 Quantile Loss for model\n",
    "# -----------------------------\n",
    "def quantile_loss_0_2(actual, forecast):\n",
    "    return np.mean(np.maximum(0.2*(actual - forecast), 0.8*(forecast - actual)))\n",
    "\n",
    "qloss_model = quantile_loss_0_2(y_test.values, test_df['forecast_received_qty_0_2'].values)\n",
    "print(\"Quantile Loss 0.2 on 2024 data (model):\", qloss_model)\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Compare against zeros baseline\n",
    "# -----------------------------\n",
    "zero_forecast = np.zeros_like(y_test.values)\n",
    "qloss_zero = quantile_loss_0_2(y_test.values, zero_forecast)\n",
    "print(\"Quantile Loss 0.2 on 2024 data (all zeros):\", qloss_zero)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Inspect forecast\n",
    "# -----------------------------\n",
    "print(test_df.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4a1ac27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000633 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 938\n",
      "[LightGBM] [Info] Number of data points in the train set: 4105, number of used features: 7\n",
      "[LightGBM] [Info] Start training from score 12172.000977\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[50]\ttraining's quantile: 28548.7\n",
      "[100]\ttraining's quantile: 22746\n",
      "[150]\ttraining's quantile: 15419.7\n",
      "[200]\ttraining's quantile: 10614.7\n",
      "[250]\ttraining's quantile: 8962.28\n",
      "[300]\ttraining's quantile: 8472.58\n",
      "[350]\ttraining's quantile: 8201.08\n",
      "[400]\ttraining's quantile: 7967.16\n",
      "[450]\ttraining's quantile: 7636.87\n",
      "[500]\ttraining's quantile: 7501.64\n",
      "[550]\ttraining's quantile: 7322.02\n",
      "[600]\ttraining's quantile: 7229.18\n",
      "[650]\ttraining's quantile: 7104.31\n",
      "[700]\ttraining's quantile: 6978.08\n",
      "[750]\ttraining's quantile: 6888.69\n",
      "[800]\ttraining's quantile: 6788.24\n",
      "[850]\ttraining's quantile: 6714\n",
      "[900]\ttraining's quantile: 6649.14\n",
      "[950]\ttraining's quantile: 6584.17\n",
      "[1000]\ttraining's quantile: 6527.36\n",
      "Quantile Loss 0.2 between model forecast and all zeros: 29305.259097306425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_11480\\1748483986.py:92: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  submission['forecast_cum_qty_0_2'] = submission.groupby('rm_id')['forecast_received_qty_0_2'].cumsum()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### TRYING TO PREDICT 2025 BY USING EARLIER YEARS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Prepare data\n",
    "# -----------------------------\n",
    "# Ensure datetime columns\n",
    "orders_with_receivals['delivery_date'] = pd.to_datetime(orders_with_receivals['delivery_date'])\n",
    "orders_with_receivals['date_arrival'] = pd.to_datetime(orders_with_receivals['date_arrival'])\n",
    "\n",
    "# Fill missing values\n",
    "orders_with_receivals['lead_time'] = orders_with_receivals['lead_time'].fillna(orders_with_receivals['lead_time'].mean())\n",
    "orders_with_receivals['fill_fraction'] = orders_with_receivals['fill_fraction'].fillna(orders_with_receivals['fill_fraction'].mean())\n",
    "orders_with_receivals['received_qty'] = orders_with_receivals['net_weight']\n",
    "\n",
    "# Filter historical data from 2014 onward\n",
    "hist_data = orders_with_receivals[orders_with_receivals['delivery_date'] >= '2014-01-01'].copy()\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Aggregate daily per rm_id\n",
    "# -----------------------------\n",
    "daily_data = hist_data.groupby(['rm_id', 'delivery_date']).agg(\n",
    "    quantity=('quantity', 'sum'),\n",
    "    avg_lead_time=('lead_time', 'mean'),\n",
    "    avg_fill_fraction=('fill_fraction', 'mean'),\n",
    "    received_qty=('received_qty', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Temporal features\n",
    "daily_data['day_of_week'] = daily_data['delivery_date'].dt.dayofweek\n",
    "daily_data['week_of_year'] = daily_data['delivery_date'].dt.isocalendar().week\n",
    "daily_data['month'] = daily_data['delivery_date'].dt.month\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Prepare features for training\n",
    "# -----------------------------\n",
    "features = ['rm_id', 'quantity', 'avg_lead_time', 'avg_fill_fraction', 'day_of_week', 'week_of_year', 'month']\n",
    "target = 'received_qty'\n",
    "\n",
    "X_train = daily_data[features].copy()\n",
    "y_train = daily_data[target]\n",
    "\n",
    "X_train['rm_id'] = X_train['rm_id'].astype('category')\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Train LightGBM 0.2 Quantile Regression\n",
    "# -----------------------------\n",
    "train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=['rm_id'])\n",
    "\n",
    "params_quantile = {\n",
    "    'objective': 'quantile',\n",
    "    'alpha': 0.2,\n",
    "    'metric': 'quantile',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 31,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "model_q = lgb.train(\n",
    "    params_quantile,\n",
    "    train_data,\n",
    "    num_boost_round=1000,\n",
    "    valid_sets=[train_data],\n",
    "    callbacks=[lgb.log_evaluation(period=50)]\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Generate future submission\n",
    "# -----------------------------\n",
    "future_dates = pd.date_range('2025-01-01', '2025-05-31', freq='D')\n",
    "rm_ids = daily_data['rm_id'].unique()\n",
    "\n",
    "submission = pd.MultiIndex.from_product([rm_ids, future_dates], names=['rm_id', 'delivery_date']).to_frame(index=False)\n",
    "\n",
    "# Fill features using historical averages per rm_id\n",
    "submission['quantity'] = submission['rm_id'].map(daily_data.groupby('rm_id')['quantity'].mean())\n",
    "submission['avg_lead_time'] = submission['rm_id'].map(daily_data.groupby('rm_id')['avg_lead_time'].mean())\n",
    "submission['avg_fill_fraction'] = submission['rm_id'].map(daily_data.groupby('rm_id')['avg_fill_fraction'].mean())\n",
    "submission['day_of_week'] = submission['delivery_date'].dt.dayofweek\n",
    "submission['week_of_year'] = submission['delivery_date'].dt.isocalendar().week\n",
    "submission['month'] = submission['delivery_date'].dt.month\n",
    "\n",
    "submission['rm_id'] = submission['rm_id'].astype('category')\n",
    "\n",
    "# Predict\n",
    "submission['forecast_received_qty_0_2'] = model_q.predict(submission[features])\n",
    "\n",
    "# Cumulative forecast per rm_id\n",
    "submission['forecast_cum_qty_0_2'] = submission.groupby('rm_id')['forecast_received_qty_0_2'].cumsum()\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Inspect sample\n",
    "# -----------------------------\n",
    "submission['zero_forecast'] = 0\n",
    "def quantile_loss_0_2(actual, forecast):\n",
    "    return np.mean(np.maximum(0.2*(actual - forecast), 0.8*(forecast - actual)))\n",
    "\n",
    "qloss_zero = quantile_loss_0_2(submission['forecast_received_qty_0_2'].values, submission['zero_forecast'].values)\n",
    "print(\"Quantile Loss 0.2 between model forecast and all zeros:\", qloss_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2d4aa093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rm_id delivery_date  quantity  avg_lead_time  avg_fill_fraction  \\\n",
      "0  355.0    2025-01-01  250000.0           -2.0            0.09888   \n",
      "1  355.0    2025-01-02  250000.0           -2.0            0.09888   \n",
      "2  355.0    2025-01-03  250000.0           -2.0            0.09888   \n",
      "\n",
      "   day_of_week  week_of_year  month  forecast_received_qty_0_2  \\\n",
      "0            2             1      1               20750.417123   \n",
      "1            3             1      1               21402.471634   \n",
      "2            4             1      1               19804.558908   \n",
      "\n",
      "   forecast_cum_qty_0_2  zero_forecast  \n",
      "0          20750.417123              0  \n",
      "1          42152.888756              0  \n",
      "2          61957.447664              0  \n"
     ]
    }
   ],
   "source": [
    "print(submission.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f367e58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ID  predicted_weight forecast_end_date\n",
      "0    1               0.0        2025-01-02\n",
      "1    2               0.0        2025-01-03\n",
      "2    3               0.0        2025-01-04\n",
      "3    4               0.0        2025-01-05\n",
      "4    5               0.0        2025-01-06\n",
      "5    6               0.0        2025-01-07\n",
      "6    7               0.0        2025-01-08\n",
      "7    8               0.0        2025-01-09\n",
      "8    9               0.0        2025-01-10\n",
      "9   10               0.0        2025-01-11\n",
      "10  11               0.0        2025-01-12\n",
      "11  12               0.0        2025-01-13\n",
      "12  13               0.0        2025-01-14\n",
      "13  14               0.0        2025-01-15\n",
      "14  15               0.0        2025-01-16\n",
      "15  16               0.0        2025-01-17\n",
      "16  17               0.0        2025-01-18\n",
      "17  18               0.0        2025-01-19\n",
      "18  19               0.0        2025-01-20\n",
      "19  20               0.0        2025-01-21\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 'submission' = your daily forecast DataFrame\n",
    "# columns: ['rm_id', 'delivery_date', 'forecast_received_qty_0_2', ...]\n",
    "\n",
    "# Load sample_submission and mapping\n",
    "sample_submission = pd.read_csv(\"./data/sample_submission.csv\")  # columns: ID, predicted_weight (empty)\n",
    "mapping = pd.read_csv(\"./data/prediction_mapping.csv\")  # columns: ID, rm_id, forecast_start_date, forecast_end_date\n",
    "\n",
    "mapping['forecast_start_date'] = pd.to_datetime(mapping['forecast_start_date'])\n",
    "mapping['forecast_end_date'] = pd.to_datetime(mapping['forecast_end_date'])\n",
    "\n",
    "# Aggregate daily forecasts into predicted_weight\n",
    "def aggregate_forecast(row):\n",
    "    mask = (\n",
    "        (submission['rm_id'] == row['rm_id']) &\n",
    "        (submission['delivery_date'] >= row['forecast_start_date']) &\n",
    "        (submission['delivery_date'] <= row['forecast_end_date'])\n",
    "    )\n",
    "    return submission.loc[mask, 'forecast_received_qty_0_2'].sum()\n",
    "\n",
    "mapping['predicted_weight'] = mapping.apply(aggregate_forecast, axis=1)\n",
    "\n",
    "# Merge back into sample_submission by ID\n",
    "sample_submission = sample_submission.drop(columns=['predicted_weight']).merge(\n",
    "    mapping[['ID','predicted_weight']], on='ID', how='left'\n",
    ")\n",
    "\n",
    "# Make filled sample_submission predicted_weight * 0.001\n",
    "sample_submission['predicted_weight'] = sample_submission['predicted_weight'] * 0.001\n",
    "\n",
    "# I want to make earlier predictions more conservative\n",
    "# So I will multiply the predictions for Jan by 0.01, Feb by 0.02, Mar by 0.5, Apr by 0.95, May by 1.0\n",
    "\n",
    "# Merge forecast_end_date into sample_submission\n",
    "sample_submission = sample_submission.merge(\n",
    "    mapping[['ID', 'forecast_end_date']],\n",
    "    on='ID',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "def adjust_for_month(row):\n",
    "    month = row['forecast_end_date'].month\n",
    "    if month == 1:\n",
    "        return row['predicted_weight'] * 0.01\n",
    "    elif month == 2:\n",
    "        return row['predicted_weight'] * 0.02\n",
    "    elif month == 3:\n",
    "        return row['predicted_weight'] * 0.5\n",
    "    elif month == 4:\n",
    "        return row['predicted_weight'] * 0.95\n",
    "    elif month == 5:\n",
    "        return row['predicted_weight'] * 1.0\n",
    "    return row['predicted_weight']\n",
    "\n",
    "sample_submission['predicted_weight'] = sample_submission.apply(adjust_for_month, axis=1)\n",
    "\n",
    "# Save ready-to-submit CSV\n",
    "sample_submission.to_csv(\"filled_sample_submission.csv\", index=False)\n",
    "print(sample_submission.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d31ccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "### I WANT TO MAKE EVERY RM_ID in the submission have 0 predicted weight if they are in materials.csv and the stock is deleted?\n",
    "import pandas as pd\n",
    "# 'submission' = your daily forecast DataFrame\n",
    "# columns: ['rm_id', 'delivery_date', 'forecast_received_qty_0_2\n",
    "# Load materials data\n",
    "materials = pd.read_csv(\"./data/extended/materials.csv\")\n",
    "# Filter materials with stock_deleted = True\n",
    "deleted_materials = materials[materials['stock_deleted'] == True]['rm_id'].unique()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
