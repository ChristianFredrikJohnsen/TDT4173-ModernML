{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "85a52037",
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTS ###\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import (\n",
    "    optimize_hyperparameters,\n",
    ")\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5fc93ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## CLEANING THE PURCHASE ORDERS DATA ##############\n",
    "\n",
    "orders = pd.read_csv(\"../data/kernel/purchase_orders.csv\")\n",
    "\n",
    "# Time is in GMT+2 which is Norway time\n",
    "# Make delivery_date, created_date_time and modified_date_time to GMT +2\n",
    "orders['delivery_date'] = pd.to_datetime(orders['delivery_date'], utc=True).dt.tz_convert('Etc/GMT-2')\n",
    "orders['created_date_time'] = pd.to_datetime(orders['created_date_time'], utc=True).dt.tz_convert('Etc/GMT-2')\n",
    "orders['modified_date_time'] = pd.to_datetime(orders['modified_date_time'], utc=True).dt.tz_convert('Etc/GMT-2')\n",
    "\n",
    "\n",
    "################# CLEANING THE RECEIVALS DATA ########################\n",
    "receivals = pd.read_csv(\"../data/kernel/receivals.csv\")\n",
    "\n",
    "# Make the date_arrival to GMT +2\n",
    "receivals['date_arrival'] = pd.to_datetime(receivals['date_arrival'], utc=True).dt.tz_convert('Etc/GMT-2')\n",
    "\n",
    "\n",
    "############### MERGE ORDERS AND RECEIVALS DATA ###########################\n",
    "# --- Merge orders and receivals WITHOUT aggregation ---\n",
    "orders_with_receivals = orders.merge(\n",
    "    receivals,\n",
    "    on=[\"purchase_order_id\", \"purchase_order_item_no\"],\n",
    "    how=\"left\",\n",
    "    suffixes=('_order', '_receival')\n",
    ")\n",
    "\n",
    "# --- Fill missing values for orders with no receivals ---\n",
    "orders_with_receivals[\"net_weight\"] = orders_with_receivals[\"net_weight\"].fillna(0)\n",
    "orders_with_receivals[\"date_arrival\"] = pd.to_datetime(orders_with_receivals[\"date_arrival\"])\n",
    "\n",
    "\n",
    "# Make the orders with PUND in KGs, and change quantity accordingly\n",
    "# 1 PUND = 0,45359237 kilogram\n",
    "orders_with_receivals.loc[orders_with_receivals['unit'] == 'PUND', 'quantity'] = orders_with_receivals.loc[orders_with_receivals['unit'] == 'PUND', 'net_weight'] * 0.45359237\n",
    "# Change the unit to KG too: orders_with_receivals.loc[orders_with_receivals['unit'] == 'PUND', 'unit'] = 'KG'\n",
    "# Drop unit_id and unit columns\n",
    "orders_with_receivals = orders_with_receivals.drop(columns=['unit_id', 'unit'])\n",
    "\n",
    "# --- Derived features ---\n",
    "orders_with_receivals[\"fill_fraction\"] = orders_with_receivals[\"net_weight\"] / orders_with_receivals[\"quantity\"]\n",
    "orders_with_receivals[\"lead_time\"] = (\n",
    "    orders_with_receivals[\"date_arrival\"] - orders_with_receivals[\"delivery_date\"]\n",
    ").dt.days\n",
    "orders_with_receivals[\"lead_time\"] = orders_with_receivals[\"lead_time\"].fillna(0)\n",
    "\n",
    "\n",
    "####################### SELECT RELEVANT COLUMNS FROM THE MERGED DATAFRAME ##################################\n",
    "orders_with_receivals = orders_with_receivals[orders_with_receivals['rm_id'].notnull() & orders_with_receivals['date_arrival'].notnull()]\n",
    "# date_arrival = actual date of receival, delivery_date = expected date of receival\n",
    "# lead_time = date_arrival - delivery_date\n",
    "# quantity  = quantity, net_weight = weight in kg (the actual target per day etc)\n",
    "selected = orders_with_receivals[[\"rm_id\", \"date_arrival\", \"net_weight\", \"supplier_id\", \"delivery_date\", \"product_id_receival\", \"quantity\", \"lead_time\"]]\n",
    "# Filter out the selected rows where rm_id is null or date_arrival is null\n",
    "selected = selected[selected['rm_id'].notnull() & selected['date_arrival'].notnull()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0b1537a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### CREATING TIME_IDX AND AGGREGATING TO DAILY LEVEL AND FILLING GAPS WITH 0 NET_WEIGHT RECEIVALS ############################\n",
    "# make a copy and normalize date_arrival to date-only (drop time) so grouping is by year-month-day\n",
    "df_agg = selected.copy()\n",
    "# ensure date_arrival is a datetime and floor to day (sets time to 00:00:00)\n",
    "df_agg['date_arrival'] = df_agg['date_arrival'].dt.floor('D')\n",
    "# Remove timezone info if present\n",
    "df_agg['date_arrival'] = df_agg['date_arrival'].dt.tz_localize(None)\n",
    "\n",
    "df_agg = df_agg.groupby(['rm_id', 'date_arrival']).agg({\n",
    "    'net_weight': 'sum',\n",
    "    'quantity': 'sum',\n",
    "}).reset_index()\n",
    "\n",
    "# Add time_idx based on days since each rm_id's minimum date\n",
    "df_agg = df_agg.sort_values(['rm_id', 'date_arrival'])\n",
    "df_agg['local_time_idx'] = (df_agg['date_arrival'] - df_agg.groupby('rm_id')['date_arrival'].transform('min')).dt.days\n",
    "\n",
    "# Fill gaps from each rm_id's min date to 2024-12-31 with 0 net_weight entries\n",
    "end_date = pd.Timestamp('2024-12-31')\n",
    "all_filled = []\n",
    "\n",
    "for rm_id, group in df_agg.groupby('rm_id'):\n",
    "    min_date = group['date_arrival'].min()\n",
    "    max_idx = (end_date - min_date).days\n",
    "    \n",
    "    full_range = pd.DataFrame({\n",
    "        'local_time_idx': range(0, max_idx + 1)\n",
    "    })\n",
    "    full_range['rm_id'] = rm_id\n",
    "    full_range['date_arrival'] = min_date + pd.to_timedelta(full_range['local_time_idx'], unit='D')\n",
    "    \n",
    "    merged = pd.merge(full_range, group, on=['rm_id', 'local_time_idx', 'date_arrival'], how='left')\n",
    "    merged['net_weight'] = merged['net_weight'].fillna(0)\n",
    "    merged['quantity'] = merged['quantity'].fillna(0)\n",
    "  \n",
    "    all_filled.append(merged)\n",
    "\n",
    "df_agg = pd.concat(all_filled, ignore_index=True)\n",
    "selected_with_local_time = df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f7b4196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### ADD ADDITIONAL FEATURES ##################################\n",
    "# Add additional features\n",
    "selected_with_local_time[\"month\"] = selected_with_local_time[\"date_arrival\"].dt.month.astype(str).astype(\"category\")\n",
    "selected_with_local_time[\"year\"] = selected_with_local_time[\"date_arrival\"].dt.year.astype(str).astype(\"category\")\n",
    "selected_with_local_time[\"day_of_week\"] = selected_with_local_time[\"date_arrival\"].dt.dayofweek.astype(str).astype(\"category\")  # 0=Monday, 6=Sunday\n",
    "selected_with_local_time[\"log_weight\"] = np.log1p(selected_with_local_time[\"net_weight\"])\n",
    "\n",
    "# Norwegian special days/holidays\n",
    "# Fixed holidays\n",
    "def get_norwegian_holidays(year):\n",
    "    \"\"\"Return dictionary of Norwegian holidays for a given year\"\"\"\n",
    "    from datetime import timedelta\n",
    "    \n",
    "    holidays = {}\n",
    "    \n",
    "    # Fixed date holidays\n",
    "    holidays[f'{year}-01-01'] = 'New Year'\n",
    "    holidays[f'{year}-05-01'] = 'Labour Day'\n",
    "    holidays[f'{year}-05-17'] = 'Constitution Day'\n",
    "    holidays[f'{year}-12-24'] = 'Christmas Eve'\n",
    "    holidays[f'{year}-12-25'] = 'Christmas Day'\n",
    "    holidays[f'{year}-12-26'] = 'Boxing Day'\n",
    "    holidays[f'{year}-12-31'] = 'New Year Eve'\n",
    "    \n",
    "    # Easter-based holidays (Easter dates vary each year)\n",
    "    # Approximate Easter calculation (Meeus/Jones/Butcher algorithm)\n",
    "    a = year % 19\n",
    "    b = year // 100\n",
    "    c = year % 100\n",
    "    d = b // 4\n",
    "    e = b % 4\n",
    "    f = (b + 8) // 25\n",
    "    g = (b - f + 1) // 3\n",
    "    h = (19 * a + b - d - g + 15) % 30\n",
    "    i = c // 4\n",
    "    k = c % 4\n",
    "    l = (32 + 2 * e + 2 * i - h - k) % 7\n",
    "    m = (a + 11 * h + 22 * l) // 451\n",
    "    month = (h + l - 7 * m + 114) // 31\n",
    "    day = ((h + l - 7 * m + 114) % 31) + 1\n",
    "    \n",
    "    easter = pd.Timestamp(year=year, month=month, day=day)\n",
    "    \n",
    "    # Easter-related holidays\n",
    "    holidays[(easter - timedelta(days=3)).strftime('%Y-%m-%d')] = 'Maundy Thursday'\n",
    "    holidays[(easter - timedelta(days=2)).strftime('%Y-%m-%d')] = 'Good Friday'\n",
    "    holidays[easter.strftime('%Y-%m-%d')] = 'Easter Sunday'\n",
    "    holidays[(easter + timedelta(days=1)).strftime('%Y-%m-%d')] = 'Easter Monday'\n",
    "    holidays[(easter + timedelta(days=39)).strftime('%Y-%m-%d')] = 'Ascension Day'\n",
    "    holidays[(easter + timedelta(days=49)).strftime('%Y-%m-%d')] = 'Whit Sunday'\n",
    "    holidays[(easter + timedelta(days=50)).strftime('%Y-%m-%d')] = 'Whit Monday'\n",
    "    \n",
    "    return holidays\n",
    "\n",
    "# Create a mapping of all dates to holidays\n",
    "all_holidays = {}\n",
    "for year in range(selected_with_local_time['date_arrival'].dt.year.min(), \n",
    "                  selected_with_local_time['date_arrival'].dt.year.max() + 1):\n",
    "    all_holidays.update(get_norwegian_holidays(year))\n",
    "\n",
    "# Add special day column\n",
    "selected_with_local_time['date_str'] = selected_with_local_time['date_arrival'].dt.strftime('%Y-%m-%d')\n",
    "selected_with_local_time['special_days'] = selected_with_local_time['date_str'].map(all_holidays).fillna('none').astype('category')\n",
    "selected_with_local_time.drop('date_str', axis=1, inplace=True)\n",
    "\n",
    "# Add binary flag for whether it's a holiday\n",
    "selected_with_local_time['is_holiday'] = (selected_with_local_time['special_days'] != 'none').astype(int)\n",
    "\n",
    "special_days = list(all_holidays.values())\n",
    "\n",
    "# Make rm_id a string instead of numeric\n",
    "selected_with_local_time[\"rm_id\"] = selected_with_local_time[\"rm_id\"].astype(int).astype(str).astype(\"category\")\n",
    "selected_with_local_time[\"is_holiday\"] = selected_with_local_time[\"is_holiday\"].astype(str).astype(\"category\")\n",
    "selected_with_local_time.drop(\"year\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "920fb7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### CREATE TIME SERIES DATASET FOR PYTORCH FORECASTING ##################################\n",
    "full_data = selected_with_local_time.copy()\n",
    "\n",
    "max_prediction_length = 151\n",
    "max_encoder_length = 365\n",
    "# V: training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    # V: data[lambda x: x.local_time_idx <= training_cutoff],\n",
    "    data = full_data,\n",
    "    time_idx=\"local_time_idx\",\n",
    "    target=\"net_weight\",\n",
    "    group_ids=[\"rm_id\"],\n",
    "    min_encoder_length=max_encoder_length\n",
    "    // 2,  # keep encoder length long (as it is in the validation set)\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"rm_id\"],\n",
    "    #static_reals= no static real yet,\n",
    "    time_varying_known_categoricals=[\"special_days\", \"month\", \"day_of_week\", \"is_holiday\"],\n",
    "    #variable_groups={\n",
    "    #    \"special_days\": special_days\n",
    "    #},  # group of categorical variables can be treated as one variable\n",
    "    time_varying_known_reals=[\"local_time_idx\"],\n",
    "    # CAN PUT YEAR IN TIME_VARYING_KNOWN_REALS\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"quantity\",\n",
    "        \"net_weight\",\n",
    "        \"log_weight\",\n",
    "    ],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"rm_id\"], transformation=\"softplus\"\n",
    "    ),  # use softplus and normalize by group\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "# create validation set (predict=True) which means to predict the last max_prediction_length points in time\n",
    "# for each series\n",
    "#V: validation = TimeSeriesDataSet.from_dataset(\n",
    "#V:    training, data, predict=True, stop_randomization=True\n",
    "#V:)\n",
    "\n",
    "# create dataloaders for model\n",
    "batch_size = 128  # set this between 32 to 128\n",
    "train_dataloader = training.to_dataloader(\n",
    "    train=True, batch_size=batch_size, num_workers=0\n",
    ")\n",
    "#V: val_dataloader = validation.to_dataloader(\n",
    "#V:    train=False, batch_size=batch_size * 10, num_workers=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e64a45f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 10.5k\n"
     ]
    }
   ],
   "source": [
    "################# DECIDING ON THE MODEL AND TRAINER PARAMETERS ##########################\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "# configure network and trainer\n",
    "pl.seed_everything(42)\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator=\"gpu\",\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=30,\n",
    "    #fast_dev_run = True,\n",
    "    callbacks=[lr_logger],\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    # not meaningful for finding the learning rate but otherwise very important\n",
    "    learning_rate=0.01,\n",
    "    hidden_size=8,  # most important hyperparameter apart from learning rate\n",
    "    # number of attention heads. Set to up to 4 for large datasets\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,  # between 0.1 and 0.3 are good values\n",
    "    hidden_continuous_size=8,  # set to <= hidden_size\n",
    "    loss=QuantileLoss(),\n",
    "    #optimizer=\"ranger\", OPTIMIZER FOR FINDING BEST LEARNING RATE\n",
    "    # reduce learning rate if no improvement in validation loss after x epochs\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size() / 1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cbbcc302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1.8 K  | train\n",
      "3  | prescalers                         | ModuleDict                      | 128    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 1.2 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 2.5 K  | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.2 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 304    | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 304    | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 304    | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 304    | train\n",
      "11 | lstm_encoder                       | LSTM                            | 576    | train\n",
      "12 | lstm_decoder                       | LSTM                            | 576    | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 144    | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 16     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 368    | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 280    | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 160    | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 304    | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 160    | train\n",
      "20 | output_layer                       | Linear                          | 63     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "10.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "10.5 K    Total params\n",
      "0.042     Total estimated model params size (MB)\n",
      "303       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:09<00:00,  3.27it/s, v_num=1, train_loss_step=731.0, train_loss_epoch=584.0]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:09<00:00,  3.25it/s, v_num=1, train_loss_step=731.0, train_loss_epoch=584.0]\n"
     ]
    }
   ],
   "source": [
    "######## TRAINING THE MODEL ##########\n",
    "\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8dca98da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction dataframe shape: (101473, 10)\n",
      "Date range: 2024-01-02 00:00:00 to 2025-05-31 00:00:00\n",
      "Time index range: 0 to 7655\n",
      "\n",
      "First few rows:\n",
      "   local_time_idx rm_id date_arrival  net_weight  quantity month day_of_week  \\\n",
      "0            7132   342   2024-01-02         0.0       0.0     1           1   \n",
      "1            7133   342   2024-01-03         0.0       0.0     1           2   \n",
      "2            7134   342   2024-01-04         0.0       0.0     1           3   \n",
      "3            7135   342   2024-01-05         0.0       0.0     1           4   \n",
      "4            7136   342   2024-01-06         0.0       0.0     1           5   \n",
      "\n",
      "   log_weight special_days is_holiday  \n",
      "0         0.0         none          0  \n",
      "1         0.0         none          0  \n",
      "2         0.0         none          0  \n",
      "3         0.0         none          0  \n",
      "4         0.0         none          0  \n"
     ]
    }
   ],
   "source": [
    "################# FULLL PREDICTION FOR ALL RM_IDs (NEED ENCODER AND DECODER DATA) ###############\n",
    "\n",
    "\n",
    "rm_ids = full_data['rm_id'].unique().tolist()\n",
    "predict_data = []\n",
    "# Create prediction date range\n",
    "pred_start = pd.Timestamp('2025-01-01')\n",
    "pred_end = pd.Timestamp('2025-05-31')\n",
    "pred_dates = pd.date_range(start=pred_start, end=pred_end, freq='D')\n",
    "\n",
    "all_predict_dfs = []\n",
    "\n",
    "for rm_id in rm_ids:\n",
    "    test_rm_id = rm_id  # must match categorical rm_id type\n",
    "    historical = full_data[full_data['rm_id'] == test_rm_id].copy()\n",
    "    if historical.empty:\n",
    "        continue\n",
    "    min_date = historical['date_arrival'].min()\n",
    "\n",
    "    # build prediction rows for this rm_id\n",
    "    rows = []\n",
    "    for date in pred_dates:\n",
    "        time_idx = (date - min_date).days\n",
    "        date_str = date.strftime('%Y-%m-%d')\n",
    "        special_day = all_holidays.get(date_str, 'none')\n",
    "        is_holiday = '1' if special_day != 'none' else '0'\n",
    "\n",
    "        rows.append({\n",
    "            'rm_id': test_rm_id,\n",
    "            'date_arrival': date,\n",
    "            'local_time_idx': time_idx,\n",
    "            'month': str(date.month),\n",
    "            'day_of_week': str(date.dayofweek),\n",
    "            'special_days': special_day,\n",
    "            'is_holiday': is_holiday,\n",
    "            'net_weight': 0,   # placeholder\n",
    "            'quantity': 0,     # placeholder\n",
    "            'log_weight': 0    # placeholder\n",
    "        })\n",
    "\n",
    "    pred_df_rm = pd.DataFrame(rows)\n",
    "\n",
    "    # encoder/context data (last max_encoder_length days)\n",
    "    encoder_data = historical.tail(max_encoder_length).copy()\n",
    "    encoder_data['local_time_idx'] = encoder_data['local_time_idx'].astype(int)\n",
    "\n",
    "    # combine encoder + prediction for this rm_id and collect\n",
    "    combined = pd.concat([encoder_data, pred_df_rm], ignore_index=True)\n",
    "    all_predict_dfs.append(combined)\n",
    "\n",
    "# final combined prediction dataframe for all rm_ids\n",
    "predict_data = pd.concat(all_predict_dfs, ignore_index=True)\n",
    "\n",
    "predict_data = pd.DataFrame(predict_data)\n",
    "\n",
    "# Convert to categorical to match training data\n",
    "predict_data['rm_id'] = predict_data['rm_id'].astype(str).astype('category')\n",
    "predict_data['month'] = predict_data['month'].astype(str).astype('category')\n",
    "predict_data['day_of_week'] = predict_data['day_of_week'].astype(str).astype('category')\n",
    "predict_data['special_days'] = predict_data['special_days'].astype(str).astype('category')\n",
    "predict_data['is_holiday'] = predict_data['is_holiday'].astype(str).astype('category')\n",
    "\n",
    "# Ensure local_time_idx is integer (required by TimeSeriesDataSet)\n",
    "predict_data['local_time_idx'] = predict_data['local_time_idx'].astype(int)\n",
    "\n",
    "print(f\"Prediction dataframe shape: {predict_data.shape}\")\n",
    "print(f\"Date range: {predict_data['date_arrival'].min()} to {predict_data['date_arrival'].max()}\")\n",
    "print(f\"Time index range: {predict_data['local_time_idx'].min()} to {predict_data['local_time_idx'].max()}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(predict_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6305fef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "################## MAKE PREDICTIONS #######################\n",
    "\n",
    "predictions = tft.predict(predict_data, return_x=True, return_index=True, return_decoder_lengths=True)\n",
    "\n",
    "ltdx_and_rmid = predictions.index\n",
    "\n",
    "output = predictions.output\n",
    "\n",
    "pred = []\n",
    "\n",
    "pred_start = pd.Timestamp('2025-01-01')\n",
    "pred_end = pd.Timestamp('2025-05-31')\n",
    "pred_dates = pd.date_range(start=pred_start, end=pred_end, freq='D')\n",
    "\n",
    "\n",
    "for rm_id_index in range(0,192):\n",
    "    rm_id_test = ltdx_and_rmid[\"rm_id\"][rm_id_index]\n",
    "    ltdx_test = ltdx_and_rmid[\"local_time_idx\"][rm_id_index]\n",
    "    for date in pred_dates:\n",
    "        pred_weight = output[rm_id_index][(date-pred_start).days].item()\n",
    "        pred.append({\n",
    "            \"rm_id\": rm_id_test,\n",
    "            \"local_time_idx\": ltdx_test,\n",
    "            \"date\": date,\n",
    "            \"predicted_weight\": pred_weight\n",
    "        })\n",
    "\n",
    "\n",
    "pred = pd.DataFrame(pred)\n",
    "pred_over_0 = pred[pred[\"predicted_weight\"]>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a277ef90",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## PREPARING THE SUBMISSION FILE #######################\n",
    "sample_submission = pd.read_csv(\"../data/sample_submission.csv\")\n",
    "prediction_mapping = pd.read_csv(\"../data/prediction_mapping.csv\", parse_dates=[\"forecast_start_date\", \"forecast_end_date\"])\n",
    "\n",
    "submission = sample_submission.merge(prediction_mapping, on=\"ID\")\n",
    "submission[\"forecast_end_date\"] = pd.to_datetime(submission[\"forecast_end_date\"])\n",
    "submission[\"forecast_start_date\"] = pd.to_datetime(submission[\"forecast_start_date\"])\n",
    "\n",
    "for p in pred_over_0.itertuples():\n",
    "    rm_id = p.rm_id\n",
    "    date_arrival = p.date.replace(tzinfo=None)\n",
    "    predicted_weight = p.predicted_weight\n",
    "    submission.loc[\n",
    "        (submission['rm_id'] == int(rm_id)) & (submission['forecast_end_date'] >= date_arrival),\n",
    "        'predicted_weight'\n",
    "    ] += predicted_weight*0.8  # applying a scaling factor of 0.8 to predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1ce8a994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL PREDICTED WEIGHTS PER RM_ID WITH THE ML MODEL:\n",
      "     rm_id  predicted_weight\n",
      "75    2130      7.143864e+06\n",
      "180   3865      5.190772e+06\n",
      "176   3781      4.608997e+06\n",
      "151   3126      3.721015e+06\n",
      "150   3125      2.529422e+06\n",
      "147   3122      2.082348e+06\n",
      "149   3124      1.840926e+06\n",
      "160   3282      1.600152e+06\n",
      "148   3123      1.590398e+06\n",
      "182   3901      9.931108e+05\n",
      "85    2142      5.116467e+04\n",
      "163   3421      2.996941e+04\n",
      "27     387      8.345096e-33\n",
      "47    1872      4.180805e-39\n"
     ]
    }
   ],
   "source": [
    "############## PRINT THE MODEL's TOTAL PREDICTIONS PER RM_ID ##############\n",
    "filtered = submission.copy()\n",
    "\n",
    "agg_df = filtered.groupby(\"rm_id\", as_index=False).agg({\n",
    "    \"predicted_weight\": \"max\",\n",
    "}).sort_values(\"predicted_weight\", ascending=False)\n",
    "\n",
    "\n",
    "print(\"TOTAL PREDICTED WEIGHTS PER RM_ID WITH THE ML MODEL:\")\n",
    "print(agg_df[agg_df[\"predicted_weight\"]>0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999c28b7",
   "metadata": {},
   "source": [
    "# BENEATH IS THE FALLBACK METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d9a00132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deprecated RM IDs: [2123.0, 2124.0, 2140.0, 2147.0, 2981.0, 3121.0, 3142.0, 3265.0, 3581.0, 3642.0, 3761.0, 4021.0, 4044.0]\n"
     ]
    }
   ],
   "source": [
    "################### FOR SOME RM_IDS THAT THE MODEL DIDN'T PREDICT MAKE FALLBACKS ###########################\n",
    "\n",
    "receivals_2024 = orders_with_receivals[orders_with_receivals['date_arrival'].dt.year == 2024]\n",
    "receivals_2024 = receivals_2024[['rm_id', 'date_arrival', 'net_weight']]\n",
    "receivals_2023 = orders_with_receivals[orders_with_receivals['date_arrival'].dt.year == 2023]\n",
    "receivals_2023 = receivals_2023[['rm_id', 'date_arrival', 'net_weight']]\n",
    "\n",
    "\n",
    "########## FINDING WHICH RM_IDS ARE DEPRECATED BASED ON RECEIVALS DATES IN 2023 AND 2024 ##################\n",
    "# I want to group the receivals by rm_id and get the latest date_arrival for each rm_id. If receivals_2024 end before month and day in receivals_2023 by 40 days, then I want to consider that rm_id as deprecated.\n",
    "latest_2024 = receivals_2024.groupby('rm_id')['date_arrival'].max().reset_index()\n",
    "latest_2023 = receivals_2023.groupby('rm_id')['date_arrival'].max().reset_index()\n",
    "\n",
    "# I want to keep all 2024 even if they are not in 2023\n",
    "merged_latest = latest_2023.merge(latest_2024, on='rm_id', suffixes=('_2023', '_2024'), how='right')\n",
    "# You need to make the years the same in date arrival before calculating the difference in days\n",
    "merged_latest['date_arrival_2023'] = merged_latest['date_arrival_2023'].apply(lambda x: x.replace(year=2024) if pd.notna(x) else x)\n",
    "merged_latest['date_diff'] = (merged_latest['date_arrival_2023'] - merged_latest['date_arrival_2024']).dt.days\n",
    "deprecated_rm_ids = merged_latest[merged_latest['date_diff'] >40]['rm_id'].tolist()\n",
    "print(f\"Deprecated RM IDs: {deprecated_rm_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "32fce081",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# FIND HOW MUCH WE WANT TO SCALE THE RM_IDS USING 2023 to 2024 AS A BASELINE ##################\n",
    "# For both receivals_2023 and receivals_2024, I want to drop the rm_ids that are in deprecated_rm_ids\n",
    "receivals_2023 = receivals_2023[~receivals_2023['rm_id'].isin(deprecated_rm_ids)]\n",
    "receivals_2024 = receivals_2024[~receivals_2024['rm_id'].isin(deprecated_rm_ids)]\n",
    "\n",
    "# Now I want to group by rm_id and sum net_weight for each rm_id by 2024-05-31\n",
    "receivals_2024_grouped = receivals_2024[receivals_2024['date_arrival'] <= '2024-05-31'].groupby('rm_id')['net_weight'].sum().reset_index()\n",
    "receivals_2023_grouped = receivals_2023[receivals_2023['date_arrival'] <= '2024-05-31'].groupby('rm_id')['net_weight'].sum().reset_index()\n",
    "receivals_comparison = receivals_2023_grouped.merge(receivals_2024_grouped, on='rm_id', suffixes=('_2023', '_2024'), how='right')\n",
    "receivals_comparison['weight_scale'] = receivals_comparison['net_weight_2024']/receivals_comparison['net_weight_2023']\n",
    "# Make the weight which are 1 or more to be 1\n",
    "receivals_comparison.loc[receivals_comparison['weight_scale'] >= 1, 'weight_scale'] = 1\n",
    "# Fill weight scale nans with 0.7\n",
    "receivals_comparison['weight_scale'] = receivals_comparison['weight_scale'].fillna(0.7)\n",
    "# Make the weight_scale that are bigger than 0.7 to 0.7\n",
    "receivals_comparison.loc[receivals_comparison['weight_scale'] > 0.7, 'weight_scale'] = 0.7\n",
    "\n",
    "concluding_fallbacks = set(receivals_comparison[\"rm_id\"].unique())\n",
    "scale_mapping = dict(zip(receivals_comparison['rm_id'], receivals_comparison['weight_scale']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "69966482",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2024 = sample_submission.merge(prediction_mapping, on=\"ID\")\n",
    "# For every forecast_start_date and forecast_end_date in submission make it 2024 instead of 2025\n",
    "test2024['forecast_start_date'] = test2024['forecast_start_date'].apply(lambda x: x.replace(year=2024))\n",
    "test2024['forecast_end_date'] = test2024['forecast_end_date'].apply(lambda x: x.replace(year=2024))\n",
    "\n",
    "\n",
    "############# CREATING A 2024 RECEIVALS SUBMISSION WITH THE DESIRED RM_IDS and SCALE ##################\n",
    "# THIS IS METHOD 2 - MORE CONSERVATIVE - ONLY ADD IF RECEIVAL IS 2 DAYS BEFORE FORECAST END DATE \n",
    "test2024['predicted_weight'] = test2024['predicted_weight'].astype(float)\n",
    "\n",
    "for receival in receivals_2024.itertuples():\n",
    "    rm_id = receival.rm_id\n",
    "    date_arrival = receival.date_arrival\n",
    "    net_weight = receival.net_weight\n",
    "    date_arrival_naive = date_arrival.replace(tzinfo=None)\n",
    "    mask = (\n",
    "        (test2024['rm_id'] == rm_id) &\n",
    "        (test2024['forecast_end_date'] >= (date_arrival_naive+pd.Timedelta(days=2)))\n",
    "    )\n",
    "    # If rm_id is in scale_mapping + apply the scale, else don't do anything\n",
    "    if rm_id in scale_mapping:\n",
    "        test2024.loc[mask, 'predicted_weight'] += net_weight*scale_mapping[rm_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e9512d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RM IDs needing fallbacks: {3201.0, 3601.0, 3362.0, 3621.0, 4263.0, 3883.0, 2741.0, 3381.0, 4161.0, 2125.0, 4302.0, 2129.0, 2131.0, 2132.0, 2133.0, 2134.0, 2135.0, 2143.0, 2144.0, 2145.0, 2161.0, 4081.0, 4222.0}\n"
     ]
    }
   ],
   "source": [
    "######## FIND WHICH RM_IDS THAT NEEDS FALLBACK AND DO FALLBACKS ON THE SUBMISSION FILE #######################\n",
    "\n",
    "def do_fallback_on(submission, fallbacks):\n",
    "    for rm_id in fallbacks:\n",
    "        # Find IDs corresponding to this rm_id\n",
    "        ids = prediction_mapping[prediction_mapping[\"rm_id\"] == rm_id][\"ID\"].tolist()\n",
    "        for id in ids:\n",
    "            # Put predicted_weight from test2024 into submission\n",
    "            weight = test2024[test2024[\"ID\"] == id][\"predicted_weight\"]\n",
    "            submission.loc[submission[\"ID\"] == id, \"predicted_weight\"] = weight\n",
    "\n",
    "best = submission.copy()\n",
    "\n",
    "best_rmids = best.copy()\n",
    "best_rmids = best_rmids[best_rmids[\"predicted_weight\"]>0]\n",
    "best_rmids = best_rmids[[\"ID\", \"predicted_weight\", \"rm_id\"]]\n",
    "\n",
    "\n",
    "best_rmids = best_rmids.groupby(\"rm_id\", as_index=False).agg({\n",
    "        \"predicted_weight\": \"max\",\n",
    "        }).sort_values(\"predicted_weight\", ascending=False)\n",
    "\n",
    "best_rmids = set(best_rmids[\"rm_id\"].unique())\n",
    "\n",
    "fallbacks_to_do = concluding_fallbacks - best_rmids\n",
    "print(f\"RM IDs needing fallbacks: {fallbacks_to_do}\")\n",
    "# do fallbacks\n",
    "\n",
    "do_fallback_on(best, fallbacks_to_do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "710db606",
   "metadata": {},
   "outputs": [],
   "source": [
    "### REMOVE SMALL WEIGHTS BELOW 1 KG FROM THE FINAL SUBMISSION INTO 0 ####\n",
    "best.loc[best[\"predicted_weight\"]<1, \"predicted_weight\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "697aff7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL PREDICTED WEIGHTS PER RM_ID WITH THE ML MODEL + FALLBACKS:\n",
      "     rm_id  predicted_weight\n",
      "75    2130      7.143864e+06\n",
      "180   3865      5.190772e+06\n",
      "176   3781      4.608997e+06\n",
      "151   3126      3.721015e+06\n",
      "150   3125      2.529422e+06\n",
      "147   3122      2.082348e+06\n",
      "149   3124      1.840926e+06\n",
      "160   3282      1.600152e+06\n",
      "148   3123      1.590398e+06\n",
      "182   3901      9.931108e+05\n",
      "79    2134      2.976199e+05\n",
      "80    2135      2.874560e+05\n",
      "181   3883      1.355480e+05\n",
      "136   2741      1.156820e+05\n",
      "88    2145      1.135763e+05\n",
      "86    2143      1.123164e+05\n",
      "76    2131      1.099434e+05\n",
      "87    2144      8.351958e+04\n",
      "161   3362      6.725600e+04\n",
      "190   4222      5.194000e+04\n",
      "85    2142      5.116467e+04\n",
      "77    2132      3.579853e+04\n",
      "187   4081      3.409000e+04\n",
      "163   3421      2.996941e+04\n",
      "74    2129      2.591660e+04\n",
      "191   4263      1.747200e+04\n",
      "156   3201      1.678600e+04\n",
      "192   4302      1.509200e+04\n",
      "162   3381      1.421107e+04\n",
      "78    2133      1.386660e+04\n",
      "171   3621      1.017752e+04\n",
      "71    2125      8.714567e+03\n",
      "189   4161      2.044000e+03\n",
      "103   2161      1.440000e+03\n",
      "170   3601      5.948701e+02\n"
     ]
    }
   ],
   "source": [
    "### PRINT THE FINAL MODEL + FALLBACKS TOTAL PREDICTIONS PER RM_ID ##############\n",
    "print(\"TOTAL PREDICTED WEIGHTS PER RM_ID WITH THE ML MODEL + FALLBACKS:\")\n",
    "agg_df2 = best.copy().groupby(\"rm_id\", as_index=False).agg({\n",
    "    \"predicted_weight\": \"max\",\n",
    "}).sort_values(\"predicted_weight\", ascending=False)\n",
    "\n",
    "print(agg_df2[agg_df2[\"predicted_weight\"]>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "46b17dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = best[[\"ID\", \"predicted_weight\"]]\n",
    "best.to_csv(\"FULL_PIPELINE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33beeea8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
