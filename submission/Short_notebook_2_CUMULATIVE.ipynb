{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85a52037",
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTS ###\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import (\n",
    "    optimize_hyperparameters,\n",
    ")\n",
    "import pandas as pd\n",
    "# Norwegian special days/holidays using the holidays package\n",
    "import holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5fc93ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## CLEANING THE PURCHASE ORDERS DATA ##############\n",
    "\n",
    "orders = pd.read_csv(\"../data/kernel/purchase_orders.csv\")\n",
    "\n",
    "# Time is in GMT+2 which is Norway time\n",
    "# Make delivery_date, created_date_time and modified_date_time to GMT +2\n",
    "orders['delivery_date'] = pd.to_datetime(orders['delivery_date'], utc=True).dt.tz_convert('Etc/GMT-2')\n",
    "orders['created_date_time'] = pd.to_datetime(orders['created_date_time'], utc=True).dt.tz_convert('Etc/GMT-2')\n",
    "orders['modified_date_time'] = pd.to_datetime(orders['modified_date_time'], utc=True).dt.tz_convert('Etc/GMT-2')\n",
    "\n",
    "\n",
    "################# CLEANING THE RECEIVALS DATA ########################\n",
    "receivals = pd.read_csv(\"../data/kernel/receivals.csv\")\n",
    "\n",
    "# Make the date_arrival to GMT +2\n",
    "receivals['date_arrival'] = pd.to_datetime(receivals['date_arrival'], utc=True).dt.tz_convert('Etc/GMT-2')\n",
    "\n",
    "\n",
    "############### MERGE ORDERS AND RECEIVALS DATA ###########################\n",
    "# --- Merge orders and receivals WITHOUT aggregation ---\n",
    "orders_with_receivals = orders.merge(\n",
    "    receivals,\n",
    "    on=[\"purchase_order_id\", \"purchase_order_item_no\"],\n",
    "    how=\"left\",\n",
    "    suffixes=('_order', '_receival')\n",
    ")\n",
    "\n",
    "# --- Fill missing values for orders with no receivals ---\n",
    "orders_with_receivals[\"net_weight\"] = orders_with_receivals[\"net_weight\"].fillna(0)\n",
    "orders_with_receivals[\"date_arrival\"] = pd.to_datetime(orders_with_receivals[\"date_arrival\"])\n",
    "\n",
    "\n",
    "# Make the orders with PUND in KGs, and change quantity accordingly\n",
    "# 1 PUND = 0,45359237 kilogram\n",
    "orders_with_receivals.loc[orders_with_receivals['unit'] == 'PUND', 'quantity'] = orders_with_receivals.loc[orders_with_receivals['unit'] == 'PUND', 'quantity'] * 0.45359237\n",
    "orders_with_receivals.loc[orders_with_receivals['unit'] == 'PUND', 'net_weight'] = orders_with_receivals.loc[orders_with_receivals['unit'] == 'PUND', 'net_weight'] * 0.45359237\n",
    "# Change the unit to KG too: orders_with_receivals.loc[orders_with_receivals['unit'] == 'PUND', 'unit'] = 'KG'\n",
    "# Drop unit_id and unit columns\n",
    "orders_with_receivals = orders_with_receivals.drop(columns=['unit_id', 'unit'])\n",
    "\n",
    "# --- Derived features ---\n",
    "orders_with_receivals[\"fill_fraction\"] = orders_with_receivals[\"net_weight\"] / orders_with_receivals[\"quantity\"]\n",
    "orders_with_receivals[\"lead_time\"] = (\n",
    "    orders_with_receivals[\"date_arrival\"] - orders_with_receivals[\"delivery_date\"]\n",
    ").dt.days\n",
    "orders_with_receivals[\"lead_time\"] = orders_with_receivals[\"lead_time\"].fillna(0)\n",
    "\n",
    "\n",
    "####################### SELECT RELEVANT COLUMNS FROM THE MERGED DATAFRAME ##################################\n",
    "orders_with_receivals = orders_with_receivals[orders_with_receivals['rm_id'].notnull() & orders_with_receivals['date_arrival'].notnull()]\n",
    "# date_arrival = actual date of receival, delivery_date = expected date of receival\n",
    "# lead_time = date_arrival - delivery_date\n",
    "# quantity  = quantity, net_weight = weight in kg (the actual target per day etc)\n",
    "selected = orders_with_receivals[[\"rm_id\", \"date_arrival\", \"net_weight\", \"supplier_id\", \"delivery_date\", \"product_id_receival\", \"quantity\", \"lead_time\"]]\n",
    "# Filter out the selected rows where rm_id is null or date_arrival is null\n",
    "selected = selected[selected['rm_id'].notnull() & selected['date_arrival'].notnull()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b1537a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### CREATING TIME_IDX AND AGGREGATING TO DAILY LEVEL AND FILLING GAPS WITH 0 NET_WEIGHT RECEIVALS ############################\n",
    "# make a copy and normalize date_arrival to date-only (drop time) so grouping is by year-month-day\n",
    "df_agg = selected.copy()\n",
    "# ensure date_arrival is a datetime and floor to day (sets time to 00:00:00)\n",
    "df_agg['date_arrival'] = df_agg['date_arrival'].dt.floor('D')\n",
    "# Remove timezone info if present\n",
    "df_agg['date_arrival'] = df_agg['date_arrival'].dt.tz_localize(None)\n",
    "\n",
    "df_agg = df_agg.groupby(['rm_id', 'date_arrival']).agg({\n",
    "    'net_weight': 'sum',\n",
    "    'quantity': 'sum',\n",
    "}).reset_index()\n",
    "\n",
    "# Add time_idx based on days since each rm_id's minimum date\n",
    "df_agg = df_agg.sort_values(['rm_id', 'date_arrival'])\n",
    "df_agg['local_time_idx'] = (df_agg['date_arrival'] - df_agg.groupby('rm_id')['date_arrival'].transform('min')).dt.days\n",
    "\n",
    "# Fill gaps from each rm_id's min date to 2024-12-31 with 0 net_weight entries\n",
    "end_date = pd.Timestamp('2024-12-31')\n",
    "all_filled = []\n",
    "\n",
    "for rm_id, group in df_agg.groupby('rm_id'):\n",
    "    min_date = group['date_arrival'].min()\n",
    "    max_idx = (end_date - min_date).days\n",
    "    \n",
    "    full_range = pd.DataFrame({\n",
    "        'local_time_idx': range(0, max_idx + 1)\n",
    "    })\n",
    "    full_range['rm_id'] = rm_id\n",
    "    full_range['date_arrival'] = min_date + pd.to_timedelta(full_range['local_time_idx'], unit='D')\n",
    "    \n",
    "    merged = pd.merge(full_range, group, on=['rm_id', 'local_time_idx', 'date_arrival'], how='left')\n",
    "    merged['net_weight'] = merged['net_weight'].fillna(0)\n",
    "    merged['quantity'] = merged['quantity'].fillna(0)\n",
    "  \n",
    "    all_filled.append(merged)\n",
    "\n",
    "df_agg = pd.concat(all_filled, ignore_index=True)\n",
    "selected_with_local_time = df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "932b1816",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CUMULATIVE TESTING#################\n",
    "\n",
    "# For rm_ids that start in 2024 set their local_time_idx to start from 0 and fill 2024-01-01 to their start date with 0 net_weight\n",
    "# For rm_ids that start in 2024, backfill from 2024-01-01 and renumber local_time_idx from 0\n",
    "rm_ids_to_fix = []\n",
    "for rm_id, group in selected_with_local_time.groupby('rm_id'):\n",
    "    min_date = group['date_arrival'].min()\n",
    "    if min_date.year == 2024:\n",
    "        rm_ids_to_fix.append((rm_id, min_date))\n",
    "\n",
    "# Process each rm_id that needs fixing\n",
    "for rm_id, original_min_date in rm_ids_to_fix:\n",
    "    # Calculate days to add from 2024-01-01 to original_min_date\n",
    "    days_to_add = (original_min_date - pd.Timestamp('2024-01-01')).days\n",
    "    \n",
    "    # Create additional rows from 2024-01-01 to day before original_min_date\n",
    "    if days_to_add > 0:\n",
    "        additional_rows = pd.DataFrame({\n",
    "            'rm_id': rm_id,\n",
    "            'date_arrival': [pd.Timestamp('2024-01-01') + pd.Timedelta(days=i) for i in range(days_to_add)],\n",
    "            'net_weight': 0.0,\n",
    "            'quantity': 0.0,\n",
    "            'local_time_idx': range(days_to_add)  # 0, 1, 2, ... up to days_to_add-1\n",
    "        })\n",
    "        \n",
    "        # Update local_time_idx for existing rows of this rm_id\n",
    "        mask = selected_with_local_time['rm_id'] == rm_id\n",
    "        selected_with_local_time.loc[mask, 'local_time_idx'] += days_to_add\n",
    "        \n",
    "        # Append the new rows\n",
    "        selected_with_local_time = pd.concat([selected_with_local_time, additional_rows], ignore_index=True)\n",
    "\n",
    "# Sort by rm_id and date_arrival to maintain order\n",
    "selected_with_local_time = selected_with_local_time.sort_values(['rm_id', 'date_arrival']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# For every year and rm_id I want to make net_weight cumulative. So day1 = day1, day2 = day1+day2, day3 = day1+day2+day3 etc.\n",
    "# But it should reset at the start of each year for each rm_id\n",
    "selected_with_local_time['year'] = selected_with_local_time['date_arrival'].dt.year\n",
    "selected_with_local_time['net_weight_cum'] = selected_with_local_time.groupby(['rm_id', 'year'])['net_weight'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f7b4196b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding lag and rolling features...\n",
      "Added 16 lag and rolling features\n",
      "Added 16 lag and rolling features\n"
     ]
    }
   ],
   "source": [
    "######################### ADD ADDITIONAL FEATURES ##################################\n",
    "# Add additional features\n",
    "selected_with_local_time[\"month\"] = selected_with_local_time[\"date_arrival\"].dt.month.astype(str).astype(\"category\")\n",
    "selected_with_local_time[\"year\"] = selected_with_local_time[\"date_arrival\"].dt.year.astype(str).astype(\"category\")\n",
    "selected_with_local_time[\"day_of_week\"] = selected_with_local_time[\"date_arrival\"].dt.dayofweek.astype(str).astype(\"category\")  # 0=Monday, 6=Sunday\n",
    "selected_with_local_time[\"day_of_month\"] = selected_with_local_time[\"date_arrival\"].dt.day.astype(str).astype(\"category\")\n",
    "selected_with_local_time[\"week_of_year\"] = selected_with_local_time[\"date_arrival\"].dt.isocalendar().week.astype(str).astype(\"category\")\n",
    "selected_with_local_time[\"quarter\"] = selected_with_local_time[\"date_arrival\"].dt.quarter.astype(str).astype(\"category\")\n",
    "selected_with_local_time[\"is_weekend\"] = (selected_with_local_time[\"date_arrival\"].dt.dayofweek >= 5).astype(int)\n",
    "selected_with_local_time[\"is_month_start\"] = selected_with_local_time[\"date_arrival\"].dt.is_month_start.astype(int)\n",
    "selected_with_local_time[\"is_month_end\"] = selected_with_local_time[\"date_arrival\"].dt.is_month_end.astype(int)\n",
    "selected_with_local_time[\"log_weight\"] = np.log1p(selected_with_local_time[\"net_weight\"])\n",
    "\n",
    "print(\"Adding lag and rolling features...\")\n",
    "# Lag features - CRITICAL for time series forecasting\n",
    "for lag in [7, 14, 28, 56, 91]:\n",
    "    selected_with_local_time[f'net_weight_lag_{lag}'] = selected_with_local_time.groupby('rm_id')['net_weight'].shift(lag)\n",
    "    selected_with_local_time[f'log_weight_lag_{lag}'] = selected_with_local_time.groupby('rm_id')['log_weight'].shift(lag)\n",
    "\n",
    "# Rolling window statistics - capture trends\n",
    "for window in [7, 28, 91]:\n",
    "    selected_with_local_time[f'net_weight_rolling_mean_{window}'] = selected_with_local_time.groupby('rm_id')['net_weight'].transform(\n",
    "        lambda x: x.shift(1).rolling(window=window, min_periods=1).mean()\n",
    "    )\n",
    "    selected_with_local_time[f'net_weight_rolling_std_{window}'] = selected_with_local_time.groupby('rm_id')['net_weight'].transform(\n",
    "        lambda x: x.shift(1).rolling(window=window, min_periods=1).std()\n",
    "    )\n",
    "\n",
    "# Fill NaN values from lag/rolling features with 0\n",
    "lag_rolling_cols = [col for col in selected_with_local_time.columns if 'lag_' in col or 'rolling_' in col]\n",
    "selected_with_local_time[lag_rolling_cols] = selected_with_local_time[lag_rolling_cols].fillna(0)\n",
    "\n",
    "print(f\"Added {len(lag_rolling_cols)} lag and rolling features\")\n",
    "\n",
    "# Norwegian special days/holidays\n",
    "# Fixed holidays\n",
    "def get_norwegian_holidays(year):\n",
    "    \"\"\"Return dictionary of Norwegian holidays for a given year\"\"\"\n",
    "    from datetime import timedelta\n",
    "    \n",
    "    holidays = {}\n",
    "    \n",
    "    # Fixed date holidays\n",
    "    holidays[f'{year}-01-01'] = 'New Year'\n",
    "    holidays[f'{year}-05-01'] = 'Labour Day'\n",
    "    holidays[f'{year}-05-17'] = 'Constitution Day'\n",
    "    holidays[f'{year}-12-24'] = 'Christmas Eve'\n",
    "    holidays[f'{year}-12-25'] = 'Christmas Day'\n",
    "    holidays[f'{year}-12-26'] = 'Boxing Day'\n",
    "    holidays[f'{year}-12-31'] = 'New Year Eve'\n",
    "    \n",
    "    # Easter-based holidays (Easter dates vary each year)\n",
    "    # Approximate Easter calculation (Meeus/Jones/Butcher algorithm)\n",
    "    a = year % 19\n",
    "    b = year // 100\n",
    "    c = year % 100\n",
    "    d = b // 4\n",
    "    e = b % 4\n",
    "    f = (b + 8) // 25\n",
    "    g = (b - f + 1) // 3\n",
    "    h = (19 * a + b - d - g + 15) % 30\n",
    "    i = c // 4\n",
    "    k = c % 4\n",
    "    l = (32 + 2 * e + 2 * i - h - k) % 7\n",
    "    m = (a + 11 * h + 22 * l) // 451\n",
    "    month = (h + l - 7 * m + 114) // 31\n",
    "    day = ((h + l - 7 * m + 114) % 31) + 1\n",
    "    \n",
    "    easter = pd.Timestamp(year=year, month=month, day=day)\n",
    "    \n",
    "    # Easter-related holidays\n",
    "    holidays[(easter - timedelta(days=3)).strftime('%Y-%m-%d')] = 'Maundy Thursday'\n",
    "    holidays[(easter - timedelta(days=2)).strftime('%Y-%m-%d')] = 'Good Friday'\n",
    "    holidays[easter.strftime('%Y-%m-%d')] = 'Easter Sunday'\n",
    "    holidays[(easter + timedelta(days=1)).strftime('%Y-%m-%d')] = 'Easter Monday'\n",
    "    holidays[(easter + timedelta(days=39)).strftime('%Y-%m-%d')] = 'Ascension Day'\n",
    "    holidays[(easter + timedelta(days=49)).strftime('%Y-%m-%d')] = 'Whit Sunday'\n",
    "    holidays[(easter + timedelta(days=50)).strftime('%Y-%m-%d')] = 'Whit Monday'\n",
    "    \n",
    "    return holidays\n",
    "\n",
    "# Create a mapping of all dates to holidays\n",
    "all_holidays = {}\n",
    "for year in range(selected_with_local_time['date_arrival'].dt.year.min(), \n",
    "                  selected_with_local_time['date_arrival'].dt.year.max() + 1):\n",
    "    all_holidays.update(get_norwegian_holidays(year))\n",
    "\n",
    "# Add special day column\n",
    "selected_with_local_time['date_str'] = selected_with_local_time['date_arrival'].dt.strftime('%Y-%m-%d')\n",
    "selected_with_local_time['special_days'] = selected_with_local_time['date_str'].map(all_holidays).fillna('none').astype('category')\n",
    "selected_with_local_time.drop('date_str', axis=1, inplace=True)\n",
    "\n",
    "# Add binary flag for whether it's a holiday\n",
    "selected_with_local_time['is_holiday'] = (selected_with_local_time['special_days'] != 'none').astype(int)\n",
    "\n",
    "special_days = list(all_holidays.values())\n",
    "\n",
    "# Make rm_id a string instead of numeric\n",
    "selected_with_local_time[\"rm_id\"] = selected_with_local_time[\"rm_id\"].astype(int).astype(str).astype(\"category\")\n",
    "selected_with_local_time[\"is_holiday\"] = selected_with_local_time[\"is_holiday\"].astype(str).astype(\"category\")\n",
    "selected_with_local_time.drop(\"year\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "920fb7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### CREATE TIME SERIES DATASET FOR PYTORCH FORECASTING ##################################\n",
    "full_data = selected_with_local_time.copy()\n",
    "\n",
    "max_prediction_length = 151\n",
    "max_encoder_length = 365\n",
    "# V: training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    # V: data[lambda x: x.local_time_idx <= training_cutoff],\n",
    "    data = full_data,\n",
    "    time_idx=\"local_time_idx\",\n",
    "    target=\"net_weight_cum\",\n",
    "    group_ids=[\"rm_id\"],\n",
    "    min_encoder_length=max_encoder_length\n",
    "    // 2,  # keep encoder length long (as it is in the validation set)\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"rm_id\"],\n",
    "    #static_reals= no static real yet,\n",
    "    time_varying_known_categoricals=[\"special_days\", \"month\", \"day_of_week\", \"is_holiday\", \"day_of_month\", \"week_of_year\", \"quarter\"],\n",
    "    #variable_groups={\n",
    "    #    \"special_days\": special_days\n",
    "    #},  # group of categorical variables can be treated as one variable\n",
    "    time_varying_known_reals=[\"local_time_idx\", \"is_weekend\", \"is_month_start\", \"is_month_end\"],\n",
    "    # CAN PUT YEAR IN TIME_VARYING_KNOWN_REALS\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"quantity\",\n",
    "        \"net_weight\",\n",
    "        \"log_weight\",\n",
    "        \"net_weight_cum\",\n",
    "        # Lag features (10 total)\n",
    "        \"net_weight_lag_7\", \"net_weight_lag_14\", \"net_weight_lag_28\", \"net_weight_lag_56\", \"net_weight_lag_91\",\n",
    "        \"log_weight_lag_7\", \"log_weight_lag_14\", \"log_weight_lag_28\", \"log_weight_lag_56\", \"log_weight_lag_91\",\n",
    "        # Rolling features (6 total)\n",
    "        \"net_weight_rolling_mean_7\", \"net_weight_rolling_mean_28\", \"net_weight_rolling_mean_91\",\n",
    "        \"net_weight_rolling_std_7\", \"net_weight_rolling_std_28\", \"net_weight_rolling_std_91\",\n",
    "    ],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"rm_id\"], transformation=\"softplus\"\n",
    "    ),  # use softplus and normalize by group\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "# create validation set (predict=True) which means to predict the last max_prediction_length points in time\n",
    "# for each series\n",
    "#V: validation = TimeSeriesDataSet.from_dataset(\n",
    "#V:    training, data, predict=True, stop_randomization=True\n",
    "#V:)\n",
    "\n",
    "# create dataloaders for model\n",
    "batch_size = 128  # set this between 32 to 128\n",
    "train_dataloader = training.to_dataloader(\n",
    "    train=True, batch_size=batch_size, num_workers=0\n",
    ")\n",
    "#V: val_dataloader = validation.to_dataloader(\n",
    "#V:    train=False, batch_size=batch_size * 10, num_workers=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e64a45f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 119.8k\n"
     ]
    }
   ],
   "source": [
    "################# DECIDING ON THE MODEL AND TRAINER PARAMETERS ##########################\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "# {'gradient_clip_val': 0.012179320703577733, 'hidden_size': 29, 'dropout': 0.12469376786070158, 'hidden_continuous_size': 14, 'attention_head_size': 1, 'learning_rate': 0.004342145465626561}. Best is trial 2 with value: 449.2545166015625\n",
    "\n",
    "# configure network and trainer\n",
    "pl.seed_everything(42)\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator=\"gpu\",\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=0.012179320703577733,\n",
    "    limit_train_batches=30,\n",
    "    #fast_dev_run = True,\n",
    "    callbacks=[lr_logger],\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    # not meaningful for finding the learning rate but otherwise very important\n",
    "    learning_rate=0.004342145465626561,\n",
    "    hidden_size=29,  # most important hyperparameter apart from learning rate\n",
    "    # number of attention heads. Set to up to 4 for large datasets\n",
    "    attention_head_size=1,\n",
    "    dropout=0.12469376786070158,  # between 0.1 and 0.3 are good values\n",
    "    hidden_continuous_size=14,  # set to <= hidden_size\n",
    "    loss=QuantileLoss(),\n",
    "    #optimizer=\"ranger\", OPTIMIZER FOR FINDING BEST LEARNING RATE\n",
    "    # reduce learning rate if no improvement in validation loss after x epochs\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size() / 1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cbbcc302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3070') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 7.2 K  | train\n",
      "3  | prescalers                         | ModuleDict                      | 784    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 4.8 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 52.5 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 10.2 K | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 3.5 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 3.5 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 3.5 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 3.5 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 7.0 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 7.0 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 1.7 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 58     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 4.4 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 3.5 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 1.8 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 3.5 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 1.8 K  | train\n",
      "20 | output_layer                       | Linear                          | 210    | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "119 K     Trainable params\n",
      "0         Non-trainable params\n",
      "119 K     Total params\n",
      "0.479     Total estimated model params size (MB)\n",
      "712       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 7.2 K  | train\n",
      "3  | prescalers                         | ModuleDict                      | 784    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 4.8 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 52.5 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 10.2 K | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 3.5 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 3.5 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 3.5 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 3.5 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 7.0 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 7.0 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 1.7 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 58     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 4.4 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 3.5 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 1.8 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 3.5 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 1.8 K  | train\n",
      "20 | output_layer                       | Linear                          | 210    | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "119 K     Trainable params\n",
      "0         Non-trainable params\n",
      "119 K     Total params\n",
      "0.479     Total estimated model params size (MB)\n",
      "712       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:47<00:00,  0.64it/s, v_num=22, train_loss_step=1.36e+4, train_loss_epoch=2.83e+4]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:47<00:00,  0.63it/s, v_num=22, train_loss_step=1.36e+4, train_loss_epoch=2.83e+4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######## TRAINING THE MODEL ##########\n",
    "\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8dca98da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction dataframe shape: (104748, 33)\n",
      "Date range: 2024-01-02 00:00:00 to 2025-05-31 00:00:00\n",
      "Time index range: 1 to 7655\n",
      "\n",
      "First few rows:\n",
      "   local_time_idx rm_id date_arrival  net_weight  quantity  net_weight_cum  \\\n",
      "0            7132   342   2024-01-02         0.0       0.0             0.0   \n",
      "1            7133   342   2024-01-03         0.0       0.0             0.0   \n",
      "2            7134   342   2024-01-04         0.0       0.0             0.0   \n",
      "3            7135   342   2024-01-05         0.0       0.0             0.0   \n",
      "4            7136   342   2024-01-06         0.0       0.0             0.0   \n",
      "\n",
      "  month day_of_week day_of_month week_of_year  ... net_weight_lag_91  \\\n",
      "0     1           1            2            1  ...               0.0   \n",
      "1     1           2            3            1  ...               0.0   \n",
      "2     1           3            4            1  ...               0.0   \n",
      "3     1           4            5            1  ...               0.0   \n",
      "4     1           5            6            1  ...               0.0   \n",
      "\n",
      "   log_weight_lag_91  net_weight_rolling_mean_7  net_weight_rolling_std_7  \\\n",
      "0                0.0                        0.0                       0.0   \n",
      "1                0.0                        0.0                       0.0   \n",
      "2                0.0                        0.0                       0.0   \n",
      "3                0.0                        0.0                       0.0   \n",
      "4                0.0                        0.0                       0.0   \n",
      "\n",
      "   net_weight_rolling_mean_28  net_weight_rolling_std_28  \\\n",
      "0                         0.0                        0.0   \n",
      "1                         0.0                        0.0   \n",
      "2                         0.0                        0.0   \n",
      "3                         0.0                        0.0   \n",
      "4                         0.0                        0.0   \n",
      "\n",
      "   net_weight_rolling_mean_91  net_weight_rolling_std_91  special_days  \\\n",
      "0                         0.0                        0.0          none   \n",
      "1                         0.0                        0.0          none   \n",
      "2                         0.0                        0.0          none   \n",
      "3                         0.0                        0.0          none   \n",
      "4                         0.0                        0.0          none   \n",
      "\n",
      "   is_holiday  \n",
      "0           0  \n",
      "1           0  \n",
      "2           0  \n",
      "3           0  \n",
      "4           0  \n",
      "\n",
      "[5 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "################# FULLL PREDICTION FOR ALL RM_IDs (NEED ENCODER AND DECODER DATA) ###############\n",
    "\n",
    "\n",
    "rm_ids = full_data['rm_id'].unique().tolist()\n",
    "predict_data = []\n",
    "# Create prediction date range\n",
    "pred_start = pd.Timestamp('2025-01-01')\n",
    "pred_end = pd.Timestamp('2025-05-31')\n",
    "pred_dates = pd.date_range(start=pred_start, end=pred_end, freq='D')\n",
    "\n",
    "all_predict_dfs = []\n",
    "\n",
    "for rm_id in rm_ids:\n",
    "    test_rm_id = rm_id  # must match categorical rm_id type\n",
    "    historical = full_data[full_data['rm_id'] == test_rm_id].copy()\n",
    "    if historical.empty:\n",
    "        continue\n",
    "    min_date = historical['date_arrival'].min()\n",
    "\n",
    "    # build prediction rows for this rm_id\n",
    "    rows = []\n",
    "    for date in pred_dates:\n",
    "        time_idx = (date - min_date).days\n",
    "        date_str = date.strftime('%Y-%m-%d')\n",
    "        special_day = all_holidays.get(date_str, 'none')\n",
    "        is_holiday = '1' if special_day != 'none' else '0'\n",
    "\n",
    "        rows.append({\n",
    "            'rm_id': test_rm_id,\n",
    "            'date_arrival': date,\n",
    "            'local_time_idx': time_idx,\n",
    "            'month': str(date.month),\n",
    "            'day_of_week': str(date.dayofweek),\n",
    "            'day_of_month': str(date.day),\n",
    "            'week_of_year': str(date.isocalendar().week),\n",
    "            'quarter': str(date.quarter),\n",
    "            'is_weekend': int(date.dayofweek >= 5),\n",
    "            'is_month_start': int(date.is_month_start),\n",
    "            'is_month_end': int(date.is_month_end),\n",
    "            'special_days': special_day,\n",
    "            'is_holiday': is_holiday,\n",
    "            'net_weight_cum': 0,  # placeholder\n",
    "            'net_weight': 0,   # placeholder\n",
    "            'quantity': 0,     # placeholder\n",
    "            'log_weight': 0,   # placeholder\n",
    "            # Lag and rolling features will be computed from historical data\n",
    "            'net_weight_lag_7': 0,\n",
    "            'net_weight_lag_14': 0,\n",
    "            'net_weight_lag_28': 0,\n",
    "            'net_weight_lag_56': 0,\n",
    "            'net_weight_lag_91': 0,\n",
    "            'log_weight_lag_7': 0,\n",
    "            'log_weight_lag_14': 0,\n",
    "            'log_weight_lag_28': 0,\n",
    "            'log_weight_lag_56': 0,\n",
    "            'log_weight_lag_91': 0,\n",
    "            'net_weight_rolling_mean_7': 0,\n",
    "            'net_weight_rolling_mean_28': 0,\n",
    "            'net_weight_rolling_mean_91': 0,\n",
    "            'net_weight_rolling_std_7': 0,\n",
    "            'net_weight_rolling_std_28': 0,\n",
    "            'net_weight_rolling_std_91': 0,\n",
    "        })\n",
    "\n",
    "    pred_df_rm = pd.DataFrame(rows)\n",
    "    \n",
    "    # Note: Lag and rolling features are already set to 0 in the rows above\n",
    "    # They are unknown for future dates and will remain as placeholders\n",
    "    \n",
    "    # encoder/context data (last max_encoder_length days)\n",
    "    encoder_data = historical.tail(max_encoder_length).copy()\n",
    "    encoder_data['local_time_idx'] = encoder_data['local_time_idx'].astype(int)\n",
    "\n",
    "    # combine encoder + prediction for this rm_id and collect\n",
    "    combined = pd.concat([encoder_data, pred_df_rm], ignore_index=True)\n",
    "    all_predict_dfs.append(combined)\n",
    "\n",
    "# final combined prediction dataframe for all rm_ids\n",
    "predict_data = pd.concat(all_predict_dfs, ignore_index=True)\n",
    "\n",
    "predict_data = pd.DataFrame(predict_data)\n",
    "\n",
    "# Convert to categorical to match training data\n",
    "predict_data['rm_id'] = predict_data['rm_id'].astype(str).astype('category')\n",
    "predict_data['month'] = predict_data['month'].astype(str).astype('category')\n",
    "predict_data['day_of_week'] = predict_data['day_of_week'].astype(str).astype('category')\n",
    "predict_data['day_of_month'] = predict_data['day_of_month'].astype(str).astype('category')\n",
    "predict_data['week_of_year'] = predict_data['week_of_year'].astype(str).astype('category')\n",
    "predict_data['quarter'] = predict_data['quarter'].astype(str).astype('category')\n",
    "predict_data['special_days'] = predict_data['special_days'].astype(str).astype('category')\n",
    "predict_data['is_holiday'] = predict_data['is_holiday'].astype(str).astype('category')\n",
    "\n",
    "# Ensure local_time_idx is integer (required by TimeSeriesDataSet)\n",
    "predict_data['local_time_idx'] = predict_data['local_time_idx'].astype(int)\n",
    "\n",
    "print(f\"Prediction dataframe shape: {predict_data.shape}\")\n",
    "print(f\"Date range: {predict_data['date_arrival'].min()} to {predict_data['date_arrival'].max()}\")\n",
    "print(f\"Time index range: {predict_data['local_time_idx'].min()} to {predict_data['local_time_idx'].max()}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(predict_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6305fef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "################## MAKE PREDICTIONS #######################\n",
    "\n",
    "predictions = tft.predict(predict_data, mode=\"raw\", return_x=True, return_index=True, return_decoder_lengths=True)\n",
    "\n",
    "ltdx_and_rmid = predictions.index\n",
    "\n",
    "output = predictions.output.prediction[:,:,0]\n",
    "\n",
    "pred = []\n",
    "\n",
    "pred_start = pd.Timestamp('2025-01-01')\n",
    "pred_end = pd.Timestamp('2025-05-31')\n",
    "pred_dates = pd.date_range(start=pred_start, end=pred_end, freq='D')\n",
    "\n",
    "\n",
    "for rm_id_index in range(0,203):\n",
    "    rm_id_test = ltdx_and_rmid[\"rm_id\"][rm_id_index]\n",
    "    ltdx_test = ltdx_and_rmid[\"local_time_idx\"][rm_id_index]\n",
    "    for date in pred_dates:\n",
    "        pred_weight = output[rm_id_index][(date-pred_start).days].item()\n",
    "        pred.append({\n",
    "            \"rm_id\": rm_id_test,\n",
    "            \"local_time_idx\": ltdx_test,\n",
    "            \"date\": date,\n",
    "            \"predicted_weight\": pred_weight\n",
    "        })\n",
    "\n",
    "\n",
    "pred = pd.DataFrame(pred)\n",
    "pred_over_0 = pred[pred[\"predicted_weight\"]>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a277ef90",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## PREPARING THE SUBMISSION FILE #######################\n",
    "sample_submission = pd.read_csv(\"../data/sample_submission.csv\")\n",
    "prediction_mapping = pd.read_csv(\"../data/prediction_mapping.csv\", parse_dates=[\"forecast_start_date\", \"forecast_end_date\"])\n",
    "\n",
    "submission = sample_submission.merge(prediction_mapping, on=\"ID\")\n",
    "submission[\"forecast_end_date\"] = pd.to_datetime(submission[\"forecast_end_date\"])\n",
    "submission[\"forecast_start_date\"] = pd.to_datetime(submission[\"forecast_start_date\"])\n",
    "\n",
    "for p in pred_over_0.itertuples():\n",
    "    rm_id = p.rm_id\n",
    "    date_arrival = p.date.replace(tzinfo=None)\n",
    "    predicted_weight = p.predicted_weight\n",
    "    submission.loc[\n",
    "        (submission['rm_id'] == int(rm_id)) & (submission['forecast_end_date'] >= date_arrival),\n",
    "        'predicted_weight'\n",
    "    ] = predicted_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9e810dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_submission = submission[[\"ID\", \"predicted_weight\"]]\n",
    "test_submission.to_csv(\"submission_tft_cumulative_lag_rolling.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce8a994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL PREDICTED WEIGHTS PER RM_ID WITH THE ML MODEL:\n",
      "     rm_id  predicted_weight\n",
      "176   3781      4.587214e+06\n",
      "180   3865      3.330814e+06\n",
      "151   3126      2.402842e+06\n",
      "150   3125      1.392744e+06\n",
      "147   3122      1.358385e+06\n",
      "160   3282      1.148517e+06\n",
      "149   3124      9.164329e+05\n",
      "148   3123      7.534633e+05\n",
      "182   3901      3.643884e+05\n",
      "85    2142      2.133031e+05\n",
      "79    2134      1.293509e+05\n",
      "87    2144      1.251198e+05\n",
      "80    2135      1.136899e+05\n",
      "159   3265      8.336219e+04\n",
      "181   3883      7.053468e+04\n",
      "172   3642      6.301469e+04\n",
      "163   3421      4.528061e+04\n",
      "185   4021      4.474627e+04\n",
      "74    2129      4.171755e+04\n",
      "191   4263      3.826533e+04\n",
      "76    2131      3.327738e+04\n",
      "190   4222      3.219623e+04\n",
      "162   3381      2.636178e+04\n",
      "187   4081      2.336391e+04\n",
      "171   3621      2.255507e+04\n",
      "197   4443      1.979580e+04\n",
      "186   4044      1.314957e+04\n",
      "169   3581      1.161203e+04\n",
      "192   4302      9.161141e+03\n",
      "195   4401      7.290643e+03\n",
      "71    2125      5.578164e+03\n",
      "193   4343      4.264394e+03\n",
      "170   3601      3.215881e+03\n",
      "136   2741      2.183844e+03\n",
      "194   4381      2.089939e+03\n",
      "189   4161      1.526388e+03\n",
      "103   2161      3.970088e+02\n"
     ]
    }
   ],
   "source": [
    "############## PRINT THE MODEL's TOTAL PREDICTIONS PER RM_ID ##############\n",
    "filtered = submission.copy()\n",
    "\n",
    "agg_df = filtered.groupby(\"rm_id\", as_index=False).agg({\n",
    "    \"predicted_weight\": \"max\",\n",
    "}).sort_values(\"predicted_weight\", ascending=False)\n",
    "\n",
    "\n",
    "print(\"TOTAL PREDICTED WEIGHTS PER RM_ID WITH THE ML MODEL:\")\n",
    "print(agg_df[agg_df[\"predicted_weight\"]>0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
