{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b53d50a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test2024 = pd.read_csv(\"../validation/testing2024manual.csv\")\n",
    "prediction_mapping = pd.read_csv(\"../data/prediction_mapping.csv\")\n",
    "merged = test2024.merge(prediction_mapping, on=\"ID\")\n",
    "filtered = merged[merged.groupby(\"rm_id\")[\"predicted_weight\"].transform(\"sum\") > 0]\n",
    "agg_df = filtered.groupby(\"rm_id\", as_index=False).agg({\n",
    "    \"predicted_weight\": \"max\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "276ddcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_rm_ids = set(agg_df[\"rm_id\"])\n",
    "\n",
    "receivals = pd.read_csv(\"../data_cleaned/orders_with_receivals_detailed.csv\")\n",
    "receivals_filtered = receivals[receivals[\"rm_id\"].isin(used_rm_ids)]\n",
    "selected = receivals_filtered[[\"rm_id\", \"date_arrival\", \"net_weight\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7d0dad7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training rows: 57083 | Validation rows: 3361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Roaming\\Python\\Python312\\site-packages\\pytorch_forecasting\\data\\timeseries\\_timeseries.py:1850: UserWarning: Min encoder length and/or min_prediction_idx and/or min prediction length and/or lags are too large for 8 series/groups which therefore are not present in the dataset index. This means no predictions can be made for those series. First 10 removed groups: [{'__group_id__rm_id': '3201.0'}, {'__group_id__rm_id': '3581.0'}, {'__group_id__rm_id': '3621.0'}, {'__group_id__rm_id': '4081.0'}, {'__group_id__rm_id': '4161.0'}, {'__group_id__rm_id': '4222.0'}, {'__group_id__rm_id': '4263.0'}, {'__group_id__rm_id': '4302.0'}]\n",
      "  warnings.warn(\n",
      "C:\\Users\\david\\AppData\\Roaming\\Python\\Python312\\site-packages\\pytorch_forecasting\\data\\timeseries\\_timeseries.py:1850: UserWarning: Min encoder length and/or min_prediction_idx and/or min prediction length and/or lags are too large for 38 series/groups which therefore are not present in the dataset index. This means no predictions can be made for those series. First 10 removed groups: [{'__group_id__rm_id': '2124.0'}, {'__group_id__rm_id': '2125.0'}, {'__group_id__rm_id': '2129.0'}, {'__group_id__rm_id': '2130.0'}, {'__group_id__rm_id': '2131.0'}, {'__group_id__rm_id': '2132.0'}, {'__group_id__rm_id': '2133.0'}, {'__group_id__rm_id': '2134.0'}, {'__group_id__rm_id': '2135.0'}, {'__group_id__rm_id': '2140.0'}]\n",
      "  warnings.warn(\n",
      "C:\\Users\\david\\AppData\\Roaming\\Python\\Python312\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:210: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "C:\\Users\\david\\AppData\\Roaming\\Python\\Python312\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:210: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 644    | train\n",
      "3  | prescalers                         | ModuleDict                      | 96     | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 1.9 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 1.8 K  | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.2 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 2.2 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 2.2 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544    | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.1 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 576    | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 576    | train\n",
      "20 | output_layer                       | Linear                          | 119    | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "19.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.6 K    Total params\n",
      "0.079     Total estimated model params size (MB)\n",
      "278       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 644    | train\n",
      "3  | prescalers                         | ModuleDict                      | 96     | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 1.9 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 1.8 K  | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.2 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 2.2 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 2.2 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544    | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.1 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 576    | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 576    | train\n",
      "20 | output_layer                       | Linear                          | 119    | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "19.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.6 K    Total params\n",
      "0.079     Total estimated model params size (MB)\n",
      "278       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Roaming\\Python\\Python312\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Roaming\\Python\\Python312\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\david\\AppData\\Roaming\\Python\\Python312\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (30) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:15<00:00,  1.99it/s, v_num=9, train_loss_step=2.11e+3, val_loss=1.36e+3, train_loss_epoch=2.25e+3]\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:15<00:00,  1.99it/s, v_num=9, train_loss_step=2.11e+3, val_loss=1.36e+3, train_loss_epoch=2.25e+3]\n"
     ]
    }
   ],
   "source": [
    "# --- TFT Model Training with 2024 Junâ€“Dec validation ---\n",
    "import pandas as pd\n",
    "from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet, GroupNormalizer, QuantileLoss\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "# Load historical data\n",
    "df_hist = receivals_filtered[[\"rm_id\", \"date_arrival\", \"net_weight\"]].copy()\n",
    "df_hist[\"date_arrival\"] = pd.to_datetime(df_hist[\"date_arrival\"])\n",
    "# Ensure naive timestamps\n",
    "if hasattr(df_hist[\"date_arrival\"].dt, \"tz\") and df_hist[\"date_arrival\"].dt.tz is not None:\n",
    "    df_hist[\"date_arrival\"] = df_hist[\"date_arrival\"].dt.tz_localize(None)\n",
    "# String groups for PF\n",
    "df_hist[\"rm_id\"] = df_hist[\"rm_id\"].astype(str)\n",
    "# Stable time index relative to min date\n",
    "base_date = df_hist[\"date_arrival\"].min()\n",
    "df_hist[\"time_idx\"] = (df_hist[\"date_arrival\"] - base_date).dt.days\n",
    "\n",
    "# Train/validation split by date: train <= 2024-05-31, val = 2024-06-01..2024-12-31\n",
    "train_cutoff = pd.Timestamp(\"2024-05-31\")\n",
    "val_start = pd.Timestamp(\"2024-06-01\")\n",
    "val_end = pd.Timestamp(\"2024-12-31\")\n",
    "\n",
    "train_df = df_hist[df_hist[\"date_arrival\"] <= train_cutoff].copy()\n",
    "val_df = df_hist[(df_hist[\"date_arrival\"] >= val_start) & (df_hist[\"date_arrival\"] <= val_end)].copy()\n",
    "\n",
    "print(f\"Training rows: {len(train_df)} | Validation rows: {len(val_df)}\")\n",
    "\n",
    "max_encoder_length = 60\n",
    "max_prediction_length = 30\n",
    "batch_size = 64\n",
    "\n",
    "# Training dataset on train split\n",
    "training = TimeSeriesDataSet(\n",
    "    train_df,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"net_weight\",\n",
    "    group_ids=[\"rm_id\"],\n",
    "    min_encoder_length=max_encoder_length,\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"rm_id\"],\n",
    "    time_varying_known_reals=[\"time_idx\"],\n",
    "    time_varying_unknown_reals=[\"net_weight\"],\n",
    "    target_normalizer=GroupNormalizer(groups=[\"rm_id\"], transformation=\"softplus\"),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    ")\n",
    "\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "# Validation dataset (if available)\n",
    "val_dataloader = None\n",
    "if len(val_df) > 0:\n",
    "    validation = TimeSeriesDataSet.from_dataset(\n",
    "        training,\n",
    "        val_df,\n",
    "        predict=True,\n",
    "        stop_randomization=True,\n",
    "    )\n",
    "    val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "# Model\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    output_size=7,  # quantiles\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=0,\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "\n",
    "# Trainer with EarlyStopping on validation when available\n",
    "callbacks = [LearningRateMonitor()]\n",
    "if val_dataloader is not None:\n",
    "    callbacks.append(EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\"))\n",
    "else:\n",
    "    callbacks.append(EarlyStopping(monitor=\"train_loss\", patience=3))\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20,\n",
    "    accelerator=\"auto\",\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=30,\n",
    "    callbacks=callbacks,\n",
    "    logger=TensorBoardLogger(\"lightning_logs\"),\n",
    "    enable_checkpointing=True,\n",
    ")\n",
    "\n",
    "if val_dataloader is not None:\n",
    "    trainer.fit(tft, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n",
    "else:\n",
    "    trainer.fit(tft, train_dataloaders=train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "638e655a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historical data time_idx range: 0 to 4784\n",
      "Prediction time_idx range: 4796 to 4946\n",
      "Combined dataset shape: (61824, 4)\n",
      "Time_idx range in combined data: 0 to 4825\n",
      "Error during TFT prediction: filters should not remove entries all entries - check encoder/decoder lengths and lags\n",
      "Falling back to N-BEATS model for more accurate time series forecasting...\n",
      "Error during TFT prediction: filters should not remove entries all entries - check encoder/decoder lengths and lags\n",
      "Falling back to N-BEATS model for more accurate time series forecasting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Roaming\\Python\\Python312\\site-packages\\pytorch_forecasting\\data\\timeseries\\_timeseries.py:1850: UserWarning: Min encoder length and/or min_prediction_idx and/or min prediction length and/or lags are too large for 46 series/groups which therefore are not present in the dataset index. This means no predictions can be made for those series. First 10 removed groups: [{'__group_id__rm_id': '2124.0'}, {'__group_id__rm_id': '2125.0'}, {'__group_id__rm_id': '2129.0'}, {'__group_id__rm_id': '2130.0'}, {'__group_id__rm_id': '2131.0'}, {'__group_id__rm_id': '2132.0'}, {'__group_id__rm_id': '2133.0'}, {'__group_id__rm_id': '2134.0'}, {'__group_id__rm_id': '2135.0'}, {'__group_id__rm_id': '2140.0'}]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for 2025 using the trained TFT model (with train<=May24, val=Jun-Dec24)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pytorch_forecasting import NBeats, TimeSeriesDataSet\n",
    "\n",
    "# Define the prediction period (2025-01-01 to 2025-05-31)\n",
    "prediction_start = pd.Timestamp(\"2025-01-01\")\n",
    "prediction_end = pd.Timestamp(\"2025-05-31\")\n",
    "\n",
    "# Convert to time_idx based on the historical data's base_date\n",
    "start_time_idx = (prediction_start - base_date).days\n",
    "end_time_idx = (prediction_end - base_date).days\n",
    "\n",
    "print(f\"Historical data time_idx range: 0 to {df_hist['time_idx'].max()}\")\n",
    "print(f\"Prediction time_idx range: {start_time_idx} to {end_time_idx}\")\n",
    "\n",
    "# Create future time steps for the prediction period\n",
    "future_time_steps = list(range(start_time_idx, end_time_idx + 1))\n",
    "\n",
    "# Prepare future data for prediction - ensure PF known/unknown structure\n",
    "future_data = []\n",
    "for rm_id in df_hist[\"rm_id\"].unique():\n",
    "    for future_time in future_time_steps[:max_prediction_length]:  # Limit to model's max prediction length\n",
    "        future_data.append({\n",
    "            \"rm_id\": rm_id,\n",
    "            \"time_idx\": future_time,\n",
    "            \"net_weight\": 0,\n",
    "        })\n",
    "\n",
    "future_df = pd.DataFrame(future_data)\n",
    "\n",
    "# Combine historical data with future data for prediction\n",
    "prediction_data = pd.concat([df_hist, future_df]).reset_index(drop=True)\n",
    "prediction_data = prediction_data.sort_values([\"rm_id\", \"time_idx\"]).reset_index(drop=True)\n",
    "\n",
    "print(f\"Combined dataset shape: {prediction_data.shape}\")\n",
    "print(f\"Time_idx range in combined data: {prediction_data['time_idx'].min()} to {prediction_data['time_idx'].max()}\")\n",
    "\n",
    "# Create prediction dataset\n",
    "try:\n",
    "    prediction_dataset = TimeSeriesDataSet.from_dataset(\n",
    "        training,\n",
    "        prediction_data,\n",
    "        predict=True,\n",
    "        stop_randomization=True,\n",
    "    )\n",
    "\n",
    "    # Create prediction dataloader\n",
    "    pred_dataloader = prediction_dataset.to_dataloader(train=False, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "    # Make predictions\n",
    "    print(\"Making TFT predictions...\")\n",
    "    predictions = tft.predict(pred_dataloader, mode=\"prediction\", return_x=True)\n",
    "\n",
    "    print(\"TFT predictions completed.\")\n",
    "    print(f\"Prediction shape: {predictions[0].shape}\")\n",
    "\n",
    "    # Process predictions to create simulated receivals using a conservative quantile\n",
    "    predicted_values = predictions[0].cpu().numpy()\n",
    "\n",
    "    results = []\n",
    "    future_prediction_data = prediction_data[prediction_data[\"time_idx\"] >= start_time_idx].copy()\n",
    "\n",
    "    prediction_idx = 0\n",
    "    for _, row in future_prediction_data.iterrows():\n",
    "        if prediction_idx < len(predicted_values):\n",
    "            predicted_date = base_date + pd.Timedelta(days=int(row[\"time_idx\"]))\n",
    "\n",
    "            # Prefer a slightly conservative quantile (e.g., 30th percentile) to mitigate overprediction\n",
    "            if predicted_values.ndim == 3:  # (batch, time, quantiles)\n",
    "                # indices: 0..6 for quantiles; choose 2 (~0.3) instead of 3 (median)\n",
    "                pred_value = predicted_values[prediction_idx, 0, 2]\n",
    "            elif predicted_values.ndim == 2:  # (batch, quantiles)\n",
    "                pred_value = predicted_values[prediction_idx, 2]\n",
    "            else:\n",
    "                pred_value = predicted_values[prediction_idx]\n",
    "\n",
    "            if pred_value > 0:\n",
    "                results.append({\n",
    "                    \"rm_id\": row[\"rm_id\"],\n",
    "                    \"time_idx\": row[\"time_idx\"],\n",
    "                    \"date_arrival\": predicted_date,\n",
    "                    \"net_weight\": float(pred_value),\n",
    "                })\n",
    "\n",
    "            prediction_idx += 1\n",
    "\n",
    "    if len(results) > 0:\n",
    "        simulated_df = pd.DataFrame(results)\n",
    "\n",
    "        if len(future_time_steps) > max_prediction_length:\n",
    "            print(f\"Extending TFT predictions from {max_prediction_length} days to {len(future_time_steps)} days with mild decay...\")\n",
    "            extended_results = results.copy()\n",
    "            for rm_id in df_hist[\"rm_id\"].unique():\n",
    "                rm_predictions = [r for r in results if r[\"rm_id\"] == rm_id]\n",
    "                if rm_predictions:\n",
    "                    weights = [r[\"net_weight\"] for r in rm_predictions]\n",
    "                    avg_weight = float(np.mean(weights))\n",
    "                    for time_idx in future_time_steps[max_prediction_length:]:\n",
    "                        pred_date = base_date + pd.Timedelta(days=int(time_idx))\n",
    "                        days_beyond = time_idx - future_time_steps[max_prediction_length-1]\n",
    "                        decay_factor = max(0.15, 1.0 / (1.0 + days_beyond * 0.03))  # 3% decay per day, floor at 0.15\n",
    "                        pred_weight = avg_weight * decay_factor * np.random.normal(1.0, 0.03)\n",
    "                        if pred_weight > 0:\n",
    "                            extended_results.append({\n",
    "                                \"rm_id\": rm_id,\n",
    "                                \"time_idx\": time_idx,\n",
    "                                \"date_arrival\": pred_date,\n",
    "                                \"net_weight\": float(pred_weight),\n",
    "                            })\n",
    "            simulated_df = pd.DataFrame(extended_results)\n",
    "    else:\n",
    "        simulated_df = pd.DataFrame()\n",
    "\n",
    "    print(\"TFT-based forecasting complete.\")\n",
    "    print(f\"Total simulated receivals for 2025: {len(simulated_df)}\")\n",
    "    if len(simulated_df) > 0:\n",
    "        print(f\"Date range: {simulated_df['date_arrival'].min()} to {simulated_df['date_arrival'].max()}\")\n",
    "        print(f\"Weight range: {simulated_df['net_weight'].min():.2f} to {simulated_df['net_weight'].max():.2f}\")\n",
    "        print(simulated_df.head())\n",
    "\n",
    "        simulated_df.to_csv(\"simulated_receivals_2025.csv\", index=False)\n",
    "        print(\"Results saved to simulated_receivals_2025.csv\")\n",
    "    else:\n",
    "        raise Exception(\"No predictions generated from TFT model\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during TFT prediction: {str(e)}\")\n",
    "    print(\"Falling back to N-BEATS model for more accurate time series forecasting...\")\n",
    "    # Keep existing N-BEATS fallback block below unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bc687980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PREDICTION VERIFICATION (Janâ€“May) ===\n",
      "Comparison of 2025 Janâ€“May predictions vs 2024 Janâ€“May actual:\n",
      "rm_id\t2024_JanMay\t2025_JanMay_pred\tratio\n",
      "3125\t3028260\t\t3632046\t\t1.20\n",
      "3122\t2183320\t\t3607043\t\t1.65\n",
      "3282\t2373080\t\t3339509\t\t1.41\n",
      "3124\t2407820\t\t3013304\t\t1.25\n",
      "3126\t2998700\t\t2824583\t\t0.94\n",
      "3123\t1782880\t\t2496968\t\t1.40\n",
      "2130\t3549704\t\t2466240\t\t0.69\n",
      "3781\t6528018\t\t2442800\t\t0.37\n",
      "3865\t5801072\t\t1752453\t\t0.30\n",
      "2140\t1046440\t\t1653727\t\t1.58\n",
      "3901\t857880\t\t1452334\t\t1.69\n",
      "2134\t612846\t\t1181230\t\t1.93\n",
      "2142\t445868\t\t554327\t\t1.24\n",
      "2135\t494030\t\t548095\t\t1.11\n",
      "3265\t576140\t\t491529\t\t0.85\n",
      "\n",
      "Warning: 23 rm_ids predicted >1.5x their 2024 Janâ€“May levels:\n",
      "rm_id\n",
      "3122    1.652091\n",
      "2140    1.580336\n",
      "3901    1.692934\n",
      "2134    1.927450\n",
      "3362    4.706150\n",
      "2131    1.595646\n",
      "2144    1.662039\n",
      "4222    4.756619\n",
      "2145    1.827073\n",
      "2741    1.890594\n",
      "Name: ratio_2025_to_2024_JanMay, dtype: float64\n",
      "\n",
      "Overall statistics (Janâ€“May):\n",
      "- Average ratio: 1.65\n",
      "- Median ratio: 1.47\n",
      "- Total 2024 Janâ€“May: 37542429\n",
      "- Total 2025 Janâ€“May predicted: 36247310\n",
      "- Overall ratio: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Verify predictions are realistic by comparing to 2024 Janâ€“May actuals\n",
    "print(\"\\n=== PREDICTION VERIFICATION (Janâ€“May) ===\")\n",
    "\n",
    "if len(simulated_df) > 0:\n",
    "    # Ensure consistent rm_id types for comparison\n",
    "    jan_may_mask_2024 = (df_hist[\"date_arrival\"] >= pd.Timestamp(\"2024-01-01\")) & (df_hist[\"date_arrival\"] <= pd.Timestamp(\"2024-05-31\"))\n",
    "    hist_2024_janm_total = df_hist.loc[jan_may_mask_2024].groupby(\"rm_id\")[\"net_weight\"].sum()\n",
    "    # Convert string rm_ids to int for consistency (handle float strings like \"2124.0\")\n",
    "    hist_2024_janm_total.index = hist_2024_janm_total.index.astype(float).astype(int)\n",
    "\n",
    "    jan_may_mask_2025 = (simulated_df[\"date_arrival\"] >= pd.Timestamp(\"2025-01-01\")) & (simulated_df[\"date_arrival\"] <= pd.Timestamp(\"2025-05-31\"))\n",
    "    pred_2025_janm_total = simulated_df.loc[jan_may_mask_2025].groupby(\"rm_id\")[\"net_weight\"].sum()\n",
    "    # Ensure int rm_ids in predictions too (handle float strings)\n",
    "    pred_2025_janm_total.index = pred_2025_janm_total.index.astype(float).astype(int)\n",
    "\n",
    "    comparison = pd.DataFrame({\n",
    "        \"hist_2024_JanMay\": hist_2024_janm_total,\n",
    "        \"pred_2025_JanMay\": pred_2025_janm_total\n",
    "    }).fillna(0)\n",
    "\n",
    "    comparison[\"ratio_2025_to_2024_JanMay\"] = comparison[\"pred_2025_JanMay\"] / (comparison[\"hist_2024_JanMay\"] + 1e-6)\n",
    "    comparison = comparison.sort_values(\"pred_2025_JanMay\", ascending=False)\n",
    "\n",
    "    print(\"Comparison of 2025 Janâ€“May predictions vs 2024 Janâ€“May actual:\")\n",
    "    print(\"rm_id\\t2024_JanMay\\t2025_JanMay_pred\\tratio\")\n",
    "    for rm_id, row in comparison.head(15).iterrows():\n",
    "        print(f\"{rm_id}\\t{row['hist_2024_JanMay']:.0f}\\t\\t{row['pred_2025_JanMay']:.0f}\\t\\t{row['ratio_2025_to_2024_JanMay']:.2f}\")\n",
    "\n",
    "    over_predicted = comparison[comparison[\"ratio_2025_to_2024_JanMay\"] > 1.5]\n",
    "    if len(over_predicted) > 0:\n",
    "        print(f\"\\nWarning: {len(over_predicted)} rm_ids predicted >1.5x their 2024 Janâ€“May levels:\")\n",
    "        print(over_predicted[\"ratio_2025_to_2024_JanMay\"].head(10))\n",
    "\n",
    "    under_predicted = comparison[comparison[\"ratio_2025_to_2024_JanMay\"] < 0.3]\n",
    "    if len(under_predicted) > 0:\n",
    "        print(f\"\\nInfo: {len(under_predicted)} rm_ids predicted <30% of their 2024 Janâ€“May levels\")\n",
    "\n",
    "    print(f\"\\nOverall statistics (Janâ€“May):\")\n",
    "    print(f\"- Average ratio: {comparison['ratio_2025_to_2024_JanMay'].mean():.2f}\")\n",
    "    print(f\"- Median ratio: {comparison['ratio_2025_to_2024_JanMay'].median():.2f}\")\n",
    "    print(f\"- Total 2024 Janâ€“May: {comparison['hist_2024_JanMay'].sum():.0f}\")\n",
    "    print(f\"- Total 2025 Janâ€“May predicted: {comparison['pred_2025_JanMay'].sum():.0f}\")\n",
    "    print(f\"- Overall ratio: {comparison['pred_2025_JanMay'].sum() / (comparison['hist_2024_JanMay'].sum() + 1e-6):.2f}\")\n",
    "else:\n",
    "    print(\"No predictions to verify!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ec8cdb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration disabled: wrote original predictions to simulated_receivals_2025.csv\n"
     ]
    }
   ],
   "source": [
    "# Calibrate predictions: optional; if CALIBRATION_MODE='off', pass-through without caps/scaling\n",
    "CALIBRATION_MODE = \"off\"  # options: 'off' | 'cap' | 'scale'\n",
    "CAL_TOLERANCE = 1.05       # used only when mode is 'cap' or 'scale'\n",
    "\n",
    "if len(simulated_df) > 0:\n",
    "    if CALIBRATION_MODE == \"off\":\n",
    "        # Write original predictions unchanged to canonical file\n",
    "        simulated_df.to_csv(\"simulated_receivals_2025.csv\", index=False)\n",
    "        print(\"Calibration disabled: wrote original predictions to simulated_receivals_2025.csv\")\n",
    "    else:\n",
    "        # Build 2024 Janâ€“May baseline\n",
    "        jan_may_mask_2024 = (df_hist[\"date_arrival\"] >= pd.Timestamp(\"2024-01-01\")) & (df_hist[\"date_arrival\"] <= pd.Timestamp(\"2024-05-31\"))\n",
    "        baseline_2024 = df_hist.loc[jan_may_mask_2024].groupby(\"rm_id\")[\"net_weight\"].sum()\n",
    "\n",
    "        # Focus on predicted Janâ€“May 2025\n",
    "        jan_may_mask_2025 = (simulated_df[\"date_arrival\"] >= pd.Timestamp(\"2025-01-01\")) & (simulated_df[\"date_arrival\"] <= pd.Timestamp(\"2025-05-31\"))\n",
    "        preds_janm = simulated_df.loc[jan_may_mask_2025].copy()\n",
    "\n",
    "        # Compute per-rm_id current totals\n",
    "        current_totals = preds_janm.groupby(\"rm_id\")[\"net_weight\"].sum()\n",
    "\n",
    "        # Prepare calibrated copy\n",
    "        calibrated = preds_janm.copy()\n",
    "\n",
    "        # Apply per-rm_id calibration\n",
    "        for rm_id, total_2025 in current_totals.items():\n",
    "            baseline = float(baseline_2024.get(rm_id, 0.0))\n",
    "            cap_value = baseline * CAL_TOLERANCE\n",
    "            if baseline <= 0:\n",
    "                # If no baseline, keep as-is but clip extreme weights\n",
    "                rm_mask = calibrated[\"rm_id\"] == rm_id\n",
    "                calibrated.loc[rm_mask, \"net_weight\"] = calibrated.loc[rm_mask, \"net_weight\"].clip(upper=calibrated.loc[rm_mask, \"net_weight\"].quantile(0.95))\n",
    "                continue\n",
    "\n",
    "            if CALIBRATION_MODE == \"cap\":\n",
    "                if total_2025 > cap_value:\n",
    "                    scale = cap_value / (total_2025 + 1e-6)\n",
    "                    rm_mask = calibrated[\"rm_id\"] == rm_id\n",
    "                    calibrated.loc[rm_mask, \"net_weight\"] *= scale\n",
    "            elif CALIBRATION_MODE == \"scale\":\n",
    "                # Scale towards baseline (not below 80% of baseline)\n",
    "                target = max(0.8 * baseline, min(cap_value, total_2025))\n",
    "                scale = target / (total_2025 + 1e-6)\n",
    "                rm_mask = calibrated[\"rm_id\"] == rm_id\n",
    "                calibrated.loc[rm_mask, \"net_weight\"] *= scale\n",
    "\n",
    "        # Merge calibrated Janâ€“May back with any predictions outside the window (shouldn't be many)\n",
    "        outside_mask = ~jan_may_mask_2025\n",
    "        outside = simulated_df.loc[outside_mask].copy()\n",
    "\n",
    "        # Replace simulated_df with calibrated version for downstream\n",
    "        simulated_df = pd.concat([calibrated, outside], ignore_index=True)\n",
    "\n",
    "        # Overwrite canonical file name with calibrated content as requested\n",
    "        simulated_df.to_csv(\"simulated_receivals_2025.csv\", index=False)\n",
    "        print(\"Calibrated predictions saved to simulated_receivals_2025.csv\")\n",
    "else:\n",
    "    print(\"No predictions found to calibrate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "008bb364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2571 simulated receivals from simulated_receivals_2025.csv\n",
      "Date range: 2024-12-31 10:15:00 to 2025-05-30 10:15:00\n"
     ]
    }
   ],
   "source": [
    "# Load submission template and prepare data\n",
    "sample_submission = pd.read_csv(\"../data/sample_submission.csv\")\n",
    "prediction_mapping = pd.read_csv(\"../data/prediction_mapping.csv\", parse_dates=[\"forecast_start_date\", \"forecast_end_date\"])\n",
    "\n",
    "# Initialize submission with zeros\n",
    "submission = sample_submission.copy()\n",
    "submission[\"predicted_weight\"] = 0.0\n",
    "\n",
    "# Merge with prediction mapping to get rm_id and date information\n",
    "submission = submission.merge(prediction_mapping, on=\"ID\")\n",
    "\n",
    "# Always load canonical calibrated output\n",
    "try:\n",
    "    simulated_df = pd.read_csv(\"simulated_receivals_2025.csv\", parse_dates=[\"date_arrival\"])  # canonical name\n",
    "    print(f\"Loaded {len(simulated_df)} simulated receivals from simulated_receivals_2025.csv\")\n",
    "    print(f\"Date range: {simulated_df['date_arrival'].min()} to {simulated_df['date_arrival'].max()}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: simulated_receivals_2025.csv not found. Please run the prediction+calibration cells first.\")\n",
    "    simulated_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9f2ab9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing predictions for submission with cumulative allocation...\n",
      "Updated 2789146779.92 total predicted weight across 30450 rows\n",
      "Non-zero predictions: 6304 out of 30450 rows\n",
      "Weight stats - Min: 1778.9801, Max: 3657333.9047, Mean: 442440.7963\n",
      "Updated 2789146779.92 total predicted weight across 30450 rows\n",
      "Non-zero predictions: 6304 out of 30450 rows\n",
      "Weight stats - Min: 1778.9801, Max: 3657333.9047, Mean: 442440.7963\n"
     ]
    }
   ],
   "source": [
    "# Generate submission with cumulative allocation: each receival is added to all submission rows where forecast_end_date >= date_arrival\n",
    "ALLOCATION_MODE = \"cumulative\"\n",
    "\n",
    "if len(simulated_df) > 0:\n",
    "    print(\"Processing predictions for submission with cumulative allocation...\")\n",
    "\n",
    "    submission[\"rm_id\"] = submission[\"rm_id\"].astype(int)\n",
    "    simulated_df[\"rm_id\"] = simulated_df[\"rm_id\"].astype(float).astype(int)\n",
    "\n",
    "    for receival in simulated_df.itertuples():\n",
    "        rm_id = int(receival.rm_id)\n",
    "        date_arrival = receival.date_arrival\n",
    "        net_weight = float(receival.net_weight)\n",
    "\n",
    "        # Ensure naive timestamp\n",
    "        if hasattr(date_arrival, 'tz') and date_arrival.tz is not None:\n",
    "            date_arrival = date_arrival.tz_localize(None)\n",
    "\n",
    "        # Find all submission rows for this rm_id where forecast_end_date >= date_arrival\n",
    "        mask = (submission[\"rm_id\"] == rm_id) & (submission[\"forecast_end_date\"] >= date_arrival)\n",
    "        idxs = submission.index[mask]\n",
    "        if len(idxs) == 0:\n",
    "            continue\n",
    "        submission.loc[idxs, \"predicted_weight\"] += net_weight\n",
    "\n",
    "    print(f\"Updated {submission['predicted_weight'].sum():.2f} total predicted weight across {len(submission)} rows\")\n",
    "    non_zero = (submission['predicted_weight'] > 0).sum()\n",
    "    print(f\"Non-zero predictions: {non_zero} out of {len(submission)} rows\")\n",
    "\n",
    "    non_zero_submission = submission[submission['predicted_weight'] > 0]\n",
    "    if len(non_zero_submission) > 0:\n",
    "        print(\n",
    "            f\"Weight stats - Min: {non_zero_submission['predicted_weight'].min():.4f}, \"\n",
    "            f\"Max: {non_zero_submission['predicted_weight'].max():.4f}, \"\n",
    "            f\"Mean: {non_zero_submission['predicted_weight'].mean():.4f}\"\n",
    "        )\n",
    "else:\n",
    "    print(\"No simulated receivals available for submission generation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3e759940",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = submission[[\"ID\", \"predicted_weight\"]]\n",
    "submission.to_csv(\"testing2025.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ac12fc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"testing2025.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "88b35b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     rm_id  predicted_weight\n",
      "150   3125      3.657334e+06\n",
      "147   3122      3.625794e+06\n",
      "160   3282      3.357534e+06\n",
      "149   3124      3.040462e+06\n",
      "151   3126      2.839868e+06\n",
      "148   3123      2.521192e+06\n",
      "75    2130      2.483351e+06\n",
      "176   3781      2.455220e+06\n",
      "180   3865      1.766089e+06\n",
      "83    2140      1.653727e+06\n",
      "182   3901      1.470989e+06\n",
      "79    2134      1.190710e+06\n",
      "80    2135      5.597194e+05\n",
      "85    2142      5.587074e+05\n",
      "159   3265      4.915287e+05\n",
      "161   3362      4.521669e+05\n",
      "76    2131      3.829215e+05\n",
      "87    2144      3.688361e+05\n",
      "190   4222      3.529411e+05\n",
      "88    2145      3.510526e+05\n",
      "136   2741      3.352526e+05\n",
      "181   3883      2.927613e+05\n",
      "86    2143      2.624352e+05\n",
      "163   3421      2.608358e+05\n",
      "174   3761      2.288889e+05\n",
      "191   4263      2.212719e+05\n",
      "152   3142      1.872517e+05\n",
      "77    2132      1.807225e+05\n",
      "172   3642      1.714405e+05\n",
      "74    2129      1.081445e+05\n",
      "156   3201      1.054540e+05\n",
      "185   4021      7.199715e+04\n",
      "78    2133      6.998173e+04\n",
      "192   4302      6.620501e+04\n",
      "90    2147      6.533938e+04\n",
      "162   3381      6.509608e+04\n",
      "187   4081      5.028392e+04\n",
      "71    2125      4.551136e+04\n",
      "142   2981      3.992491e+04\n",
      "171   3621      3.170320e+04\n",
      "169   3581      2.083169e+04\n",
      "186   4044      1.385565e+04\n",
      "70    2124      1.357177e+04\n",
      "170   3601      4.581930e+03\n",
      "189   4161      3.613986e+03\n",
      "103   2161      3.079101e+03\n"
     ]
    }
   ],
   "source": [
    "test_df = submission.merge(prediction_mapping, on=\"ID\")\n",
    "test_df = test_df.groupby(\"rm_id\", as_index=False).agg({\n",
    "    \"predicted_weight\": \"max\",\n",
    "}).sort_values(\"predicted_weight\", ascending=False)\n",
    "\n",
    "print(test_df[0:46])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ea237fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ID  predicted_weight  rm_id forecast_start_date forecast_end_date\n",
      "0          1               0.0    365          2025-01-01        2025-01-02\n",
      "1          2               0.0    365          2025-01-01        2025-01-03\n",
      "2          3               0.0    365          2025-01-01        2025-01-04\n",
      "3          4               0.0    365          2025-01-01        2025-01-05\n",
      "4          5               0.0    365          2025-01-01        2025-01-06\n",
      "...      ...               ...    ...                 ...               ...\n",
      "30445  30446               0.0   4501          2025-01-01        2025-05-27\n",
      "30446  30447               0.0   4501          2025-01-01        2025-05-28\n",
      "30447  30448               0.0   4501          2025-01-01        2025-05-29\n",
      "30448  30449               0.0   4501          2025-01-01        2025-05-30\n",
      "30449  30450               0.0   4501          2025-01-01        2025-05-31\n",
      "\n",
      "[30450 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# SCALE SPECIFIC RM_ID DOWN\n",
    "testing_scaled = pd.read_csv(\"testing2025.csv\")\n",
    "testing_scaled = testing_scaled.merge(prediction_mapping, on=\"ID\", how=\"inner\")\n",
    "\n",
    "testing_scaled['predicted_weight'] *= 0.8\n",
    "\n",
    "print(testing_scaled)\n",
    "testing_scaled = testing_scaled[[\"ID\", \"predicted_weight\"]]\n",
    "\n",
    "testing_scaled.to_csv(\"testing2025_scaled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833a360c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
