{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b53d50a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test2024 = pd.read_csv(\"../validation/testing2024manual.csv\")\n",
    "prediction_mapping = pd.read_csv(\"../data/prediction_mapping.csv\")\n",
    "merged = test2024.merge(prediction_mapping, on=\"ID\")\n",
    "filtered = merged[merged.groupby(\"rm_id\")[\"predicted_weight\"].transform(\"sum\") > 0]\n",
    "agg_df = filtered.groupby(\"rm_id\", as_index=False).agg({\n",
    "    \"predicted_weight\": \"max\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "276ddcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_rm_ids = set(agg_df[\"rm_id\"])\n",
    "\n",
    "receivals = pd.read_csv(\"../data_cleaned/orders_with_receivals_detailed.csv\")\n",
    "receivals_filtered = receivals[receivals[\"rm_id\"].isin(used_rm_ids)]\n",
    "selected = receivals_filtered[[\"rm_id\", \"date_arrival\", \"net_weight\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d0dad7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Roaming\\Python\\Python312\\site-packages\\pytorch_forecasting\\models\\base\\_base_model.py:28: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n",
      "C:\\Users\\david\\AppData\\Roaming\\Python\\Python312\\site-packages\\pytorch_forecasting\\data\\timeseries\\_timeseries.py:1850: UserWarning: Min encoder length and/or min_prediction_idx and/or min prediction length and/or lags are too large for 4 series/groups which therefore are not present in the dataset index. This means no predictions can be made for those series. First 10 removed groups: [{'__group_id__rm_id': '3581.0'}, {'__group_id__rm_id': '3621.0'}, {'__group_id__rm_id': '4081.0'}, {'__group_id__rm_id': '4161.0'}]\n",
      "  warnings.warn(\n",
      "C:\\Users\\david\\AppData\\Roaming\\Python\\Python312\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:210: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "C:\\Users\\david\\AppData\\Roaming\\Python\\Python312\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:210: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\david\\AppData\\Roaming\\Python\\Python312\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 644    | train\n",
      "3  | prescalers                         | ModuleDict                      | 96     | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 1.9 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 1.8 K  | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.2 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 2.2 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 2.2 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544    | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.1 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 576    | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 576    | train\n",
      "20 | output_layer                       | Linear                          | 119    | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "19.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.6 K    Total params\n",
      "0.079     Total estimated model params size (MB)\n",
      "278       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "C:\\Users\\david\\AppData\\Roaming\\Python\\Python312\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\david\\AppData\\Roaming\\Python\\Python312\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (30) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/30 [00:00<?, ?it/s, v_num=7, train_loss_step=2.22e+3, train_loss_epoch=2.46e+3]         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Roaming\\Python\\Python312\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:492: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:33<00:00,  0.90it/s, v_num=7, train_loss_step=2.32e+3, train_loss_epoch=2.22e+3]\n"
     ]
    }
   ],
   "source": [
    "# --- TFT Model Training: Use all data for training, no validation split ---\n",
    "import pandas as pd\n",
    "from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet, GroupNormalizer, QuantileLoss\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "# Load historical data\n",
    "df_hist = receivals_filtered[[\"rm_id\", \"date_arrival\", \"net_weight\"]].copy()\n",
    "df_hist[\"date_arrival\"] = pd.to_datetime(df_hist[\"date_arrival\"])\n",
    "if hasattr(df_hist[\"date_arrival\"].dt, \"tz\") and df_hist[\"date_arrival\"].dt.tz is not None:\n",
    "    df_hist[\"date_arrival\"] = df_hist[\"date_arrival\"].dt.tz_localize(None)\n",
    "df_hist[\"rm_id\"] = df_hist[\"rm_id\"].astype(str)\n",
    "df_hist[\"time_idx\"] = (df_hist[\"date_arrival\"] - df_hist[\"date_arrival\"].min()).dt.days\n",
    "\n",
    "# Use all data for training\n",
    "train_data = df_hist.copy()\n",
    "\n",
    "max_encoder_length = 60\n",
    "max_prediction_length = 30\n",
    "batch_size = 64\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    train_data,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"net_weight\",\n",
    "    group_ids=[\"rm_id\"],\n",
    "    min_encoder_length=max_encoder_length,\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"rm_id\"],\n",
    "    time_varying_known_reals=[\"time_idx\"],\n",
    "    time_varying_unknown_reals=[\"net_weight\"],\n",
    "    target_normalizer=GroupNormalizer(groups=[\"rm_id\"], transformation=\"softplus\"),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    ")\n",
    "\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    output_size=7,\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=0,\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20,\n",
    "    accelerator=\"auto\",\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=30,\n",
    "    callbacks=[LearningRateMonitor(), EarlyStopping(monitor=\"train_loss\", patience=3)],\n",
    "    logger=TensorBoardLogger(\"lightning_logs\"),\n",
    "    enable_checkpointing=True,\n",
    ")\n",
    "\n",
    "trainer.fit(tft, train_dataloaders=train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "638e655a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historical data time_idx range: 0 to 4784\n",
      "Prediction time_idx range: 4796 to 4946\n",
      "Combined dataset shape: (67390, 4)\n",
      "Time_idx range in combined data: 0 to 4946\n",
      "Making predictions...\n",
      "Predictions completed.\n",
      "Prediction shape: torch.Size([46, 30])\n",
      "Error during prediction: index -60375 is out of bounds for axis 0 with size 46\n",
      "This might be due to insufficient historical data or model training issues.\n",
      "Creating fallback predictions based on historical averages...\n",
      "Fallback predictions generated: 6946 records\n",
      "Predictions completed.\n",
      "Prediction shape: torch.Size([46, 30])\n",
      "Error during prediction: index -60375 is out of bounds for axis 0 with size 46\n",
      "This might be due to insufficient historical data or model training issues.\n",
      "Creating fallback predictions based on historical averages...\n",
      "Fallback predictions generated: 6946 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Roaming\\Python\\Python312\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:433: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for 2025 using the trained TFT model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Get the last available data point for each rm_id\n",
    "last_data = df_hist.groupby(\"rm_id\").tail(max_encoder_length).reset_index(drop=True)\n",
    "\n",
    "# Define the prediction period (2025-01-01 to 2025-05-31)\n",
    "prediction_start = pd.Timestamp(\"2025-01-01\")\n",
    "prediction_end = pd.Timestamp(\"2025-05-31\")\n",
    "\n",
    "# Convert to time_idx based on the historical data's date range\n",
    "base_date = df_hist[\"date_arrival\"].min()\n",
    "start_time_idx = (prediction_start - base_date).days\n",
    "end_time_idx = (prediction_end - base_date).days\n",
    "\n",
    "print(f\"Historical data time_idx range: 0 to {df_hist['time_idx'].max()}\")\n",
    "print(f\"Prediction time_idx range: {start_time_idx} to {end_time_idx}\")\n",
    "\n",
    "# Create future time steps for the prediction period\n",
    "future_time_steps = list(range(start_time_idx, end_time_idx + 1))\n",
    "\n",
    "# Prepare future data for prediction\n",
    "future_data = []\n",
    "for rm_id in df_hist[\"rm_id\"].unique():\n",
    "    for future_time in future_time_steps:\n",
    "        future_data.append({\n",
    "            \"rm_id\": rm_id,\n",
    "            \"time_idx\": future_time,\n",
    "            \"net_weight\": 0,  # Placeholder - will be predicted\n",
    "        })\n",
    "\n",
    "future_df = pd.DataFrame(future_data)\n",
    "\n",
    "# Combine historical data with future data for prediction\n",
    "prediction_data = pd.concat([df_hist, future_df]).reset_index(drop=True)\n",
    "prediction_data = prediction_data.sort_values([\"rm_id\", \"time_idx\"]).reset_index(drop=True)\n",
    "\n",
    "print(f\"Combined dataset shape: {prediction_data.shape}\")\n",
    "print(f\"Time_idx range in combined data: {prediction_data['time_idx'].min()} to {prediction_data['time_idx'].max()}\")\n",
    "\n",
    "# Create prediction dataset\n",
    "try:\n",
    "    prediction_dataset = TimeSeriesDataSet.from_dataset(\n",
    "        training, \n",
    "        prediction_data, \n",
    "        predict=True, \n",
    "        stop_randomization=True\n",
    "    )\n",
    "    \n",
    "    # Create prediction dataloader\n",
    "    pred_dataloader = prediction_dataset.to_dataloader(train=False, batch_size=batch_size, num_workers=0)\n",
    "    \n",
    "    # Make predictions\n",
    "    print(\"Making predictions...\")\n",
    "    predictions = tft.predict(pred_dataloader, mode=\"prediction\", return_x=True)\n",
    "    \n",
    "    print(\"Predictions completed.\")\n",
    "    print(f\"Prediction shape: {predictions[0].shape}\")\n",
    "    \n",
    "    # Process predictions to create simulated receivals\n",
    "    predicted_values = predictions[0].cpu().numpy()\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results = []\n",
    "    \n",
    "    # Get the prediction data that corresponds to the future period\n",
    "    future_prediction_data = prediction_data[prediction_data[\"time_idx\"] >= start_time_idx].copy()\n",
    "    \n",
    "    for idx, row in future_prediction_data.iterrows():\n",
    "        if idx - len(df_hist) < len(predicted_values):\n",
    "            pred_idx = idx - len(df_hist)\n",
    "            \n",
    "            # Convert time_idx back to date\n",
    "            predicted_date = base_date + pd.Timedelta(days=int(row[\"time_idx\"]))\n",
    "            \n",
    "            # Get the predicted value (handle different output formats)\n",
    "            if predicted_values.ndim == 3:  # (batch, time, quantiles)\n",
    "                pred_value = predicted_values[pred_idx, 0, 3]  # Middle quantile (median)\n",
    "            elif predicted_values.ndim == 2:  # (batch, quantiles)\n",
    "                pred_value = predicted_values[pred_idx, 3]  # Middle quantile (median)\n",
    "            else:\n",
    "                pred_value = predicted_values[pred_idx]\n",
    "            \n",
    "            # Only include positive predictions\n",
    "            if pred_value > 0:\n",
    "                results.append({\n",
    "                    \"rm_id\": row[\"rm_id\"],\n",
    "                    \"time_idx\": row[\"time_idx\"],\n",
    "                    \"date_arrival\": predicted_date,\n",
    "                    \"net_weight\": pred_value,\n",
    "                })\n",
    "    \n",
    "    simulated_df = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"TFT-based forecasting complete.\")\n",
    "    print(f\"Total simulated receivals for 2025: {len(simulated_df)}\")\n",
    "    if len(simulated_df) > 0:\n",
    "        print(f\"Date range: {simulated_df['date_arrival'].min()} to {simulated_df['date_arrival'].max()}\")\n",
    "        print(f\"Weight range: {simulated_df['net_weight'].min():.2f} to {simulated_df['net_weight'].max():.2f}\")\n",
    "        print(simulated_df.head())\n",
    "        \n",
    "        # Save the results\n",
    "        simulated_df.to_csv(\"simulated_receivals_2025.csv\", index=False)\n",
    "        print(\"Results saved to simulated_receivals_2025.csv\")\n",
    "    else:\n",
    "        print(\"No positive predictions generated. Check model training and data.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during prediction: {str(e)}\")\n",
    "    print(\"This might be due to insufficient historical data or model training issues.\")\n",
    "    \n",
    "    # Create a fallback simple prediction based on historical patterns\n",
    "    print(\"Creating fallback predictions based on historical averages...\")\n",
    "    \n",
    "    # Calculate daily averages for each rm_id\n",
    "    daily_avg = df_hist.groupby(\"rm_id\")[\"net_weight\"].mean().reset_index()\n",
    "    \n",
    "    fallback_results = []\n",
    "    for _, avg_row in daily_avg.iterrows():\n",
    "        rm_id = avg_row[\"rm_id\"]\n",
    "        avg_weight = avg_row[\"net_weight\"]\n",
    "        \n",
    "        # Generate predictions for each day in the prediction period\n",
    "        for time_idx in future_time_steps:\n",
    "            pred_date = base_date + pd.Timedelta(days=int(time_idx))\n",
    "            \n",
    "            # Add some randomness to avoid identical predictions\n",
    "            random_factor = np.random.normal(1.0, 0.1)  # Â±10% variation\n",
    "            pred_weight = max(0, avg_weight * random_factor)\n",
    "            \n",
    "            if pred_weight > 0:\n",
    "                fallback_results.append({\n",
    "                    \"rm_id\": rm_id,\n",
    "                    \"time_idx\": time_idx,\n",
    "                    \"date_arrival\": pred_date,\n",
    "                    \"net_weight\": pred_weight,\n",
    "                })\n",
    "    \n",
    "    simulated_df = pd.DataFrame(fallback_results)\n",
    "    print(f\"Fallback predictions generated: {len(simulated_df)} records\")\n",
    "    simulated_df.to_csv(\"simulated_receivals_2025.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "008bb364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6946 simulated receivals\n",
      "Date range: 2024-12-31 10:15:00 to 2025-05-30 10:15:00\n"
     ]
    }
   ],
   "source": [
    "# Load submission template and prepare data\n",
    "sample_submission = pd.read_csv(\"../data/sample_submission.csv\")\n",
    "prediction_mapping = pd.read_csv(\"../data/prediction_mapping.csv\", parse_dates=[\"forecast_start_date\", \"forecast_end_date\"])\n",
    "\n",
    "# Initialize submission with zeros\n",
    "submission = sample_submission.copy()\n",
    "submission[\"predicted_weight\"] = 0.0\n",
    "\n",
    "# Merge with prediction mapping to get rm_id and date information\n",
    "submission = submission.merge(prediction_mapping, on=\"ID\")\n",
    "\n",
    "# Load the simulated receivals\n",
    "try:\n",
    "    simulated_df = pd.read_csv(\"simulated_receivals_2025.csv\", parse_dates=[\"date_arrival\"])\n",
    "    print(f\"Loaded {len(simulated_df)} simulated receivals\")\n",
    "    print(f\"Date range: {simulated_df['date_arrival'].min()} to {simulated_df['date_arrival'].max()}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: simulated_receivals_2025.csv not found. Please run the TFT prediction cell first.\")\n",
    "    simulated_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f2ab9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated 7844164248.18 total predicted weight across 30450 rows\n",
      "Non-zero predictions: 6900 out of 30450 rows\n"
     ]
    }
   ],
   "source": [
    "# Generate submission with proper 2025 date handling\n",
    "if len(simulated_df) > 0:\n",
    "    print(\"Processing TFT predictions for submission...\")\n",
    "    \n",
    "    for receival in simulated_df.itertuples():\n",
    "        rm_id = receival.rm_id\n",
    "        date_arrival = receival.date_arrival\n",
    "        net_weight = receival.net_weight\n",
    "        \n",
    "        # Convert rm_id to the same type as in submission (likely int or string)\n",
    "        try:\n",
    "            rm_id_converted = int(float(rm_id)) if isinstance(rm_id, str) else rm_id\n",
    "        except:\n",
    "            rm_id_converted = rm_id\n",
    "        \n",
    "        # Ensure date_arrival is timezone-naive for comparison\n",
    "        if hasattr(date_arrival, 'tz') and date_arrival.tz is not None:\n",
    "            date_arrival_naive = date_arrival.tz_localize(None)\n",
    "        else:\n",
    "            date_arrival_naive = date_arrival\n",
    "        \n",
    "        # Find matching rows in submission where:\n",
    "        # 1. rm_id matches\n",
    "        # 2. the forecast_end_date is on or after the predicted arrival date\n",
    "        # This means the receival could contribute to forecasts ending on or after that date\n",
    "        mask = (\n",
    "            (submission['rm_id'] == rm_id_converted) & \n",
    "            (submission['forecast_end_date'] >= date_arrival_naive)\n",
    "        )\n",
    "        \n",
    "        # Add the predicted weight to matching rows\n",
    "        # Apply a decay factor based on how far the forecast end date is from arrival\n",
    "        matched_rows = submission[mask].copy()\n",
    "        if len(matched_rows) > 0:\n",
    "            for idx in matched_rows.index:\n",
    "                forecast_end = submission.loc[idx, 'forecast_end_date']\n",
    "                days_diff = (forecast_end - date_arrival_naive).days\n",
    "                \n",
    "                # Apply decay factor - weight decreases as forecast extends further past arrival\n",
    "                decay_factor = max(0.1, 1.0 / (1.0 + days_diff * 0.1))\n",
    "                \n",
    "                submission.loc[idx, 'predicted_weight'] += net_weight * decay_factor\n",
    "\n",
    "    print(f\"Updated {submission['predicted_weight'].sum():.2f} total predicted weight across {len(submission)} rows\")\n",
    "    print(f\"Non-zero predictions: {(submission['predicted_weight'] > 0).sum()} out of {len(submission)} rows\")\n",
    "    \n",
    "    # Show some statistics\n",
    "    non_zero_submission = submission[submission['predicted_weight'] > 0]\n",
    "    if len(non_zero_submission) > 0:\n",
    "        print(f\"Weight statistics - Min: {non_zero_submission['predicted_weight'].min():.4f}, \"\n",
    "              f\"Max: {non_zero_submission['predicted_weight'].max():.4f}, \"\n",
    "              f\"Mean: {non_zero_submission['predicted_weight'].mean():.4f}\")\n",
    "else:\n",
    "    print(\"No simulated receivals available for submission generation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e759940",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = submission[[\"ID\", \"predicted_weight\"]]\n",
    "submission.to_csv(\"testing2025.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac12fc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"testing2025.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88b35b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     rm_id  predicted_weight\n",
      "191   4263      3.810537e+06\n",
      "149   3124      3.770091e+06\n",
      "160   3282      3.768698e+06\n",
      "83    2140      3.727930e+06\n",
      "148   3123      3.692572e+06\n",
      "150   3125      3.670355e+06\n",
      "190   4222      3.668641e+06\n",
      "156   3201      3.662033e+06\n",
      "147   3122      3.648571e+06\n",
      "174   3761      3.639693e+06\n",
      "172   3642      3.590106e+06\n",
      "181   3883      3.580041e+06\n",
      "152   3142      3.566936e+06\n",
      "169   3581      3.549365e+06\n",
      "159   3265      3.424846e+06\n",
      "185   4021      3.322451e+06\n",
      "136   2741      3.231165e+06\n",
      "187   4081      3.086082e+06\n",
      "151   3126      2.798929e+06\n",
      "176   3781      2.592025e+06\n",
      "142   2981      2.570669e+06\n",
      "171   3621      2.455260e+06\n",
      "161   3362      2.450508e+06\n",
      "90    2147      2.395735e+06\n",
      "186   4044      2.283560e+06\n",
      "75    2130      2.279215e+06\n",
      "182   3901      2.125484e+06\n",
      "192   4302      1.992285e+06\n",
      "180   3865      1.694056e+06\n",
      "162   3381      1.606221e+06\n",
      "74    2129      1.469537e+06\n",
      "71    2125      1.314172e+06\n",
      "78    2133      1.237740e+06\n",
      "76    2131      1.035908e+06\n",
      "80    2135      9.306905e+05\n",
      "79    2134      8.785403e+05\n",
      "85    2142      6.752214e+05\n",
      "87    2144      6.696652e+05\n",
      "77    2132      6.031198e+05\n",
      "189   4161      5.534569e+05\n",
      "70    2124      5.037669e+05\n",
      "88    2145      4.884350e+05\n",
      "170   3601      3.611170e+05\n",
      "86    2143      3.580487e+05\n",
      "163   3421      3.178987e+05\n",
      "103   2161      2.690544e+05\n"
     ]
    }
   ],
   "source": [
    "test_df = submission.merge(prediction_mapping, on=\"ID\")\n",
    "test_df = test_df.groupby(\"rm_id\", as_index=False).agg({\n",
    "    \"predicted_weight\": \"max\",\n",
    "}).sort_values(\"predicted_weight\", ascending=False)\n",
    "\n",
    "print(test_df[0:46])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8772cb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCALE SPECIFIC RM_ID DOWN\n",
    "testing_2130 = pd.read_csv(\"testing2025.csv\")\n",
    "testing_2130 = testing_2130.merge(prediction_mapping, on=\"ID\")\n",
    "\n",
    "testing_2130.loc[testing_2130['rm_id'] == 2130, 'predicted_weight'] *= 0.6\n",
    "\n",
    "testing = testing_2130.groupby(\"rm_id\", as_index=False).agg({\n",
    "    \"predicted_weight\": \"max\",\n",
    "}).sort_values(\"predicted_weight\", ascending=False)\n",
    "\n",
    "print(testing)\n",
    "testing_2130 = testing_2130[[\"ID\", \"predicted_weight\"]]\n",
    "\n",
    "testing_2130.to_csv(\"testing2025_2130_only.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
