{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b53d50a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test2024 = pd.read_csv(\"../validation/testing2024manual.csv\")\n",
    "prediction_mapping = pd.read_csv(\"../data/prediction_mapping.csv\")\n",
    "merged = test2024.merge(prediction_mapping, on=\"ID\")\n",
    "filtered = merged[merged.groupby(\"rm_id\")[\"predicted_weight\"].transform(\"sum\") > 0]\n",
    "agg_df = filtered.groupby(\"rm_id\", as_index=False).agg({\n",
    "    \"predicted_weight\": \"max\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "276ddcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_rm_ids = set(agg_df[\"rm_id\"])\n",
    "\n",
    "receivals = pd.read_csv(\"../data_cleaned/orders_with_receivals_detailed.csv\")\n",
    "receivals_filtered = receivals[receivals[\"rm_id\"].isin(used_rm_ids)]\n",
    "selected = receivals_filtered[[\"rm_id\", \"date_arrival\", \"net_weight\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7d0dad7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Roaming\\Python\\Python312\\site-packages\\pytorch_forecasting\\data\\timeseries\\_timeseries.py:1850: UserWarning: Min encoder length and/or min_prediction_idx and/or min prediction length and/or lags are too large for 4 series/groups which therefore are not present in the dataset index. This means no predictions can be made for those series. First 10 removed groups: [{'__group_id__rm_id': '3581.0'}, {'__group_id__rm_id': '3621.0'}, {'__group_id__rm_id': '4081.0'}, {'__group_id__rm_id': '4161.0'}]\n",
      "  warnings.warn(\n",
      "C:\\Users\\david\\AppData\\Roaming\\Python\\Python312\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:210: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "C:\\Users\\david\\AppData\\Roaming\\Python\\Python312\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:210: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\david\\AppData\\Roaming\\Python\\Python312\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 644    | train\n",
      "3  | prescalers                         | ModuleDict                      | 96     | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 1.9 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 1.8 K  | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.2 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 2.2 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 2.2 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544    | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.1 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 576    | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 576    | train\n",
      "20 | output_layer                       | Linear                          | 119    | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "19.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.6 K    Total params\n",
      "0.079     Total estimated model params size (MB)\n",
      "278       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "C:\\Users\\david\\AppData\\Roaming\\Python\\Python312\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 644    | train\n",
      "3  | prescalers                         | ModuleDict                      | 96     | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 1.9 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 1.8 K  | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.2 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 2.2 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 2.2 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544    | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.1 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 576    | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 576    | train\n",
      "20 | output_layer                       | Linear                          | 119    | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "19.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.6 K    Total params\n",
      "0.079     Total estimated model params size (MB)\n",
      "278       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "C:\\Users\\david\\AppData\\Roaming\\Python\\Python312\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\david\\AppData\\Roaming\\Python\\Python312\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (30) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "C:\\Users\\david\\AppData\\Roaming\\Python\\Python312\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\david\\AppData\\Roaming\\Python\\Python312\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (30) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/30 [00:00<?, ?it/s, v_num=8, train_loss_step=2.24e+3, train_loss_epoch=2.41e+3]         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Roaming\\Python\\Python312\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:492: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:11<00:00,  2.65it/s, v_num=8, train_loss_step=2.14e+3, train_loss_epoch=2.2e+3] \n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:11<00:00,  2.65it/s, v_num=8, train_loss_step=2.14e+3, train_loss_epoch=2.2e+3]\n"
     ]
    }
   ],
   "source": [
    "# --- TFT Model Training: Use all data for training, no validation split ---\n",
    "import pandas as pd\n",
    "from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet, GroupNormalizer, QuantileLoss\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "# Load historical data\n",
    "df_hist = receivals_filtered[[\"rm_id\", \"date_arrival\", \"net_weight\"]].copy()\n",
    "df_hist[\"date_arrival\"] = pd.to_datetime(df_hist[\"date_arrival\"])\n",
    "if hasattr(df_hist[\"date_arrival\"].dt, \"tz\") and df_hist[\"date_arrival\"].dt.tz is not None:\n",
    "    df_hist[\"date_arrival\"] = df_hist[\"date_arrival\"].dt.tz_localize(None)\n",
    "df_hist[\"rm_id\"] = df_hist[\"rm_id\"].astype(str)\n",
    "df_hist[\"time_idx\"] = (df_hist[\"date_arrival\"] - df_hist[\"date_arrival\"].min()).dt.days\n",
    "\n",
    "# Use all data for training\n",
    "train_data = df_hist.copy()\n",
    "\n",
    "max_encoder_length = 60\n",
    "max_prediction_length = 30\n",
    "batch_size = 64\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    train_data,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"net_weight\",\n",
    "    group_ids=[\"rm_id\"],\n",
    "    min_encoder_length=max_encoder_length,\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"rm_id\"],\n",
    "    time_varying_known_reals=[\"time_idx\"],\n",
    "    time_varying_unknown_reals=[\"net_weight\"],\n",
    "    target_normalizer=GroupNormalizer(groups=[\"rm_id\"], transformation=\"softplus\"),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    ")\n",
    "\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    output_size=7,\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=0,\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20,\n",
    "    accelerator=\"auto\",\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=30,\n",
    "    callbacks=[LearningRateMonitor(), EarlyStopping(monitor=\"train_loss\", patience=3)],\n",
    "    logger=TensorBoardLogger(\"lightning_logs\"),\n",
    "    enable_checkpointing=True,\n",
    ")\n",
    "\n",
    "trainer.fit(tft, train_dataloaders=train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "638e655a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historical data time_idx range: 0 to 4784\n",
      "Prediction time_idx range: 4796 to 4946\n",
      "Combined dataset shape: (61824, 4)\n",
      "Time_idx range in combined data: 0 to 4825\n",
      "Error during TFT prediction: filters should not remove entries all entries - check encoder/decoder lengths and lags\n",
      "Falling back to N-BEATS model for more accurate time series forecasting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Roaming\\Python\\Python312\\site-packages\\pytorch_forecasting\\data\\timeseries\\_timeseries.py:1850: UserWarning: Min encoder length and/or min_prediction_idx and/or min prediction length and/or lags are too large for 46 series/groups which therefore are not present in the dataset index. This means no predictions can be made for those series. First 10 removed groups: [{'__group_id__rm_id': '2124.0'}, {'__group_id__rm_id': '2125.0'}, {'__group_id__rm_id': '2129.0'}, {'__group_id__rm_id': '2130.0'}, {'__group_id__rm_id': '2131.0'}, {'__group_id__rm_id': '2132.0'}, {'__group_id__rm_id': '2133.0'}, {'__group_id__rm_id': '2134.0'}, {'__group_id__rm_id': '2135.0'}, {'__group_id__rm_id': '2140.0'}]\n",
      "  warnings.warn(\n",
      "C:\\Users\\david\\AppData\\Roaming\\Python\\Python312\\site-packages\\pytorch_forecasting\\data\\timeseries\\_timeseries.py:1850: UserWarning: Min encoder length and/or min_prediction_idx and/or min prediction length and/or lags are too large for 5 series/groups which therefore are not present in the dataset index. This means no predictions can be made for those series. First 10 removed groups: [{'__group_id__rm_id': '3581.0'}, {'__group_id__rm_id': '3621.0'}, {'__group_id__rm_id': '4044.0'}, {'__group_id__rm_id': '4081.0'}, {'__group_id__rm_id': '4161.0'}]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training N-BEATS model...\n",
      "N-BEATS also failed: only fixed prediction length is allowed, but max_prediction_length != min_prediction_length\n",
      "Using conservative historical scaling as final fallback...\n",
      "Conservative fallback generated: 2617 deliveries\n",
      "Total simulated receivals for 2025: 2617\n",
      "Date range: 2024-12-31 10:15:00 to 2025-05-30 10:15:00\n",
      "Weight range: 1073.88 to 35832.97\n",
      "\n",
      "Top predicted totals by rm_id:\n",
      "rm_id\n",
      "3124.0    3.890817e+06\n",
      "3125.0    3.711971e+06\n",
      "3282.0    3.346941e+06\n",
      "3122.0    3.219032e+06\n",
      "3123.0    2.975350e+06\n",
      "3126.0    2.836046e+06\n",
      "2130.0    2.588358e+06\n",
      "3781.0    2.544335e+06\n",
      "3901.0    1.734604e+06\n",
      "3865.0    1.695918e+06\n",
      "Name: net_weight, dtype: float64\n",
      "Results saved to simulated_receivals_2025.csv\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for 2025 using the trained TFT model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pytorch_forecasting import NBeats, TimeSeriesDataSet\n",
    "\n",
    "# Get the last available data point for each rm_id\n",
    "last_data = df_hist.groupby(\"rm_id\").tail(max_encoder_length).reset_index(drop=True)\n",
    "\n",
    "# Define the prediction period (2025-01-01 to 2025-05-31)\n",
    "prediction_start = pd.Timestamp(\"2025-01-01\")\n",
    "prediction_end = pd.Timestamp(\"2025-05-31\")\n",
    "\n",
    "# Convert to time_idx based on the historical data's date range\n",
    "base_date = df_hist[\"date_arrival\"].min()\n",
    "start_time_idx = (prediction_start - base_date).days\n",
    "end_time_idx = (prediction_end - base_date).days\n",
    "\n",
    "print(f\"Historical data time_idx range: 0 to {df_hist['time_idx'].max()}\")\n",
    "print(f\"Prediction time_idx range: {start_time_idx} to {end_time_idx}\")\n",
    "\n",
    "# Create future time steps for the prediction period\n",
    "future_time_steps = list(range(start_time_idx, end_time_idx + 1))\n",
    "\n",
    "# Prepare future data for prediction - but limit to reasonable batch size\n",
    "future_data = []\n",
    "for rm_id in df_hist[\"rm_id\"].unique():\n",
    "    for future_time in future_time_steps[:max_prediction_length]:  # Limit to model's max prediction length\n",
    "        future_data.append({\n",
    "            \"rm_id\": rm_id,\n",
    "            \"time_idx\": future_time,\n",
    "            \"net_weight\": 0,  # Placeholder - will be predicted\n",
    "        })\n",
    "\n",
    "future_df = pd.DataFrame(future_data)\n",
    "\n",
    "# Combine historical data with future data for prediction\n",
    "prediction_data = pd.concat([df_hist, future_df]).reset_index(drop=True)\n",
    "prediction_data = prediction_data.sort_values([\"rm_id\", \"time_idx\"]).reset_index(drop=True)\n",
    "\n",
    "print(f\"Combined dataset shape: {prediction_data.shape}\")\n",
    "print(f\"Time_idx range in combined data: {prediction_data['time_idx'].min()} to {prediction_data['time_idx'].max()}\")\n",
    "\n",
    "# Create prediction dataset\n",
    "try:\n",
    "    prediction_dataset = TimeSeriesDataSet.from_dataset(\n",
    "        training, \n",
    "        prediction_data, \n",
    "        predict=True, \n",
    "        stop_randomization=True\n",
    "    )\n",
    "    \n",
    "    # Create prediction dataloader\n",
    "    pred_dataloader = prediction_dataset.to_dataloader(train=False, batch_size=batch_size, num_workers=0)\n",
    "    \n",
    "    # Make predictions\n",
    "    print(\"Making TFT predictions...\")\n",
    "    predictions = tft.predict(pred_dataloader, mode=\"prediction\", return_x=True)\n",
    "    \n",
    "    print(\"TFT predictions completed.\")\n",
    "    print(f\"Prediction shape: {predictions[0].shape}\")\n",
    "    \n",
    "    # Process predictions to create simulated receivals\n",
    "    predicted_values = predictions[0].cpu().numpy()\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results = []\n",
    "    \n",
    "    # Get the prediction data that corresponds to the future period\n",
    "    future_prediction_data = prediction_data[prediction_data[\"time_idx\"] >= start_time_idx].copy()\n",
    "    \n",
    "    prediction_idx = 0\n",
    "    for _, row in future_prediction_data.iterrows():\n",
    "        if prediction_idx < len(predicted_values):\n",
    "            # Convert time_idx back to date\n",
    "            predicted_date = base_date + pd.Timedelta(days=int(row[\"time_idx\"]))\n",
    "            \n",
    "            # Get the predicted value (handle different output formats)\n",
    "            if predicted_values.ndim == 3:  # (batch, time, quantiles)\n",
    "                pred_value = predicted_values[prediction_idx, 0, 3]  # Middle quantile (median)\n",
    "            elif predicted_values.ndim == 2:  # (batch, quantiles)\n",
    "                pred_value = predicted_values[prediction_idx, 3]  # Middle quantile (median)\n",
    "            else:\n",
    "                pred_value = predicted_values[prediction_idx]\n",
    "            \n",
    "            # Only include positive predictions\n",
    "            if pred_value > 0:\n",
    "                results.append({\n",
    "                    \"rm_id\": row[\"rm_id\"],\n",
    "                    \"time_idx\": row[\"time_idx\"],\n",
    "                    \"date_arrival\": predicted_date,\n",
    "                    \"net_weight\": pred_value,\n",
    "                })\n",
    "            \n",
    "            prediction_idx += 1\n",
    "    \n",
    "    # Extend predictions for the full period if we only got a limited window\n",
    "    if len(results) > 0:\n",
    "        simulated_df = pd.DataFrame(results)\n",
    "        \n",
    "        # If we only have predictions for the first 30 days, extend them with patterns\n",
    "        if len(future_time_steps) > max_prediction_length:\n",
    "            print(f\"Extending TFT predictions from {max_prediction_length} days to {len(future_time_steps)} days...\")\n",
    "            \n",
    "            # Calculate patterns from the predicted period\n",
    "            extended_results = results.copy()\n",
    "            \n",
    "            for rm_id in df_hist[\"rm_id\"].unique():\n",
    "                rm_predictions = [r for r in results if r[\"rm_id\"] == rm_id]\n",
    "                if rm_predictions:\n",
    "                    # Calculate average and trend from initial predictions\n",
    "                    weights = [r[\"net_weight\"] for r in rm_predictions]\n",
    "                    avg_weight = np.mean(weights)\n",
    "                    \n",
    "                    # Extend to remaining days with decay\n",
    "                    for time_idx in future_time_steps[max_prediction_length:]:\n",
    "                        pred_date = base_date + pd.Timedelta(days=int(time_idx))\n",
    "                        \n",
    "                        # Apply gradual decay (reduce predictions over time)\n",
    "                        days_beyond = time_idx - future_time_steps[max_prediction_length-1]\n",
    "                        decay_factor = max(0.1, 1.0 / (1.0 + days_beyond * 0.02))  # 2% decay per day\n",
    "                        \n",
    "                        pred_weight = avg_weight * decay_factor * np.random.normal(1.0, 0.05)  # 5% noise\n",
    "                        \n",
    "                        if pred_weight > 0:\n",
    "                            extended_results.append({\n",
    "                                \"rm_id\": rm_id,\n",
    "                                \"time_idx\": time_idx,\n",
    "                                \"date_arrival\": pred_date,\n",
    "                                \"net_weight\": pred_weight,\n",
    "                            })\n",
    "            \n",
    "            simulated_df = pd.DataFrame(extended_results)\n",
    "    else:\n",
    "        simulated_df = pd.DataFrame()\n",
    "    \n",
    "    print(\"TFT-based forecasting complete.\")\n",
    "    print(f\"Total simulated receivals for 2025: {len(simulated_df)}\")\n",
    "    if len(simulated_df) > 0:\n",
    "        print(f\"Date range: {simulated_df['date_arrival'].min()} to {simulated_df['date_arrival'].max()}\")\n",
    "        print(f\"Weight range: {simulated_df['net_weight'].min():.2f} to {simulated_df['net_weight'].max():.2f}\")\n",
    "        print(simulated_df.head())\n",
    "        \n",
    "        # Save the results\n",
    "        simulated_df.to_csv(\"simulated_receivals_2025.csv\", index=False)\n",
    "        print(\"Results saved to simulated_receivals_2025.csv\")\n",
    "    else:\n",
    "        raise Exception(\"No predictions generated from TFT model\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during TFT prediction: {str(e)}\")\n",
    "    print(\"Falling back to N-BEATS model for more accurate time series forecasting...\")\n",
    "    \n",
    "    # N-BEATS FALLBACK - Much better than simple averages!\n",
    "    try:\n",
    "        # Prepare data for N-BEATS (simpler requirements than TFT)\n",
    "        nbeats_encoder_length = 30  # Shorter than TFT\n",
    "        nbeats_prediction_length = 30\n",
    "        \n",
    "        # Create N-BEATS dataset\n",
    "        nbeats_training = TimeSeriesDataSet(\n",
    "            df_hist,\n",
    "            time_idx=\"time_idx\",\n",
    "            target=\"net_weight\",\n",
    "            group_ids=[\"rm_id\"],\n",
    "            min_encoder_length=nbeats_encoder_length,\n",
    "            max_encoder_length=nbeats_encoder_length,\n",
    "            min_prediction_length=1,\n",
    "            max_prediction_length=nbeats_prediction_length,\n",
    "            static_categoricals=[\"rm_id\"],\n",
    "            time_varying_known_reals=[\"time_idx\"],\n",
    "            time_varying_unknown_reals=[\"net_weight\"],\n",
    "            add_relative_time_idx=True,\n",
    "            allow_missing_timesteps=True,\n",
    "        )\n",
    "        \n",
    "        nbeats_dataloader = nbeats_training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "        \n",
    "        # Create and train N-BEATS model\n",
    "        print(\"Training N-BEATS model...\")\n",
    "        nbeats = NBeats.from_dataset(\n",
    "            nbeats_training,\n",
    "            learning_rate=0.02,\n",
    "            log_interval=0,\n",
    "            stack_types=[\"trend\", \"seasonality\"],  # Focus on trend and seasonality\n",
    "            num_blocks=[3, 3],\n",
    "            num_block_layers=[4, 4],\n",
    "            widths=[256, 256],\n",
    "            sharing=[True, True],\n",
    "        )\n",
    "        \n",
    "        nbeats_trainer = pl.Trainer(\n",
    "            max_epochs=15,  # Faster training than TFT\n",
    "            accelerator=\"auto\",\n",
    "            limit_train_batches=20,\n",
    "            callbacks=[EarlyStopping(monitor=\"train_loss\", patience=2)],\n",
    "            enable_checkpointing=False,\n",
    "            logger=False,\n",
    "        )\n",
    "        \n",
    "        nbeats_trainer.fit(nbeats, train_dataloaders=nbeats_dataloader)\n",
    "        \n",
    "        # Prepare prediction data for N-BEATS\n",
    "        nbeats_future_data = []\n",
    "        for rm_id in df_hist[\"rm_id\"].unique():\n",
    "            for future_time in future_time_steps[:nbeats_prediction_length]:\n",
    "                nbeats_future_data.append({\n",
    "                    \"rm_id\": rm_id,\n",
    "                    \"time_idx\": future_time,\n",
    "                    \"net_weight\": 0,\n",
    "                })\n",
    "        \n",
    "        nbeats_future_df = pd.DataFrame(nbeats_future_data)\n",
    "        nbeats_prediction_data = pd.concat([df_hist, nbeats_future_df]).reset_index(drop=True)\n",
    "        nbeats_prediction_data = nbeats_prediction_data.sort_values([\"rm_id\", \"time_idx\"]).reset_index(drop=True)\n",
    "        \n",
    "        # Create N-BEATS prediction dataset\n",
    "        nbeats_pred_dataset = TimeSeriesDataSet.from_dataset(\n",
    "            nbeats_training, \n",
    "            nbeats_prediction_data, \n",
    "            predict=True, \n",
    "            stop_randomization=True\n",
    "        )\n",
    "        \n",
    "        nbeats_pred_dataloader = nbeats_pred_dataset.to_dataloader(train=False, batch_size=batch_size, num_workers=0)\n",
    "        \n",
    "        # Make N-BEATS predictions\n",
    "        print(\"Making N-BEATS predictions...\")\n",
    "        nbeats_predictions = nbeats.predict(nbeats_pred_dataloader, mode=\"prediction\", return_x=True)\n",
    "        nbeats_predicted_values = nbeats_predictions[0].cpu().numpy()\n",
    "        \n",
    "        print(f\"N-BEATS prediction shape: {nbeats_predicted_values.shape}\")\n",
    "        \n",
    "        # Process N-BEATS predictions\n",
    "        nbeats_results = []\n",
    "        nbeats_future_pred_data = nbeats_prediction_data[nbeats_prediction_data[\"time_idx\"] >= start_time_idx].copy()\n",
    "        \n",
    "        pred_idx = 0\n",
    "        for _, row in nbeats_future_pred_data.iterrows():\n",
    "            if pred_idx < len(nbeats_predicted_values):\n",
    "                predicted_date = base_date + pd.Timedelta(days=int(row[\"time_idx\"]))\n",
    "                \n",
    "                # Get predicted value\n",
    "                if nbeats_predicted_values.ndim == 2:\n",
    "                    pred_value = nbeats_predicted_values[pred_idx, 0]\n",
    "                else:\n",
    "                    pred_value = nbeats_predicted_values[pred_idx]\n",
    "                \n",
    "                if pred_value > 0:\n",
    "                    nbeats_results.append({\n",
    "                        \"rm_id\": row[\"rm_id\"],\n",
    "                        \"time_idx\": row[\"time_idx\"],\n",
    "                        \"date_arrival\": predicted_date,\n",
    "                        \"net_weight\": pred_value,\n",
    "                    })\n",
    "                \n",
    "                pred_idx += 1\n",
    "        \n",
    "        # Extend N-BEATS predictions for full period with realistic patterns\n",
    "        if len(nbeats_results) > 0 and len(future_time_steps) > nbeats_prediction_length:\n",
    "            print(f\"Extending N-BEATS predictions to full period...\")\n",
    "            \n",
    "            extended_nbeats = nbeats_results.copy()\n",
    "            \n",
    "            # Calculate historical seasonality and trends for each rm_id\n",
    "            for rm_id in df_hist[\"rm_id\"].unique():\n",
    "                rm_hist = df_hist[df_hist[\"rm_id\"] == rm_id].copy()\n",
    "                rm_nbeats_pred = [r for r in nbeats_results if r[\"rm_id\"] == rm_id]\n",
    "                \n",
    "                if len(rm_nbeats_pred) > 0 and len(rm_hist) > 0:\n",
    "                    # Get trend from N-BEATS predictions\n",
    "                    pred_weights = [r[\"net_weight\"] for r in rm_nbeats_pred]\n",
    "                    avg_pred_weight = np.mean(pred_weights)\n",
    "                    \n",
    "                    # Calculate historical total for realistic scaling\n",
    "                    hist_2024 = rm_hist[rm_hist[\"date_arrival\"].dt.year == 2024]\n",
    "                    if len(hist_2024) > 0:\n",
    "                        total_2024 = hist_2024[\"net_weight\"].sum()\n",
    "                        # Scale predictions to be more realistic (e.g., similar to 2024 levels)\n",
    "                        scale_factor = min(1.0, total_2024 / (avg_pred_weight * len(future_time_steps)))\n",
    "                    else:\n",
    "                        scale_factor = 0.5  # Conservative scaling\n",
    "                    \n",
    "                    # Extend with decay and seasonality\n",
    "                    for time_idx in future_time_steps[nbeats_prediction_length:]:\n",
    "                        pred_date = base_date + pd.Timedelta(days=int(time_idx))\n",
    "                        \n",
    "                        # Seasonal component (weekly pattern)\n",
    "                        day_of_week = pred_date.dayofweek\n",
    "                        seasonal_factor = 0.8 + 0.4 * np.sin(2 * np.pi * day_of_week / 7)\n",
    "                        \n",
    "                        # Time decay\n",
    "                        days_beyond = time_idx - future_time_steps[nbeats_prediction_length-1]\n",
    "                        decay_factor = max(0.05, np.exp(-days_beyond * 0.01))  # Exponential decay\n",
    "                        \n",
    "                        pred_weight = avg_pred_weight * scale_factor * seasonal_factor * decay_factor\n",
    "                        \n",
    "                        # Add some randomness but keep it realistic\n",
    "                        pred_weight *= np.random.uniform(0.7, 1.3)\n",
    "                        \n",
    "                        if pred_weight > 0.1:  # Only meaningful predictions\n",
    "                            extended_nbeats.append({\n",
    "                                \"rm_id\": rm_id,\n",
    "                                \"time_idx\": time_idx,\n",
    "                                \"date_arrival\": pred_date,\n",
    "                                \"net_weight\": pred_weight,\n",
    "                            })\n",
    "            \n",
    "            simulated_df = pd.DataFrame(extended_nbeats)\n",
    "        else:\n",
    "            simulated_df = pd.DataFrame(nbeats_results)\n",
    "        \n",
    "        print(\"N-BEATS forecasting complete!\")\n",
    "        \n",
    "    except Exception as nbeats_error:\n",
    "        print(f\"N-BEATS also failed: {str(nbeats_error)}\")\n",
    "        print(\"Using conservative historical scaling as final fallback...\")\n",
    "        \n",
    "        # CONSERVATIVE FALLBACK: Scale historical totals\n",
    "        fallback_results = []\n",
    "        \n",
    "        for rm_id in df_hist[\"rm_id\"].unique():\n",
    "            rm_hist = df_hist[df_hist[\"rm_id\"] == rm_id]\n",
    "            \n",
    "            # Get 2024 total (most recent full year)\n",
    "            hist_2024 = rm_hist[rm_hist[\"date_arrival\"].dt.year == 2024]\n",
    "            if len(hist_2024) > 0:\n",
    "                total_2024 = hist_2024[\"net_weight\"].sum()\n",
    "                num_deliveries_2024 = len(hist_2024)\n",
    "                \n",
    "                # Conservative prediction: 60-80% of 2024 levels\n",
    "                conservative_factor = np.random.uniform(0.6, 0.8)\n",
    "                expected_total_2025 = total_2024 * conservative_factor\n",
    "                \n",
    "                # Distribute over fewer deliveries (not every day!)\n",
    "                expected_deliveries = max(1, int(num_deliveries_2024 * conservative_factor))\n",
    "                \n",
    "                if expected_deliveries > 0 and expected_total_2025 > 0:\n",
    "                    avg_delivery_weight = expected_total_2025 / expected_deliveries\n",
    "                    \n",
    "                    # Random delivery dates\n",
    "                    delivery_dates = np.random.choice(\n",
    "                        len(future_time_steps), \n",
    "                        size=min(expected_deliveries, len(future_time_steps)), \n",
    "                        replace=False\n",
    "                    )\n",
    "                    \n",
    "                    for date_idx in delivery_dates:\n",
    "                        time_idx = future_time_steps[date_idx]\n",
    "                        pred_date = base_date + pd.Timedelta(days=int(time_idx))\n",
    "                        \n",
    "                        # Vary delivery weights\n",
    "                        delivery_weight = avg_delivery_weight * np.random.uniform(0.7, 1.3)\n",
    "                        \n",
    "                        fallback_results.append({\n",
    "                            \"rm_id\": rm_id,\n",
    "                            \"time_idx\": time_idx,\n",
    "                            \"date_arrival\": pred_date,\n",
    "                            \"net_weight\": delivery_weight,\n",
    "                        })\n",
    "        \n",
    "        simulated_df = pd.DataFrame(fallback_results)\n",
    "        print(f\"Conservative fallback generated: {len(simulated_df)} deliveries\")\n",
    "\n",
    "    # Final output\n",
    "    if len(simulated_df) > 0:\n",
    "        print(f\"Total simulated receivals for 2025: {len(simulated_df)}\")\n",
    "        print(f\"Date range: {simulated_df['date_arrival'].min()} to {simulated_df['date_arrival'].max()}\")\n",
    "        print(f\"Weight range: {simulated_df['net_weight'].min():.2f} to {simulated_df['net_weight'].max():.2f}\")\n",
    "        \n",
    "        # Show totals by rm_id for verification\n",
    "        totals_by_rm = simulated_df.groupby(\"rm_id\")[\"net_weight\"].sum().sort_values(ascending=False)\n",
    "        print(f\"\\nTop predicted totals by rm_id:\")\n",
    "        print(totals_by_rm.head(10))\n",
    "        \n",
    "        simulated_df.to_csv(\"simulated_receivals_2025.csv\", index=False)\n",
    "        print(\"Results saved to simulated_receivals_2025.csv\")\n",
    "    else:\n",
    "        print(\"No predictions generated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bc687980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PREDICTION VERIFICATION (Janâ€“May) ===\n",
      "Comparison of 2025 Janâ€“May predictions vs 2024 Janâ€“May actual:\n",
      "rm_id\t2024_JanMay\t2025_JanMay_pred\tratio\n",
      "3124.0\t2407820\t\t3861904\t\t1.60\n",
      "3125.0\t3028260\t\t3680603\t\t1.22\n",
      "3282.0\t2373080\t\t3314825\t\t1.40\n",
      "3122.0\t2183320\t\t3192046\t\t1.46\n",
      "3123.0\t1782880\t\t2956522\t\t1.66\n",
      "3126.0\t2998700\t\t2817530\t\t0.94\n",
      "2130.0\t3549704\t\t2572618\t\t0.72\n",
      "3781.0\t6528018\t\t2529385\t\t0.39\n",
      "3901.0\t857880\t\t1718956\t\t2.00\n",
      "3865.0\t5801072\t\t1684277\t\t0.29\n",
      "2140.0\t1046440\t\t1416831\t\t1.35\n",
      "2134.0\t612846\t\t1082541\t\t1.77\n",
      "2135.0\t494030\t\t598706\t\t1.21\n",
      "2142.0\t445868\t\t563450\t\t1.26\n",
      "3265.0\t576140\t\t464798\t\t0.81\n",
      "\n",
      "Warning: 21 rm_ids predicted >1.5x their 2024 Janâ€“May levels:\n",
      "rm_id\n",
      "3124.0    1.603901\n",
      "3123.0    1.658285\n",
      "3901.0    2.003726\n",
      "2134.0    1.766416\n",
      "3362.0    4.575627\n",
      "2144.0    1.757154\n",
      "4222.0    5.039979\n",
      "2131.0    1.567518\n",
      "2145.0    1.886724\n",
      "2741.0    2.011615\n",
      "Name: ratio_2025_to_2024_JanMay, dtype: float64\n",
      "\n",
      "Info: 1 rm_ids predicted <30% of their 2024 Janâ€“May levels\n",
      "\n",
      "Overall statistics (Janâ€“May):\n",
      "- Average ratio: 1.66\n",
      "- Median ratio: 1.43\n",
      "- Total 2024 Janâ€“May: 37542429\n",
      "- Total 2025 Janâ€“May predicted: 37279018\n",
      "- Overall ratio: 0.99\n"
     ]
    }
   ],
   "source": [
    "# Verify predictions are realistic by comparing to 2024 Janâ€“May actuals\n",
    "print(\"\\n=== PREDICTION VERIFICATION (Janâ€“May) ===\")\n",
    "\n",
    "if len(simulated_df) > 0:\n",
    "    jan_may_mask_2024 = (df_hist[\"date_arrival\"] >= pd.Timestamp(\"2024-01-01\")) & (df_hist[\"date_arrival\"] <= pd.Timestamp(\"2024-05-31\"))\n",
    "    hist_2024_janm_total = df_hist.loc[jan_may_mask_2024].groupby(\"rm_id\")[\"net_weight\"].sum()\n",
    "\n",
    "    jan_may_mask_2025 = (simulated_df[\"date_arrival\"] >= pd.Timestamp(\"2025-01-01\")) & (simulated_df[\"date_arrival\"] <= pd.Timestamp(\"2025-05-31\"))\n",
    "    pred_2025_janm_total = simulated_df.loc[jan_may_mask_2025].groupby(\"rm_id\")[\"net_weight\"].sum()\n",
    "\n",
    "    comparison = pd.DataFrame({\n",
    "        \"hist_2024_JanMay\": hist_2024_janm_total,\n",
    "        \"pred_2025_JanMay\": pred_2025_janm_total\n",
    "    }).fillna(0)\n",
    "\n",
    "    comparison[\"ratio_2025_to_2024_JanMay\"] = comparison[\"pred_2025_JanMay\"] / (comparison[\"hist_2024_JanMay\"] + 1e-6)\n",
    "    comparison = comparison.sort_values(\"pred_2025_JanMay\", ascending=False)\n",
    "\n",
    "    print(\"Comparison of 2025 Janâ€“May predictions vs 2024 Janâ€“May actual:\")\n",
    "    print(\"rm_id\\t2024_JanMay\\t2025_JanMay_pred\\tratio\")\n",
    "    for rm_id, row in comparison.head(15).iterrows():\n",
    "        print(f\"{rm_id}\\t{row['hist_2024_JanMay']:.0f}\\t\\t{row['pred_2025_JanMay']:.0f}\\t\\t{row['ratio_2025_to_2024_JanMay']:.2f}\")\n",
    "\n",
    "    over_predicted = comparison[comparison[\"ratio_2025_to_2024_JanMay\"] > 1.5]\n",
    "    if len(over_predicted) > 0:\n",
    "        print(f\"\\nWarning: {len(over_predicted)} rm_ids predicted >1.5x their 2024 Janâ€“May levels:\")\n",
    "        print(over_predicted[\"ratio_2025_to_2024_JanMay\"].head(10))\n",
    "\n",
    "    under_predicted = comparison[comparison[\"ratio_2025_to_2024_JanMay\"] < 0.3]\n",
    "    if len(under_predicted) > 0:\n",
    "        print(f\"\\nInfo: {len(under_predicted)} rm_ids predicted <30% of their 2024 Janâ€“May levels\")\n",
    "\n",
    "    print(f\"\\nOverall statistics (Janâ€“May):\")\n",
    "    print(f\"- Average ratio: {comparison['ratio_2025_to_2024_JanMay'].mean():.2f}\")\n",
    "    print(f\"- Median ratio: {comparison['ratio_2025_to_2024_JanMay'].median():.2f}\")\n",
    "    print(f\"- Total 2024 Janâ€“May: {comparison['hist_2024_JanMay'].sum():.0f}\")\n",
    "    print(f\"- Total 2025 Janâ€“May predicted: {comparison['pred_2025_JanMay'].sum():.0f}\")\n",
    "    print(f\"- Overall ratio: {comparison['pred_2025_JanMay'].sum() / (comparison['hist_2024_JanMay'].sum() + 1e-6):.2f}\")\n",
    "else:\n",
    "    print(\"No predictions to verify!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ec8cdb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration disabled: wrote original predictions to simulated_receivals_2025.csv\n"
     ]
    }
   ],
   "source": [
    "# Calibrate predictions: optional; if CALIBRATION_MODE='off', pass-through without caps/scaling\n",
    "CALIBRATION_MODE = \"off\"  # options: 'off' | 'cap' | 'scale'\n",
    "CAL_TOLERANCE = 1.05       # used only when mode is 'cap' or 'scale'\n",
    "\n",
    "if len(simulated_df) > 0:\n",
    "    if CALIBRATION_MODE == \"off\":\n",
    "        # Write original predictions unchanged to canonical file\n",
    "        simulated_df.to_csv(\"simulated_receivals_2025.csv\", index=False)\n",
    "        print(\"Calibration disabled: wrote original predictions to simulated_receivals_2025.csv\")\n",
    "    else:\n",
    "        # Build 2024 Janâ€“May baseline\n",
    "        jan_may_mask_2024 = (df_hist[\"date_arrival\"] >= pd.Timestamp(\"2024-01-01\")) & (df_hist[\"date_arrival\"] <= pd.Timestamp(\"2024-05-31\"))\n",
    "        baseline_2024 = df_hist.loc[jan_may_mask_2024].groupby(\"rm_id\")[\"net_weight\"].sum()\n",
    "\n",
    "        # Focus on predicted Janâ€“May 2025\n",
    "        jan_may_mask_2025 = (simulated_df[\"date_arrival\"] >= pd.Timestamp(\"2025-01-01\")) & (simulated_df[\"date_arrival\"] <= pd.Timestamp(\"2025-05-31\"))\n",
    "        preds_janm = simulated_df.loc[jan_may_mask_2025].copy()\n",
    "\n",
    "        # Compute per-rm_id current totals\n",
    "        current_totals = preds_janm.groupby(\"rm_id\")[\"net_weight\"].sum()\n",
    "\n",
    "        # Prepare calibrated copy\n",
    "        calibrated = preds_janm.copy()\n",
    "\n",
    "        # Apply per-rm_id calibration\n",
    "        for rm_id, total_2025 in current_totals.items():\n",
    "            baseline = float(baseline_2024.get(rm_id, 0.0))\n",
    "            cap_value = baseline * CAL_TOLERANCE\n",
    "            if baseline <= 0:\n",
    "                # If no baseline, keep as-is but clip extreme weights\n",
    "                rm_mask = calibrated[\"rm_id\"] == rm_id\n",
    "                calibrated.loc[rm_mask, \"net_weight\"] = calibrated.loc[rm_mask, \"net_weight\"].clip(upper=calibrated.loc[rm_mask, \"net_weight\"].quantile(0.95))\n",
    "                continue\n",
    "\n",
    "            if CALIBRATION_MODE == \"cap\":\n",
    "                if total_2025 > cap_value:\n",
    "                    scale = cap_value / (total_2025 + 1e-6)\n",
    "                    rm_mask = calibrated[\"rm_id\"] == rm_id\n",
    "                    calibrated.loc[rm_mask, \"net_weight\"] *= scale\n",
    "            elif CALIBRATION_MODE == \"scale\":\n",
    "                # Scale towards baseline (not below 80% of baseline)\n",
    "                target = max(0.8 * baseline, min(cap_value, total_2025))\n",
    "                scale = target / (total_2025 + 1e-6)\n",
    "                rm_mask = calibrated[\"rm_id\"] == rm_id\n",
    "                calibrated.loc[rm_mask, \"net_weight\"] *= scale\n",
    "\n",
    "        # Merge calibrated Janâ€“May back with any predictions outside the window (shouldn't be many)\n",
    "        outside_mask = ~jan_may_mask_2025\n",
    "        outside = simulated_df.loc[outside_mask].copy()\n",
    "\n",
    "        # Replace simulated_df with calibrated version for downstream\n",
    "        simulated_df = pd.concat([calibrated, outside], ignore_index=True)\n",
    "\n",
    "        # Overwrite canonical file name with calibrated content as requested\n",
    "        simulated_df.to_csv(\"simulated_receivals_2025.csv\", index=False)\n",
    "        print(\"Calibrated predictions saved to simulated_receivals_2025.csv\")\n",
    "else:\n",
    "    print(\"No predictions found to calibrate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "008bb364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2617 simulated receivals from simulated_receivals_2025.csv\n",
      "Date range: 2024-12-31 10:15:00 to 2025-05-30 10:15:00\n"
     ]
    }
   ],
   "source": [
    "# Load submission template and prepare data\n",
    "sample_submission = pd.read_csv(\"../data/sample_submission.csv\")\n",
    "prediction_mapping = pd.read_csv(\"../data/prediction_mapping.csv\", parse_dates=[\"forecast_start_date\", \"forecast_end_date\"])\n",
    "\n",
    "# Initialize submission with zeros\n",
    "submission = sample_submission.copy()\n",
    "submission[\"predicted_weight\"] = 0.0\n",
    "\n",
    "# Merge with prediction mapping to get rm_id and date information\n",
    "submission = submission.merge(prediction_mapping, on=\"ID\")\n",
    "\n",
    "# Always load canonical calibrated output\n",
    "try:\n",
    "    simulated_df = pd.read_csv(\"simulated_receivals_2025.csv\", parse_dates=[\"date_arrival\"])  # canonical name\n",
    "    print(f\"Loaded {len(simulated_df)} simulated receivals from simulated_receivals_2025.csv\")\n",
    "    print(f\"Date range: {simulated_df['date_arrival'].min()} to {simulated_df['date_arrival'].max()}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: simulated_receivals_2025.csv not found. Please run the prediction+calibration cells first.\")\n",
    "    simulated_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9f2ab9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing calibrated predictions for submission with normalized allocation...\n",
      "Updated 37553415.03 total predicted weight across 30450 rows\n",
      "Non-zero predictions: 6060 out of 30450 rows\n",
      "Weight stats - Min: 0.2371, Max: 99144.2589, Mean: 6196.9332\n",
      "Updated 37553415.03 total predicted weight across 30450 rows\n",
      "Non-zero predictions: 6060 out of 30450 rows\n",
      "Weight stats - Min: 0.2371, Max: 99144.2589, Mean: 6196.9332\n"
     ]
    }
   ],
   "source": [
    "# Generate submission with cumulative allocation across forecast_end_date >= arrival\n",
    "ALLOCATION_MODE = \"cumulative\"  # options: 'cumulative' | 'normalized'\n",
    "ALLOCATION_ALPHA = 0.05  # decay strength (only used for cumulative)\n",
    "\n",
    "if len(simulated_df) > 0:\n",
    "    print(f\"Processing calibrated predictions for submission with {ALLOCATION_MODE} allocation...\")\n",
    "\n",
    "    # Pre-compute availability of rm_ids\n",
    "    rm_ids_in_submission = set(submission[\"rm_id\"].unique())\n",
    "\n",
    "    for receival in simulated_df.itertuples():\n",
    "        rm_id = receival.rm_id\n",
    "        date_arrival = receival.date_arrival\n",
    "        net_weight = float(receival.net_weight)\n",
    "\n",
    "        # Normalize rm_id type\n",
    "        try:\n",
    "            rm_id_converted = int(float(rm_id)) if isinstance(rm_id, str) else int(rm_id)\n",
    "        except Exception:\n",
    "            rm_id_converted = rm_id\n",
    "\n",
    "        # Ensure naive timestamp\n",
    "        if hasattr(date_arrival, 'tz') and date_arrival.tz is not None:\n",
    "            date_arrival_naive = date_arrival.tz_localize(None)\n",
    "        else:\n",
    "            date_arrival_naive = date_arrival\n",
    "\n",
    "        if rm_id_converted not in rm_ids_in_submission:\n",
    "            continue\n",
    "\n",
    "        # Rows to update: all forecast_end_date >= arrival (cumulative semantics)\n",
    "        mask = (\n",
    "            (submission['rm_id'] == rm_id_converted) &\n",
    "            (submission['forecast_end_date'] >= date_arrival_naive)\n",
    "        )\n",
    "        idxs = submission.index[mask]\n",
    "        if len(idxs) == 0:\n",
    "            continue\n",
    "\n",
    "        if ALLOCATION_MODE == \"normalized\":\n",
    "            # Distribute net_weight across rows, sums to net_weight (non-cumulative)\n",
    "            days_diffs = (submission.loc[idxs, 'forecast_end_date'] - date_arrival_naive).dt.days.astype(float)\n",
    "            raw_w = np.exp(-0.05 * days_diffs.clip(lower=0))\n",
    "            w_sum = raw_w.sum()\n",
    "            if w_sum > 0:\n",
    "                weights = raw_w / w_sum\n",
    "                submission.loc[idxs, 'predicted_weight'] += net_weight * weights.values\n",
    "        else:\n",
    "            # Cumulative: add a decayed contribution to EVERY future end date\n",
    "            days_diffs = (submission.loc[idxs, 'forecast_end_date'] - date_arrival_naive).dt.days.astype(float)\n",
    "            decay = 1.0 / (1.0 + ALLOCATION_ALPHA * days_diffs.clip(lower=0))\n",
    "            submission.loc[idxs, 'predicted_weight'] += net_weight * decay.values\n",
    "\n",
    "    print(f\"Updated {submission['predicted_weight'].sum():.2f} total predicted weight across {len(submission)} rows\")\n",
    "    non_zero = (submission['predicted_weight'] > 0).sum()\n",
    "    print(f\"Non-zero predictions: {non_zero} out of {len(submission)} rows\")\n",
    "\n",
    "    # Diagnostics\n",
    "    non_zero_submission = submission[submission['predicted_weight'] > 0]\n",
    "    if len(non_zero_submission) > 0:\n",
    "        print(\n",
    "            f\"Weight stats - Min: {non_zero_submission['predicted_weight'].min():.4f}, \"\n",
    "            f\"Max: {non_zero_submission['predicted_weight'].max():.4f}, \"\n",
    "            f\"Mean: {non_zero_submission['predicted_weight'].mean():.4f}\"\n",
    "        )\n",
    "else:\n",
    "    print(\"No simulated receivals available for submission generation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3e759940",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = submission[[\"ID\", \"predicted_weight\"]]\n",
    "submission.to_csv(\"testing2025.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ac12fc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"testing2025.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "88b35b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     rm_id  predicted_weight\n",
      "149   3124      99144.258919\n",
      "150   3125      91633.846489\n",
      "160   3282      88123.074305\n",
      "147   3122      85736.640816\n",
      "148   3123      71091.827661\n",
      "151   3126      70674.156580\n",
      "176   3781      63329.346191\n",
      "75    2130      58686.515401\n",
      "182   3901      52494.491984\n",
      "180   3865      38617.395106\n",
      "185   4021      27746.738498\n",
      "79    2134      24364.829171\n",
      "80    2135      22212.267677\n",
      "83    2140      18603.411995\n",
      "85    2142      13418.466426\n",
      "76    2131       9564.192515\n",
      "77    2132       9009.685543\n",
      "88    2145       8832.662061\n",
      "136   2741       8483.566907\n",
      "163   3421       6762.052542\n",
      "86    2143       6576.823541\n",
      "87    2144       6531.963428\n",
      "161   3362       5716.555905\n",
      "190   4222       5614.832178\n",
      "159   3265       5354.580240\n",
      "181   3883       5260.071897\n",
      "152   3142       4058.500316\n",
      "191   4263       3580.568614\n",
      "174   3761       3351.112677\n",
      "172   3642       2633.257231\n",
      "171   3621       2402.296552\n",
      "90    2147       2169.614730\n",
      "71    2125       2143.899314\n",
      "74    2129       1811.885188\n",
      "78    2133       1755.601647\n",
      "156   3201       1682.495312\n",
      "142   2981       1660.494300\n",
      "187   4081       1595.852842\n",
      "162   3381       1435.412796\n",
      "192   4302       1422.767395\n",
      "169   3581        923.492765\n",
      "186   4044        725.119192\n",
      "70    2124        373.174556\n",
      "189   4161        318.868954\n",
      "103   2161        171.441794\n",
      "170   3601        156.127080\n"
     ]
    }
   ],
   "source": [
    "test_df = submission.merge(prediction_mapping, on=\"ID\")\n",
    "test_df = test_df.groupby(\"rm_id\", as_index=False).agg({\n",
    "    \"predicted_weight\": \"max\",\n",
    "}).sort_values(\"predicted_weight\", ascending=False)\n",
    "\n",
    "print(test_df[0:46])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea237fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
